{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw1myN1eW8mA"
      },
      "source": [
        "# **Experiment one:**\n",
        "For this experiment, we adress the following research questions\n",
        "\n",
        "*   How important is word order for this task?\n",
        "*   Does the tree structure help to get a better accuracy?\n",
        "\n",
        "To address both questions we run the models: DeepBOW, LSTM and Tree-LSTM and compare their accuracies. We first tune the hyperparameters using validation sets and then we test them over 3 seeds to get the best models and mean accuracy on the test set.\n",
        "\n",
        "To adress the question cooncerned with word order we compare the models DeepBow and LSTM and how they perform. To address the question regarding tree structure, we compare the LSTM and Tree structure as it builds on LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNKef3lymaj"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jxTkpg59FlU"
      },
      "source": [
        "Let's first download the data set and take a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WZp53HmMP3F2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QprhgkFodAV",
        "outputId": "22109c99-3f02-4d4e-901a-24f7988d502c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-12 14:00:39--  http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip [following]\n",
            "--2024-12-12 14:00:39--  https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 789539 (771K) [application/zip]\n",
            "Saving to: ‘trainDevTestTrees_PTB.zip’\n",
            "\n",
            "trainDevTestTrees_P 100%[===================>] 771.03K  1017KB/s    in 0.8s    \n",
            "\n",
            "2024-12-12 14:00:40 (1017 KB/s) - ‘trainDevTestTrees_PTB.zip’ saved [789539/789539]\n",
            "\n",
            "Archive:  trainDevTestTrees_PTB.zip\n",
            "   creating: trees/\n",
            "  inflating: trees/dev.txt           \n",
            "  inflating: trees/test.txt          \n",
            "  inflating: trees/train.txt         \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
        "!unzip trainDevTestTrees_PTB.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0IpAphkBO5eW"
      },
      "outputs": [],
      "source": [
        "# this function reads in a textfile and fixes an issue with \"\\\\\"\n",
        "def filereader(path):\n",
        "  with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      yield line.strip().replace(\"\\\\\",\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP_jpquiprH8"
      },
      "source": [
        "Let's look at a data point. It is a **flattened binary tree**, with sentiment scores at every node, and words as the leaves (or *terminal nodes*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ylkIopm0QJML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9debd51d-9e19-4956-ecec-f298b16c4323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n"
          ]
        }
      ],
      "source": [
        "s = next(filereader(\"trees/dev.txt\"))\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7_U7HTFwdrWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12aaf1f0-8bf7-4862-c87b-4ea0551f76a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              3                                                                     \n",
            "  ____________|____________________                                                  \n",
            " |                                 4                                                \n",
            " |        _________________________|______________________________________________   \n",
            " |       4                                                                        | \n",
            " |    ___|______________                                                          |  \n",
            " |   |                  4                                                         | \n",
            " |   |         _________|__________                                               |  \n",
            " |   |        |                    3                                              | \n",
            " |   |        |               _____|______________________                        |  \n",
            " |   |        |              |                            4                       | \n",
            " |   |        |              |            ________________|_______                |  \n",
            " |   |        |              |           |                        2               | \n",
            " |   |        |              |           |                 _______|___            |  \n",
            " |   |        3              |           |                |           2           | \n",
            " |   |    ____|_____         |           |                |        ___|_____      |  \n",
            " |   |   |          4        |           3                |       2         |     | \n",
            " |   |   |     _____|___     |      _____|_______         |    ___|___      |     |  \n",
            " 2   2   2    3         2    2     3             2        2   2       2     2     2 \n",
            " |   |   |    |         |    |     |             |        |   |       |     |     |  \n",
            " It  's  a  lovely     film with lovely     performances  by Buy     and Accorsi  . \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-6ab7e95feba4>:5: DeprecationWarning: \n",
            "    Class TreePrettyPrinter has been deprecated.  Import\n",
            "    `TreePrettyPrinter` using `from nltk.tree import\n",
            "    TreePrettyPrinter` instead.\n",
            "  print(TreePrettyPrinter(tree))\n"
          ]
        }
      ],
      "source": [
        "# We can use NLTK to better visualise the tree structure of the sentence\n",
        "from nltk import Tree\n",
        "from nltk.treeprettyprinter import TreePrettyPrinter\n",
        "tree = Tree.fromstring(s)\n",
        "print(TreePrettyPrinter(tree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekAWKsji9t93"
      },
      "source": [
        "The sentiment scores range from 0 (very negative) to 5 (very positive). Again, as you can see, every node in the tree is labeled with a sentiment score. For now, we will only use the score at the **root node**, i.e., the sentiment score for the complete sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DKynLm0xPKr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d8d7bc-8c43-4021-dac6-8d8ed4d57f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', \"'s\", 'a', 'lovely', 'film', 'with', 'lovely', 'performances', 'by', 'Buy', 'and', 'Accorsi', '.']\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "# Let's first make a function that extracts the tokens (the leaves).\n",
        "\n",
        "def tokens_from_treestring(s):\n",
        "  \"\"\"extract the tokens from a sentiment tree\"\"\"\n",
        "  return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
        "\n",
        "# let's try it on our example tree\n",
        "tokens = tokens_from_treestring(s)\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8vFkeqN-NLP"
      },
      "source": [
        "> *Warning: you could also parse a treestring using NLTK and ask it to return the leaves, but there seems to be an issue with NLTK not always correctly parsing the input, so do not rely on it.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Akr9K_Mv4dym"
      },
      "outputs": [],
      "source": [
        "# We will also need the following function, but you can ignore this for now.\n",
        "# It is explained later on.\n",
        "\n",
        "SHIFT = 0\n",
        "REDUCE = 1\n",
        "\n",
        "\n",
        "def transitions_from_treestring(s):\n",
        "  s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
        "  s = re.sub(\"\\)\", \" )\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\([0-4] \", \"\", s)\n",
        "  s = re.sub(\"\\)\", \"1\", s)\n",
        "  return list(map(int, s.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mNtPdlwPgRat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926b83ff-e1f2-45ac-e07d-2ed0ced400e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trees/train.txt  8544\n",
            "trees/dev.txt    1101\n",
            "trees/test.txt   2210\n"
          ]
        }
      ],
      "source": [
        "# Now let's first see how large our data sets are.\n",
        "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
        "  print(\"{:16s} {:4d}\".format(path, sum(1 for _ in filereader(path))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YP0wc22ohWlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabc95b3-50ff-44cf-cb88-f1bc324ba9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sentence in trees/train.txt:\n",
            "(3 (2 (2 The) (2 Rock)) (4 (3 (2 is) (4 (2 destined) (2 (2 (2 (2 (2 to) (2 (2 be) (2 (2 the) (2 (2 21st) (2 (2 (2 Century) (2 's)) (2 (3 new) (2 (2 ``) (2 Conan)))))))) (2 '')) (2 and)) (3 (2 that) (3 (2 he) (3 (2 's) (3 (2 going) (3 (2 to) (4 (3 (2 make) (3 (3 (2 a) (3 splash)) (2 (2 even) (3 greater)))) (2 (2 than) (2 (2 (2 (2 (1 (2 Arnold) (2 Schwarzenegger)) (2 ,)) (2 (2 Jean-Claud) (2 (2 Van) (2 Damme)))) (2 or)) (2 (2 Steven) (2 Segal))))))))))))) (2 .)))\n",
            "First sentence in trees/dev.txt:\n",
            "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n",
            "First sentence in trees/test.txt:\n",
            "(2 (3 (3 Effective) (2 but)) (1 (1 too-tepid) (2 biopic)))\n"
          ]
        }
      ],
      "source": [
        "for path in (\"trees/train.txt\", \"trees/dev.txt\", \"trees/test.txt\"):\n",
        "    print(f\"First sentence in {path}:\")\n",
        "    print(next(filereader(path)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HexlSqTR_UrY"
      },
      "source": [
        "You can see that the number of sentences is not very large. That's probably because the data set required so much manual annotation. However, it is large enough to train a neural network on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfRjelOcsXuC"
      },
      "source": [
        "It will be useful to store each data example in an `Example` object,\n",
        "containing everything that we may need for each data point.\n",
        "It will contain the tokens, the tree, the top-level sentiment label, and\n",
        "the transitions (explained later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4I07Hb_-q8wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb111c4-7362-4e28-8093-78b2ab858fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 8544\n",
            "dev 1101\n",
            "test 2210\n"
          ]
        }
      ],
      "source": [
        "from collections import namedtuple\n",
        "from nltk import Tree\n",
        "\n",
        "# A simple way to define a class is using namedtuple.\n",
        "Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
        "\n",
        "\n",
        "def examplereader(path, lower=False):\n",
        "  \"\"\"Returns all examples in a file one by one.\"\"\"\n",
        "  for line in filereader(path):\n",
        "    line = line.lower() if lower else line\n",
        "    tokens = tokens_from_treestring(line)\n",
        "    tree = Tree.fromstring(line)  # use NLTK's Tree\n",
        "    label = int(line[1])\n",
        "    trans = transitions_from_treestring(line)\n",
        "    yield Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
        "\n",
        "\n",
        "# Let's load the data into memory.\n",
        "LOWER = False  # we will keep the original casing\n",
        "train_data = list(examplereader(\"trees/train.txt\", lower=LOWER))\n",
        "dev_data = list(examplereader(\"trees/dev.txt\", lower=LOWER))\n",
        "test_data = list(examplereader(\"trees/test.txt\", lower=LOWER))\n",
        "\n",
        "print(\"train\", len(train_data))\n",
        "print(\"dev\", len(dev_data))\n",
        "print(\"test\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to count label distribution in the data\n",
        "def count_labels(data):\n",
        "    labels = [example.label for example in data]\n",
        "    return Counter(labels)\n",
        "\n",
        "# Count label distributions for train, dev, and test data\n",
        "train_labels = count_labels(train_data)\n",
        "dev_labels = count_labels(dev_data)\n",
        "test_labels = count_labels(test_data)\n",
        "\n",
        "print(\"Train Label Distribution:\", train_labels)\n",
        "print(\"Dev Label Distribution:\", dev_labels)\n",
        "print(\"Test Label Distribution:\", test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oso5yBvkUYWF",
        "outputId": "3aa1b371-6f71-4a21-eb02-0ce0d065ecac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Label Distribution: Counter({3: 2322, 1: 2218, 2: 1624, 4: 1288, 0: 1092})\n",
            "Dev Label Distribution: Counter({1: 289, 3: 279, 2: 229, 4: 165, 0: 139})\n",
            "Test Label Distribution: Counter({1: 633, 3: 510, 4: 399, 2: 389, 0: 279})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot label distributions\n",
        "def plot_label_distribution(label_counts, title):\n",
        "    labels, counts = zip(*sorted(label_counts.items()))  # Sort by label for consistency\n",
        "    plt.bar(labels, counts)\n",
        "    plt.xlabel('Labels')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot distributions for train, dev, and test datasets\n",
        "plot_label_distribution(train_labels, \"Training Data Label Distribution\")\n",
        "plot_label_distribution(dev_labels, \"Development Data Label Distribution\")\n",
        "plot_label_distribution(test_labels, \"Test Data Label Distribution\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tns3Q_UFUbAU",
        "outputId": "180fa6a2-7fbb-47cf-e578-5cfad50e5b35"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KM0bDyeVZtP"
      },
      "source": [
        "Let's check out an `Example` object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WDSprDBVcr-"
      },
      "source": [
        "#### Vocabulary\n",
        "A first step in most NLP tasks is collecting all the word types that appear in the data into a vocabulary, and counting the frequency of their occurrences. On the one hand, this will give us an overview of the word distribution of the data set (what are the most frequent words, how many rare words are there, ...). On the other hand, we will also use the vocabulary to map each word to a unique numeric ID, which is a more handy index than a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VvNgKx7usRSt"
      },
      "outputs": [],
      "source": [
        "# Here we first define a class that can map a word to an ID (w2i)\n",
        "# and back (i2w).\n",
        "\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "\n",
        "\n",
        "class OrderedCounter(Counter, OrderedDict):\n",
        "  \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
        "  def __repr__(self):\n",
        "    return '%s(%r)' % (self.__class__.__name__,\n",
        "                      OrderedDict(self))\n",
        "  def __reduce__(self):\n",
        "    return self.__class__, (OrderedDict(self),)\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "  \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.freqs = OrderedCounter()\n",
        "    self.w2i = {}\n",
        "    self.i2w = []\n",
        "\n",
        "  def count_token(self, t):\n",
        "    self.freqs[t] += 1\n",
        "\n",
        "  def add_token(self, t):\n",
        "    self.w2i[t] = len(self.w2i)\n",
        "    self.i2w.append(t)\n",
        "\n",
        "  def build(self, min_freq=0):\n",
        "    '''\n",
        "    min_freq: minimum number of occurrences for a word to be included\n",
        "              in the vocabulary\n",
        "    '''\n",
        "    self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
        "    self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)\n",
        "\n",
        "    tok_freq = list(self.freqs.items())\n",
        "    tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "    for tok, freq in tok_freq:\n",
        "      if freq >= min_freq:\n",
        "        self.add_token(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOvkH_llVsoW"
      },
      "source": [
        "The vocabulary has by default an `<unk>` token and a `<pad>` token. The `<unk>` token is reserved for all words which do not appear in the training data (and for which, therefore, we cannot learn word representations). The function of the `<pad>` token will be explained later.\n",
        "\n",
        "\n",
        "Let's build the vocabulary!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GwGQgQQBNUSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6f8369-8d6e-41fd-ea24-a1f05bdd38bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 18280\n"
          ]
        }
      ],
      "source": [
        "# This process should be deterministic and should have the same result\n",
        "# if run multiple times on the same data set.\n",
        "\n",
        "v = Vocabulary()\n",
        "for data_set in (train_data,):\n",
        "  for ex in data_set:\n",
        "    for token in ex.tokens:\n",
        "      v.count_token(token)\n",
        "\n",
        "v.build()\n",
        "print(\"Vocabulary size:\", len(v.w2i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UNIedPrPdCw"
      },
      "source": [
        "Let's have a closer look at the properties of our vocabulary. Having a good idea of what it is like can facilitate data analysis and debugging later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AmTC-rvQelpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936256de-9ce2-4de6-a414-608441af0008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
            "very positive\n"
          ]
        }
      ],
      "source": [
        "# Now let's map the sentiment labels 0-4 to a more readable form\n",
        "i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "print(i2t)\n",
        "print(i2t[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "D7UI26DP2dr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7c81ad-df48-4bc0-9e66-1a4f694cc7a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('very negative', 0), ('negative', 1), ('neutral', 2), ('positive', 3), ('very positive', 4)])\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "# And let's also create the opposite mapping.\n",
        "# We won't use a Vocabulary for this (although we could), since the labels\n",
        "# are already numeric.\n",
        "t2i = OrderedDict({p : i for p, i in zip(i2t, range(len(i2t)))})\n",
        "print(t2i)\n",
        "print(t2i['very positive'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set seed"
      ],
      "metadata": {
        "id": "AVMgsbIea8bA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fGQcMCLBoF3R"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Enforce determinism in PyTorch\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0067ax54-rd"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "We are going to need PyTorch and Google Colab does not have it installed by default. Run the cell below to install it.\n",
        "\n",
        "*For installing PyTorch in your own computer, follow the instructions on [pytorch.org](pytorch.org) instead. This is for Google Colab only.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qKQMGtkR5KWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4540e60-4081-446f-ea9b-3e408357c513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Using torch\", torch.__version__) # should say 1.7.0+cu101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mnvPcd_E1xH8"
      },
      "outputs": [],
      "source": [
        "# Let's also import torch.nn, a PyTorch package that\n",
        "# makes building neural networks more convenient.\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BYt8uTyGCKc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffca872-f727-41e4-b226-17b328a6de3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# PyTorch can run on CPU or on Nvidia GPU (video card) using CUDA\n",
        "# This cell selects the GPU if one is available.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2d1VMOOYx1Bw"
      },
      "outputs": [],
      "source": [
        "# Seed manually to make runs reproducible\n",
        "# You need to set this again if you do multiple runs of the same model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# When running on the CuDNN backend two further options must be set for reproducibility\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfCx-HvMH1qQ"
      },
      "source": [
        "> **Hey, wait, where is the bias vector?**\n",
        "> PyTorch does not print Parameters, only Modules!\n",
        "\n",
        "> We can print it ourselves though, to check that it is there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Fhvk5HenAroT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Here we print each parameter name, shape, and if it is trainable.\n",
        "def print_parameters(model):\n",
        "  total = 0\n",
        "  for name, p in model.named_parameters():\n",
        "    total += np.prod(p.shape)\n",
        "    print(\"{:24s} {:12s} requires_grad={}\".format(name, str(list(p.shape)), p.requires_grad))\n",
        "  print(\"\\nTotal number of parameters: {}\\n\".format(total))\n",
        "\n",
        "\n",
        "#print_parameters(bow_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSAw292WxuP4"
      },
      "source": [
        "#### Preparing an example for input\n",
        "\n",
        "To feed sentences to our PyTorch model, we need to convert a sequence of tokens to a sequence of IDs. The `prepare_example` function below takes care of this for us. We then use these IDs as indices for the word embedding table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YWeGTC_OGReV"
      },
      "outputs": [],
      "source": [
        "def prepare_example(example, vocab):\n",
        "  \"\"\"\n",
        "  Map tokens to their IDs for a single example\n",
        "  \"\"\"\n",
        "\n",
        "  # vocab returns 0 if the word is not there (i2w[0] = <unk>)\n",
        "  x = [vocab.w2i.get(t, 0) for t in example.tokens]\n",
        "\n",
        "  x = torch.LongTensor([x])\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = torch.LongTensor([example.label])\n",
        "  y = y.to(device)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sfbdv9px3uFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78790106-8c97-45e8-88c1-3f51744c0c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([[  28,    9,    6,  998,   16,   18,  998,  135,   32, 7688,    5,    0,\n",
            "            2]], device='cuda:0')\n",
            "y: tensor([3], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "x, y = prepare_example(dev_data[0], v)\n",
        "print('x:', x)\n",
        "print('y:', y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWBTzkuE3CtZ"
      },
      "source": [
        "# BOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBAjYYySOA5W"
      },
      "source": [
        "Our first model is a rather simple neural **bag-of-words (BOW) model**.\n",
        "Unlike the bag-of-words model that you used in the previous lab, where we would look at the presence / frequency of words in a text, here we associate each word with a multi-dimensional vector which expresses what sentiment is conveyed by the word. In particular, our BOW vectors will be of size 5, exactly our number of sentiment classes.\n",
        "\n",
        "To classify a sentence, we **sum** the vectors of the words in the sentence and a bias vector. Because we sum the vectors, we lose word order: that's why we call this a neural bag-of-words model.\n",
        "\n",
        "```\n",
        "this   [0.0, 0.1, 0.1, 0.1, 0.0]\n",
        "movie  [0.0, 0.1, 0.1, 0.2, 0.1]\n",
        "is     [0.0, 0.1, 0.0, 0.0, 0.0]\n",
        "stupid [0.9, 0.5, 0.1, 0.0, 0.0]\n",
        "\n",
        "bias   [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "--------------------------------\n",
        "sum    [0.9, 0.8, 0.3, 0.3, 0.1]\n",
        "\n",
        "argmax: 0 (very negative)\n",
        "```\n",
        "\n",
        "The **argmax** of this sum is our predicted label.\n",
        "\n",
        "We initialize all vectors *randomly* and train them using cross-entropy loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLtBAIQGynkB"
      },
      "source": [
        "#### Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QZfNklWf3tvs"
      },
      "outputs": [],
      "source": [
        "class BOW(nn.Module):\n",
        "  \"\"\"A simple bag-of-words model\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, vocab):\n",
        "    super(BOW, self).__init__()\n",
        "    self.vocab = vocab\n",
        "\n",
        "    # this is a trainable look-up table with word embeddings\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # this is a trainable bias term\n",
        "    self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # this is the forward pass of the neural network\n",
        "    # it applies a function to the input and returns the output\n",
        "\n",
        "    # this looks up the embeddings for each word ID in inputs\n",
        "    # the result is a sequence of word embeddings\n",
        "    embeds = self.embed(inputs)\n",
        "\n",
        "    # the output is the sum across the time dimension (1)\n",
        "    # with the bias term added\n",
        "    logits = embeds.sum(1) + self.bias\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eKHvBnoBAr6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9e1ec5-0f47-4b1e-c9ee-a64ab558efad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW(\n",
            "  (embed): Embedding(18280, 5)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Let's create a model.\n",
        "vocab_size = len(v.w2i)\n",
        "n_classes = len(t2i)\n",
        "bow_model = BOW(vocab_size, n_classes, v)\n",
        "print(bow_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKNQjEc0yXnJ"
      },
      "source": [
        "#### Evaluation\n",
        "We now need to define an evaluation metric.\n",
        "How many predictions do we get right? The accuracy will tell us.\n",
        "Make sure that you understand this code block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yGmQLcVYKZsh"
      },
      "outputs": [],
      "source": [
        "def simple_evaluate(model, data, prep_fn=prepare_example, **kwargs):\n",
        "    \"\"\"Evaluate a model on a given data set and return y_true, y_pred, and accuracy.\"\"\"\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # Disable dropout\n",
        "\n",
        "    for example in data:\n",
        "        # Convert the example input and label to PyTorch tensors\n",
        "        x, target = prep_fn(example, model.vocab)\n",
        "\n",
        "        # Forward pass without backpropagation (no_grad)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "\n",
        "        # Get the prediction\n",
        "        prediction = logits.argmax(dim=-1)\n",
        "\n",
        "        # Collect true and predicted labels\n",
        "        y_true.append(target.item())\n",
        "        y_pred.append(prediction.item())\n",
        "\n",
        "        # Calculate the number of correct predictions for accuracy\n",
        "        correct += (prediction == target).sum().item()\n",
        "        total += 1\n",
        "\n",
        "    # Return all labels for additional metrics and the accuracy\n",
        "    accuracy = correct / float(total)\n",
        "    return y_true, y_pred, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KlIGFXllWWm"
      },
      "source": [
        "We are using accuracy as a handy evaluation metric. Please consider using [alternative metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) for your experiments if that makes more theoretical sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIk6OtSdzGRP"
      },
      "source": [
        "#### Example feed\n",
        "For stochastic gradient descent (SGD) we will need a random training example for every update.\n",
        "We implement this by shuffling the training data and returning examples one by one using `yield`.\n",
        "\n",
        "Shuffling is optional so that we get to use this function to get validation and test examples, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dxDFOZLfCXvJ"
      },
      "outputs": [],
      "source": [
        "def get_examples(data, shuffle=True, **kwargs):\n",
        "  \"\"\"Shuffle data set and return 1 example at a time (until nothing left)\"\"\"\n",
        "  if shuffle:\n",
        "    print(\"Shuffling training data\")\n",
        "    random.shuffle(data)  # shuffle training data each epoch\n",
        "  for example in data:\n",
        "    yield example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g09SM8yb2cjx"
      },
      "source": [
        "#### Exercise: Training function\n",
        "\n",
        "Your task is now to complete the training loop below.\n",
        "Before you do so, please read the section about optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVfUukVdM_1c"
      },
      "source": [
        "**Optimisation**\n",
        "\n",
        "As mentioned in the \"Intro to PyTorch\" notebook, one of the perks of using PyTorch is automatic differentiation. We will use it to train our BOW model.\n",
        "\n",
        "We train our model by feeding it an input, performing a **forward** pass, obtaining an output prediction, and calculating a **loss** with our loss function.\n",
        "After the gradients are computed in the **backward** pass, we can take a step on the surface of the loss function towards more optimal parameter settings (gradient descent).\n",
        "\n",
        "The package we will use to do this optimisation is [torch.optim](https://pytorch.org/docs/stable/optim.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KhQigDrQ--YU"
      },
      "outputs": [],
      "source": [
        "from torch import optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGIvcTZU_Cez"
      },
      "source": [
        "Besides implementations of stochastic gradient descent (SGD), this package also implements the optimisation algorithm Adam, which we'll be using in this practical.\n",
        "For the purposes of this assignment you do not need to know what Adam does besides that it uses gradient information to update our model parameters by calling:\n",
        "\n",
        "```\n",
        "optimizer.step()\n",
        "```\n",
        "Remember when we updated our parameters in the PyTorch tutorial in a loop?\n",
        "\n",
        "\n",
        "```python\n",
        "# update weights\n",
        "learning_rate = 0.5\n",
        "for f in net.parameters():\n",
        "    # for each parameter, take a small step in the opposite dir of the gradient\n",
        "    p.data = p.data - p.grad.data * learning_rate\n",
        "\n",
        "```\n",
        "The function call optimizer.step() does effectively the same thing.\n",
        "\n",
        "*(If you want to know more about optimisation algorithms using gradient information, [this blog](http://ruder.io/optimizing-gradient-descent/.) gives a nice intuitive overview.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "ktFnKBux25lD"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, num_iterations=10000,\n",
        "                print_every=1000, eval_every=1000,\n",
        "                batch_fn=get_examples,\n",
        "                prep_fn=prepare_example,\n",
        "                eval_fn=simple_evaluate,\n",
        "                batch_size=1, eval_batch_size=None):\n",
        "  \"\"\"Train a model.\"\"\"\n",
        "  training_data = train_data[:]\n",
        "  iter_i = 0\n",
        "  train_loss = 0.\n",
        "  print_num = 0\n",
        "  start = time.time()\n",
        "  criterion = nn.CrossEntropyLoss() # loss function\n",
        "  best_eval = 0.\n",
        "  best_iter = 0\n",
        "\n",
        "  # store train loss and validation accuracy during training\n",
        "  # so we can plot them afterwards\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "\n",
        "  if eval_batch_size is None:\n",
        "    eval_batch_size = batch_size\n",
        "\n",
        "  while True:  # when we run out of examples, shuffle and continue\n",
        "    for batch in batch_fn(train_data, batch_size=batch_size):\n",
        "\n",
        "      # forward pass\n",
        "      model.train()\n",
        "      x, targets = prep_fn(batch, model.vocab)\n",
        "      logits = model(x)\n",
        "\n",
        "      B = targets.size(0)  # later we will use B examples per update\n",
        "\n",
        "      # compute cross-entropy loss (our criterion)\n",
        "      # note that the cross entropy loss function computes the softmax for us\n",
        "      loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # backward pass (tip: check the Introduction to PyTorch notebook)\n",
        "\n",
        "      # erase previous gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # update weights - take a small step in the opposite dir of the gradient\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "      print_num += 1\n",
        "      iter_i += 1\n",
        "\n",
        "      # print info\n",
        "      if iter_i % print_every == 0:\n",
        "        print(\"Iter %r: loss=%.4f, time=%.2fs\" %\n",
        "              (iter_i, train_loss, time.time()-start))\n",
        "        losses.append(train_loss)\n",
        "        print_num = 0\n",
        "        train_loss = 0.\n",
        "\n",
        "      # evaluate\n",
        "      if iter_i % eval_every == 0:\n",
        "        _, _, accuracy = eval_fn(model, dev_data, batch_size=eval_batch_size,\n",
        "                                 batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        accuracies.append(accuracy)\n",
        "        print(\"iter %r: dev acc=%.4f\" % (iter_i, accuracy))\n",
        "\n",
        "        # save best model parameters\n",
        "        if accuracy > best_eval:\n",
        "          print(\"new highscore\")\n",
        "          best_eval = accuracy\n",
        "          best_iter = iter_i\n",
        "          path = \"{}.pt\".format(model.__class__.__name__)\n",
        "          ckpt = {\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "              \"best_eval\": best_eval,\n",
        "              \"best_iter\": best_iter\n",
        "          }\n",
        "          torch.save(ckpt, path)\n",
        "\n",
        "      # done training\n",
        "      if iter_i == num_iterations:\n",
        "        print(\"Done training\")\n",
        "\n",
        "        # evaluate on train, dev, and test with best model\n",
        "        print(\"Loading best model\")\n",
        "        path = \"{}.pt\".format(model.__class__.__name__)\n",
        "        ckpt = torch.load(path)\n",
        "        model.load_state_dict(ckpt[\"state_dict\"])\n",
        "\n",
        "        _, _, train_acc = eval_fn(\n",
        "            model, train_data, batch_size=eval_batch_size,\n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        _, _, dev_acc = eval_fn(\n",
        "            model, dev_data, batch_size=eval_batch_size,\n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "        _, _, test_acc = eval_fn(\n",
        "            model, test_data, batch_size=eval_batch_size,\n",
        "            batch_fn=batch_fn, prep_fn=prep_fn)\n",
        "\n",
        "        print(\"best model iter {:d}: \"\n",
        "              \"train acc={:.4f}, dev acc={:.4f}, test acc={:.4f}\".format(\n",
        "                  best_iter, train_acc, dev_acc, test_acc))\n",
        "\n",
        "        return losses, accuracies, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEPsLvI-3D5b"
      },
      "source": [
        "### Training the BOW model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9luJnNuN_d3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9833407-819f-47e5-d23c-0ab9d43eca4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling training data\n",
            "Iter 250: loss=1328.0669, time=1.24s\n",
            "iter 250: dev acc=0.1935\n",
            "new highscore\n",
            "Iter 500: loss=1307.1405, time=1.87s\n",
            "iter 500: dev acc=0.1926\n",
            "Iter 750: loss=1481.4087, time=2.45s\n",
            "iter 750: dev acc=0.1853\n",
            "Iter 1000: loss=1392.0646, time=3.02s\n",
            "iter 1000: dev acc=0.1907\n",
            "Iter 1250: loss=1348.1545, time=3.58s\n",
            "iter 1250: dev acc=0.1916\n",
            "Iter 1500: loss=1196.0284, time=4.15s\n",
            "iter 1500: dev acc=0.1916\n",
            "Iter 1750: loss=1280.2403, time=4.72s\n",
            "iter 1750: dev acc=0.1953\n",
            "new highscore\n",
            "Iter 2000: loss=1150.3451, time=5.30s\n",
            "iter 2000: dev acc=0.1944\n",
            "Iter 2250: loss=1152.3489, time=5.86s\n",
            "iter 2250: dev acc=0.1935\n",
            "Iter 2500: loss=1108.7458, time=6.44s\n",
            "iter 2500: dev acc=0.1907\n",
            "Iter 2750: loss=1137.3586, time=7.01s\n",
            "iter 2750: dev acc=0.1898\n",
            "Iter 3000: loss=1289.1320, time=7.58s\n",
            "iter 3000: dev acc=0.1926\n",
            "Iter 3250: loss=1089.3906, time=8.15s\n",
            "iter 3250: dev acc=0.1926\n",
            "Iter 3500: loss=1212.3204, time=8.73s\n",
            "iter 3500: dev acc=0.1926\n",
            "Iter 3750: loss=1123.5985, time=9.31s\n",
            "iter 3750: dev acc=0.1926\n",
            "Iter 4000: loss=1131.8507, time=9.94s\n",
            "iter 4000: dev acc=0.1907\n",
            "Iter 4250: loss=1117.4397, time=10.53s\n",
            "iter 4250: dev acc=0.1944\n",
            "Iter 4500: loss=1144.9779, time=11.10s\n",
            "iter 4500: dev acc=0.1998\n",
            "new highscore\n",
            "Iter 4750: loss=1082.5437, time=11.67s\n",
            "iter 4750: dev acc=0.1998\n",
            "Iter 5000: loss=1161.2412, time=12.24s\n",
            "iter 5000: dev acc=0.1980\n",
            "Iter 5250: loss=1142.2890, time=12.80s\n",
            "iter 5250: dev acc=0.1998\n",
            "Iter 5500: loss=1119.1575, time=13.36s\n",
            "iter 5500: dev acc=0.2016\n",
            "new highscore\n",
            "Iter 5750: loss=1104.9382, time=13.92s\n",
            "iter 5750: dev acc=0.2035\n",
            "new highscore\n",
            "Iter 6000: loss=1096.7244, time=14.49s\n",
            "iter 6000: dev acc=0.1998\n",
            "Iter 6250: loss=1051.1720, time=15.04s\n",
            "iter 6250: dev acc=0.2044\n",
            "new highscore\n",
            "Iter 6500: loss=1176.0028, time=15.60s\n",
            "iter 6500: dev acc=0.2025\n",
            "Iter 6750: loss=1042.3995, time=16.17s\n",
            "iter 6750: dev acc=0.2035\n",
            "Iter 7000: loss=1068.3356, time=16.73s\n",
            "iter 7000: dev acc=0.2035\n",
            "Iter 7250: loss=1104.8860, time=17.31s\n",
            "iter 7250: dev acc=0.2044\n",
            "Iter 7500: loss=1026.9198, time=17.90s\n",
            "iter 7500: dev acc=0.2025\n",
            "Iter 7750: loss=1045.0215, time=18.47s\n",
            "iter 7750: dev acc=0.2035\n",
            "Iter 8000: loss=982.3594, time=19.03s\n",
            "iter 8000: dev acc=0.2107\n",
            "new highscore\n",
            "Iter 8250: loss=1138.5370, time=19.60s\n",
            "iter 8250: dev acc=0.2107\n",
            "Iter 8500: loss=1083.7854, time=20.16s\n",
            "iter 8500: dev acc=0.2053\n",
            "Shuffling training data\n",
            "Iter 8750: loss=1046.4391, time=20.74s\n",
            "iter 8750: dev acc=0.2062\n",
            "Iter 9000: loss=972.5336, time=21.33s\n",
            "iter 9000: dev acc=0.2071\n",
            "Iter 9250: loss=1072.4983, time=21.90s\n",
            "iter 9250: dev acc=0.2071\n",
            "Iter 9500: loss=977.6327, time=22.46s\n",
            "iter 9500: dev acc=0.2089\n",
            "Iter 9750: loss=985.9776, time=23.02s\n",
            "iter 9750: dev acc=0.2107\n",
            "Iter 10000: loss=1041.7980, time=23.58s\n",
            "iter 10000: dev acc=0.2071\n",
            "Iter 10250: loss=1024.6424, time=24.14s\n",
            "iter 10250: dev acc=0.2071\n",
            "Iter 10500: loss=956.1114, time=24.70s\n",
            "iter 10500: dev acc=0.2098\n",
            "Iter 10750: loss=967.5407, time=25.26s\n",
            "iter 10750: dev acc=0.2098\n",
            "Iter 11000: loss=973.6233, time=25.82s\n",
            "iter 11000: dev acc=0.2071\n",
            "Iter 11250: loss=998.0318, time=26.38s\n",
            "iter 11250: dev acc=0.2089\n",
            "Iter 11500: loss=1018.3932, time=26.94s\n",
            "iter 11500: dev acc=0.2080\n",
            "Iter 11750: loss=994.9362, time=27.50s\n",
            "iter 11750: dev acc=0.2107\n",
            "Iter 12000: loss=1002.1129, time=28.05s\n",
            "iter 12000: dev acc=0.2125\n",
            "new highscore\n",
            "Iter 12250: loss=1045.2234, time=28.62s\n",
            "iter 12250: dev acc=0.2107\n",
            "Iter 12500: loss=967.5636, time=29.18s\n",
            "iter 12500: dev acc=0.2107\n",
            "Iter 12750: loss=925.9528, time=29.74s\n",
            "iter 12750: dev acc=0.2125\n",
            "Iter 13000: loss=1016.0226, time=30.31s\n",
            "iter 13000: dev acc=0.2125\n",
            "Iter 13250: loss=1041.8602, time=30.86s\n",
            "iter 13250: dev acc=0.2171\n",
            "new highscore\n",
            "Iter 13500: loss=919.6830, time=31.49s\n",
            "iter 13500: dev acc=0.2180\n",
            "new highscore\n",
            "Iter 13750: loss=1049.4084, time=32.06s\n",
            "iter 13750: dev acc=0.2162\n",
            "Iter 14000: loss=965.3171, time=32.61s\n",
            "iter 14000: dev acc=0.2153\n",
            "Iter 14250: loss=971.0423, time=33.18s\n",
            "iter 14250: dev acc=0.2134\n",
            "Iter 14500: loss=898.7062, time=33.73s\n",
            "iter 14500: dev acc=0.2153\n",
            "Iter 14750: loss=1001.6318, time=34.28s\n",
            "iter 14750: dev acc=0.2180\n",
            "Iter 15000: loss=1061.3301, time=34.83s\n",
            "iter 15000: dev acc=0.2162\n",
            "Iter 15250: loss=1017.7346, time=35.39s\n",
            "iter 15250: dev acc=0.2162\n",
            "Iter 15500: loss=902.9574, time=35.94s\n",
            "iter 15500: dev acc=0.2207\n",
            "new highscore\n",
            "Iter 15750: loss=956.3047, time=36.50s\n",
            "iter 15750: dev acc=0.2207\n",
            "Iter 16000: loss=943.9080, time=37.05s\n",
            "iter 16000: dev acc=0.2234\n",
            "new highscore\n",
            "Iter 16250: loss=1020.0361, time=37.61s\n",
            "iter 16250: dev acc=0.2198\n",
            "Iter 16500: loss=845.3734, time=38.16s\n",
            "iter 16500: dev acc=0.2216\n",
            "Iter 16750: loss=927.3075, time=38.71s\n",
            "iter 16750: dev acc=0.2271\n",
            "new highscore\n",
            "Iter 17000: loss=915.4594, time=39.28s\n",
            "iter 17000: dev acc=0.2243\n",
            "Shuffling training data\n",
            "Iter 17250: loss=860.8135, time=39.84s\n",
            "iter 17250: dev acc=0.2243\n",
            "Iter 17500: loss=889.6143, time=40.40s\n",
            "iter 17500: dev acc=0.2252\n",
            "Iter 17750: loss=965.7414, time=40.94s\n",
            "iter 17750: dev acc=0.2252\n",
            "Iter 18000: loss=900.3221, time=41.48s\n",
            "iter 18000: dev acc=0.2262\n",
            "Iter 18250: loss=856.2768, time=42.03s\n",
            "iter 18250: dev acc=0.2252\n",
            "Iter 18500: loss=928.4535, time=42.58s\n",
            "iter 18500: dev acc=0.2243\n",
            "Iter 18750: loss=805.4517, time=43.13s\n",
            "iter 18750: dev acc=0.2252\n",
            "Iter 19000: loss=922.5402, time=43.70s\n",
            "iter 19000: dev acc=0.2243\n",
            "Iter 19250: loss=905.1795, time=44.26s\n",
            "iter 19250: dev acc=0.2216\n",
            "Iter 19500: loss=913.9730, time=44.82s\n",
            "iter 19500: dev acc=0.2234\n",
            "Iter 19750: loss=792.4570, time=45.38s\n",
            "iter 19750: dev acc=0.2243\n",
            "Iter 20000: loss=897.8774, time=45.93s\n",
            "iter 20000: dev acc=0.2252\n",
            "Iter 20250: loss=853.6513, time=46.49s\n",
            "iter 20250: dev acc=0.2280\n",
            "new highscore\n",
            "Iter 20500: loss=862.2790, time=47.04s\n",
            "iter 20500: dev acc=0.2280\n",
            "Iter 20750: loss=900.4251, time=47.59s\n",
            "iter 20750: dev acc=0.2280\n",
            "Iter 21000: loss=938.4097, time=48.15s\n",
            "iter 21000: dev acc=0.2289\n",
            "new highscore\n",
            "Iter 21250: loss=881.5558, time=48.71s\n",
            "iter 21250: dev acc=0.2298\n",
            "new highscore\n",
            "Iter 21500: loss=799.3197, time=49.27s\n",
            "iter 21500: dev acc=0.2307\n",
            "new highscore\n",
            "Iter 21750: loss=899.6437, time=49.82s\n",
            "iter 21750: dev acc=0.2280\n",
            "Iter 22000: loss=850.1805, time=50.38s\n",
            "iter 22000: dev acc=0.2316\n",
            "new highscore\n",
            "Iter 22250: loss=870.1661, time=50.94s\n",
            "iter 22250: dev acc=0.2307\n",
            "Iter 22500: loss=876.1564, time=51.49s\n",
            "iter 22500: dev acc=0.2298\n",
            "Iter 22750: loss=854.2152, time=52.04s\n",
            "iter 22750: dev acc=0.2289\n",
            "Iter 23000: loss=913.7410, time=52.60s\n",
            "iter 23000: dev acc=0.2316\n",
            "Iter 23250: loss=803.3537, time=53.18s\n",
            "iter 23250: dev acc=0.2307\n",
            "Iter 23500: loss=810.5619, time=53.74s\n",
            "iter 23500: dev acc=0.2307\n",
            "Iter 23750: loss=773.7100, time=54.30s\n",
            "iter 23750: dev acc=0.2334\n",
            "new highscore\n",
            "Iter 24000: loss=722.0386, time=54.88s\n",
            "iter 24000: dev acc=0.2343\n",
            "new highscore\n",
            "Iter 24250: loss=867.9539, time=55.46s\n",
            "iter 24250: dev acc=0.2343\n",
            "Iter 24500: loss=849.7561, time=56.02s\n",
            "iter 24500: dev acc=0.2343\n",
            "Iter 24750: loss=903.5567, time=56.60s\n",
            "iter 24750: dev acc=0.2334\n",
            "Iter 25000: loss=814.0556, time=57.15s\n",
            "iter 25000: dev acc=0.2334\n",
            "Iter 25250: loss=763.6381, time=57.70s\n",
            "iter 25250: dev acc=0.2361\n",
            "new highscore\n",
            "Iter 25500: loss=874.1356, time=58.25s\n",
            "iter 25500: dev acc=0.2334\n",
            "Shuffling training data\n",
            "Iter 25750: loss=866.5815, time=58.81s\n",
            "iter 25750: dev acc=0.2361\n",
            "Iter 26000: loss=823.0993, time=59.37s\n",
            "iter 26000: dev acc=0.2334\n",
            "Iter 26250: loss=842.3562, time=59.92s\n",
            "iter 26250: dev acc=0.2352\n",
            "Iter 26500: loss=793.7673, time=60.47s\n",
            "iter 26500: dev acc=0.2343\n",
            "Iter 26750: loss=743.7760, time=61.02s\n",
            "iter 26750: dev acc=0.2325\n",
            "Iter 27000: loss=800.3832, time=61.57s\n",
            "iter 27000: dev acc=0.2352\n",
            "Iter 27250: loss=771.0806, time=62.13s\n",
            "iter 27250: dev acc=0.2371\n",
            "new highscore\n",
            "Iter 27500: loss=751.6830, time=62.69s\n",
            "iter 27500: dev acc=0.2352\n",
            "Iter 27750: loss=743.1717, time=63.24s\n",
            "iter 27750: dev acc=0.2352\n",
            "Iter 28000: loss=738.0050, time=63.79s\n",
            "iter 28000: dev acc=0.2334\n",
            "Iter 28250: loss=707.0988, time=64.34s\n",
            "iter 28250: dev acc=0.2361\n",
            "Iter 28500: loss=812.9386, time=64.89s\n",
            "iter 28500: dev acc=0.2398\n",
            "new highscore\n",
            "Iter 28750: loss=750.0774, time=65.46s\n",
            "iter 28750: dev acc=0.2371\n",
            "Iter 29000: loss=726.6460, time=66.01s\n",
            "iter 29000: dev acc=0.2361\n",
            "Iter 29250: loss=803.3025, time=66.56s\n",
            "iter 29250: dev acc=0.2316\n",
            "Iter 29500: loss=779.9520, time=67.13s\n",
            "iter 29500: dev acc=0.2343\n",
            "Iter 29750: loss=796.5667, time=67.70s\n",
            "iter 29750: dev acc=0.2334\n",
            "Iter 30000: loss=782.4186, time=68.28s\n",
            "iter 30000: dev acc=0.2334\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-07f34c5b57a1>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 28500: train acc=0.3071, dev acc=0.2398, test acc=0.2443\n",
            "Best Test Accuracy for seed 42: 0.2443\n",
            "Shuffling training data\n",
            "Iter 250: loss=1234.9920, time=0.26s\n",
            "iter 250: dev acc=0.1862\n",
            "new highscore\n",
            "Iter 500: loss=1271.1098, time=0.81s\n",
            "iter 500: dev acc=0.1880\n",
            "new highscore\n",
            "Iter 750: loss=1365.2055, time=1.37s\n",
            "iter 750: dev acc=0.1889\n",
            "new highscore\n",
            "Iter 1000: loss=1288.1588, time=1.93s\n",
            "iter 1000: dev acc=0.1871\n",
            "Iter 1250: loss=1263.3492, time=2.49s\n",
            "iter 1250: dev acc=0.1880\n",
            "Iter 1500: loss=1312.0686, time=3.05s\n",
            "iter 1500: dev acc=0.1916\n",
            "new highscore\n",
            "Iter 1750: loss=1124.6366, time=3.63s\n",
            "iter 1750: dev acc=0.1898\n",
            "Iter 2000: loss=1253.6047, time=4.19s\n",
            "iter 2000: dev acc=0.1889\n",
            "Iter 2250: loss=1241.2774, time=4.75s\n",
            "iter 2250: dev acc=0.1907\n",
            "Iter 2500: loss=1206.1156, time=5.31s\n",
            "iter 2500: dev acc=0.1935\n",
            "new highscore\n",
            "Iter 2750: loss=1156.0051, time=5.88s\n",
            "iter 2750: dev acc=0.1953\n",
            "new highscore\n",
            "Iter 3000: loss=1196.7076, time=6.44s\n",
            "iter 3000: dev acc=0.1980\n",
            "new highscore\n",
            "Iter 3250: loss=1232.9899, time=7.01s\n",
            "iter 3250: dev acc=0.1971\n",
            "Iter 3500: loss=1188.1419, time=7.59s\n",
            "iter 3500: dev acc=0.1980\n",
            "Iter 3750: loss=1134.0702, time=8.16s\n",
            "iter 3750: dev acc=0.2007\n",
            "new highscore\n",
            "Iter 4000: loss=1345.2002, time=8.73s\n",
            "iter 4000: dev acc=0.1998\n",
            "Iter 4250: loss=1249.6342, time=9.30s\n",
            "iter 4250: dev acc=0.1971\n",
            "Iter 4500: loss=1175.3934, time=9.86s\n",
            "iter 4500: dev acc=0.1989\n",
            "Iter 4750: loss=1222.2131, time=10.42s\n",
            "iter 4750: dev acc=0.1998\n",
            "Iter 5000: loss=1068.0355, time=10.99s\n",
            "iter 5000: dev acc=0.1989\n",
            "Iter 5250: loss=1018.1061, time=11.56s\n",
            "iter 5250: dev acc=0.2007\n",
            "Iter 5500: loss=1148.1514, time=12.12s\n",
            "iter 5500: dev acc=0.2007\n",
            "Iter 5750: loss=1041.3423, time=12.69s\n",
            "iter 5750: dev acc=0.1998\n",
            "Iter 6000: loss=1167.1885, time=13.25s\n",
            "iter 6000: dev acc=0.2025\n",
            "new highscore\n",
            "Iter 6250: loss=1122.0177, time=13.81s\n",
            "iter 6250: dev acc=0.2016\n",
            "Iter 6500: loss=1013.1672, time=14.37s\n",
            "iter 6500: dev acc=0.2007\n",
            "Iter 6750: loss=1214.4039, time=14.94s\n",
            "iter 6750: dev acc=0.1971\n",
            "Iter 7000: loss=1297.5544, time=15.50s\n",
            "iter 7000: dev acc=0.1980\n",
            "Iter 7250: loss=1120.4665, time=16.06s\n",
            "iter 7250: dev acc=0.1971\n",
            "Iter 7500: loss=1002.1958, time=16.60s\n",
            "iter 7500: dev acc=0.1989\n",
            "Iter 7750: loss=1097.4221, time=17.16s\n",
            "iter 7750: dev acc=0.1989\n",
            "Iter 8000: loss=1095.4067, time=17.72s\n",
            "iter 8000: dev acc=0.1998\n",
            "Iter 8250: loss=1050.7369, time=18.29s\n",
            "iter 8250: dev acc=0.1989\n",
            "Iter 8500: loss=940.0921, time=18.88s\n",
            "iter 8500: dev acc=0.1980\n",
            "Shuffling training data\n",
            "Iter 8750: loss=1084.0189, time=19.46s\n",
            "iter 8750: dev acc=0.1971\n",
            "Iter 9000: loss=1022.6403, time=20.04s\n",
            "iter 9000: dev acc=0.1989\n",
            "Iter 9250: loss=961.4430, time=20.60s\n",
            "iter 9250: dev acc=0.1980\n",
            "Iter 9500: loss=952.1446, time=21.20s\n",
            "iter 9500: dev acc=0.1962\n",
            "Iter 9750: loss=1008.9447, time=21.76s\n",
            "iter 9750: dev acc=0.1989\n",
            "Iter 10000: loss=997.2019, time=22.31s\n",
            "iter 10000: dev acc=0.2025\n",
            "Iter 10250: loss=1127.6147, time=22.86s\n",
            "iter 10250: dev acc=0.2053\n",
            "new highscore\n",
            "Iter 10500: loss=1076.6153, time=23.42s\n",
            "iter 10500: dev acc=0.2053\n",
            "Iter 10750: loss=1048.9581, time=23.98s\n",
            "iter 10750: dev acc=0.2080\n",
            "new highscore\n",
            "Iter 11000: loss=920.9780, time=24.53s\n",
            "iter 11000: dev acc=0.2062\n",
            "Iter 11250: loss=1044.5293, time=25.07s\n",
            "iter 11250: dev acc=0.1989\n",
            "Iter 11500: loss=1075.1331, time=25.64s\n",
            "iter 11500: dev acc=0.2035\n",
            "Iter 11750: loss=1028.7865, time=26.19s\n",
            "iter 11750: dev acc=0.2035\n",
            "Iter 12000: loss=907.3013, time=26.74s\n",
            "iter 12000: dev acc=0.2007\n",
            "Iter 12250: loss=941.8959, time=27.30s\n",
            "iter 12250: dev acc=0.2044\n",
            "Iter 12500: loss=1053.6824, time=27.85s\n",
            "iter 12500: dev acc=0.2016\n",
            "Iter 12750: loss=1048.8807, time=28.40s\n",
            "iter 12750: dev acc=0.2035\n",
            "Iter 13000: loss=938.7563, time=28.95s\n",
            "iter 13000: dev acc=0.1998\n",
            "Iter 13250: loss=839.8152, time=29.50s\n",
            "iter 13250: dev acc=0.2016\n",
            "Iter 13500: loss=950.2878, time=30.06s\n",
            "iter 13500: dev acc=0.2035\n",
            "Iter 13750: loss=1001.0706, time=30.61s\n",
            "iter 13750: dev acc=0.2025\n",
            "Iter 14000: loss=971.9470, time=31.17s\n",
            "iter 14000: dev acc=0.2016\n",
            "Iter 14250: loss=916.6490, time=31.72s\n",
            "iter 14250: dev acc=0.2053\n",
            "Iter 14500: loss=1014.8917, time=32.27s\n",
            "iter 14500: dev acc=0.2044\n",
            "Iter 14750: loss=927.0687, time=32.82s\n",
            "iter 14750: dev acc=0.2035\n",
            "Iter 15000: loss=892.1462, time=33.38s\n",
            "iter 15000: dev acc=0.1998\n",
            "Iter 15250: loss=893.5921, time=33.95s\n",
            "iter 15250: dev acc=0.2062\n",
            "Iter 15500: loss=1094.7879, time=34.51s\n",
            "iter 15500: dev acc=0.2044\n",
            "Iter 15750: loss=932.5601, time=35.08s\n",
            "iter 15750: dev acc=0.2098\n",
            "new highscore\n",
            "Iter 16000: loss=938.3356, time=35.65s\n",
            "iter 16000: dev acc=0.2071\n",
            "Iter 16250: loss=1023.6779, time=36.20s\n",
            "iter 16250: dev acc=0.2071\n",
            "Iter 16500: loss=962.5541, time=36.76s\n",
            "iter 16500: dev acc=0.2089\n",
            "Iter 16750: loss=835.0988, time=37.32s\n",
            "iter 16750: dev acc=0.2107\n",
            "new highscore\n",
            "Iter 17000: loss=850.9230, time=37.88s\n",
            "iter 17000: dev acc=0.2134\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 17250: loss=1025.7709, time=38.45s\n",
            "iter 17250: dev acc=0.2089\n",
            "Iter 17500: loss=921.0389, time=39.00s\n",
            "iter 17500: dev acc=0.2098\n",
            "Iter 17750: loss=933.2103, time=39.56s\n",
            "iter 17750: dev acc=0.2107\n",
            "Iter 18000: loss=912.6264, time=40.10s\n",
            "iter 18000: dev acc=0.2116\n",
            "Iter 18250: loss=781.0056, time=40.66s\n",
            "iter 18250: dev acc=0.2098\n",
            "Iter 18500: loss=861.6903, time=41.21s\n",
            "iter 18500: dev acc=0.2080\n",
            "Iter 18750: loss=826.7360, time=41.79s\n",
            "iter 18750: dev acc=0.2125\n",
            "Iter 19000: loss=963.2561, time=42.36s\n",
            "iter 19000: dev acc=0.2162\n",
            "new highscore\n",
            "Iter 19250: loss=854.1548, time=42.95s\n",
            "iter 19250: dev acc=0.2180\n",
            "new highscore\n",
            "Iter 19500: loss=883.2050, time=43.51s\n",
            "iter 19500: dev acc=0.2162\n",
            "Iter 19750: loss=845.0705, time=44.07s\n",
            "iter 19750: dev acc=0.2153\n",
            "Iter 20000: loss=895.2079, time=44.62s\n",
            "iter 20000: dev acc=0.2107\n",
            "Iter 20250: loss=891.2299, time=45.17s\n",
            "iter 20250: dev acc=0.2134\n",
            "Iter 20500: loss=864.3045, time=45.72s\n",
            "iter 20500: dev acc=0.2134\n",
            "Iter 20750: loss=775.4463, time=46.28s\n",
            "iter 20750: dev acc=0.2098\n",
            "Iter 21000: loss=867.1015, time=46.83s\n",
            "iter 21000: dev acc=0.2116\n",
            "Iter 21250: loss=894.3903, time=47.38s\n",
            "iter 21250: dev acc=0.2134\n",
            "Iter 21500: loss=855.0991, time=47.93s\n",
            "iter 21500: dev acc=0.2144\n",
            "Iter 21750: loss=929.6977, time=48.49s\n",
            "iter 21750: dev acc=0.2134\n",
            "Iter 22000: loss=887.4217, time=49.05s\n",
            "iter 22000: dev acc=0.2134\n",
            "Iter 22250: loss=855.8284, time=49.60s\n",
            "iter 22250: dev acc=0.2125\n",
            "Iter 22500: loss=779.3125, time=50.16s\n",
            "iter 22500: dev acc=0.2144\n",
            "Iter 22750: loss=803.0406, time=50.71s\n",
            "iter 22750: dev acc=0.2153\n",
            "Iter 23000: loss=750.0606, time=51.26s\n",
            "iter 23000: dev acc=0.2171\n",
            "Iter 23250: loss=770.5388, time=51.81s\n",
            "iter 23250: dev acc=0.2144\n",
            "Iter 23500: loss=886.7330, time=52.36s\n",
            "iter 23500: dev acc=0.2125\n",
            "Iter 23750: loss=798.2283, time=52.91s\n",
            "iter 23750: dev acc=0.2144\n",
            "Iter 24000: loss=765.8907, time=53.48s\n",
            "iter 24000: dev acc=0.2180\n",
            "Iter 24250: loss=856.9077, time=54.04s\n",
            "iter 24250: dev acc=0.2189\n",
            "new highscore\n",
            "Iter 24500: loss=812.1060, time=54.60s\n",
            "iter 24500: dev acc=0.2207\n",
            "new highscore\n",
            "Iter 24750: loss=861.9959, time=55.16s\n",
            "iter 24750: dev acc=0.2207\n",
            "Iter 25000: loss=823.3635, time=55.72s\n",
            "iter 25000: dev acc=0.2243\n",
            "new highscore\n",
            "Iter 25250: loss=883.6745, time=56.30s\n",
            "iter 25250: dev acc=0.2252\n",
            "new highscore\n",
            "Iter 25500: loss=744.0084, time=56.89s\n",
            "iter 25500: dev acc=0.2262\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 25750: loss=772.7975, time=57.48s\n",
            "iter 25750: dev acc=0.2252\n",
            "Iter 26000: loss=891.4611, time=58.05s\n",
            "iter 26000: dev acc=0.2262\n",
            "Iter 26250: loss=826.3942, time=58.63s\n",
            "iter 26250: dev acc=0.2225\n",
            "Iter 26500: loss=779.7782, time=59.21s\n",
            "iter 26500: dev acc=0.2225\n",
            "Iter 26750: loss=796.1316, time=59.76s\n",
            "iter 26750: dev acc=0.2216\n",
            "Iter 27000: loss=786.1480, time=60.30s\n",
            "iter 27000: dev acc=0.2225\n",
            "Iter 27250: loss=887.8485, time=60.85s\n",
            "iter 27250: dev acc=0.2252\n",
            "Iter 27500: loss=650.1245, time=61.41s\n",
            "iter 27500: dev acc=0.2262\n",
            "Iter 27750: loss=752.7404, time=61.97s\n",
            "iter 27750: dev acc=0.2243\n",
            "Iter 28000: loss=817.7043, time=62.54s\n",
            "iter 28000: dev acc=0.2216\n",
            "Iter 28250: loss=775.5006, time=63.12s\n",
            "iter 28250: dev acc=0.2207\n",
            "Iter 28500: loss=765.8428, time=63.69s\n",
            "iter 28500: dev acc=0.2207\n",
            "Iter 28750: loss=793.7951, time=64.24s\n",
            "iter 28750: dev acc=0.2234\n",
            "Iter 29000: loss=759.2440, time=64.81s\n",
            "iter 29000: dev acc=0.2262\n",
            "Iter 29250: loss=746.2361, time=65.37s\n",
            "iter 29250: dev acc=0.2243\n",
            "Iter 29500: loss=789.8397, time=65.93s\n",
            "iter 29500: dev acc=0.2262\n",
            "Iter 29750: loss=752.1159, time=66.49s\n",
            "iter 29750: dev acc=0.2243\n",
            "Iter 30000: loss=729.6140, time=67.04s\n",
            "iter 30000: dev acc=0.2271\n",
            "new highscore\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 30000: train acc=0.3242, dev acc=0.2271, test acc=0.2443\n",
            "Best Test Accuracy for seed 123: 0.2443\n",
            "Shuffling training data\n",
            "Iter 250: loss=1293.5548, time=0.26s\n",
            "iter 250: dev acc=0.2361\n",
            "new highscore\n",
            "Iter 500: loss=1387.1282, time=0.81s\n",
            "iter 500: dev acc=0.2452\n",
            "new highscore\n",
            "Iter 750: loss=1302.7411, time=1.37s\n",
            "iter 750: dev acc=0.2498\n",
            "new highscore\n",
            "Iter 1000: loss=1285.2708, time=1.92s\n",
            "iter 1000: dev acc=0.2461\n",
            "Iter 1250: loss=1231.4429, time=2.48s\n",
            "iter 1250: dev acc=0.2498\n",
            "Iter 1500: loss=1286.9703, time=3.05s\n",
            "iter 1500: dev acc=0.2525\n",
            "new highscore\n",
            "Iter 1750: loss=1274.7684, time=3.61s\n",
            "iter 1750: dev acc=0.2480\n",
            "Iter 2000: loss=1215.5138, time=4.17s\n",
            "iter 2000: dev acc=0.2480\n",
            "Iter 2250: loss=1248.4504, time=4.74s\n",
            "iter 2250: dev acc=0.2516\n",
            "Iter 2500: loss=1204.8907, time=5.30s\n",
            "iter 2500: dev acc=0.2525\n",
            "Iter 2750: loss=1099.7460, time=5.87s\n",
            "iter 2750: dev acc=0.2507\n",
            "Iter 3000: loss=1235.4058, time=6.44s\n",
            "iter 3000: dev acc=0.2516\n",
            "Iter 3250: loss=1215.4653, time=7.02s\n",
            "iter 3250: dev acc=0.2525\n",
            "Iter 3500: loss=1193.5498, time=7.59s\n",
            "iter 3500: dev acc=0.2525\n",
            "Iter 3750: loss=1208.9166, time=8.16s\n",
            "iter 3750: dev acc=0.2552\n",
            "new highscore\n",
            "Iter 4000: loss=1150.1260, time=8.72s\n",
            "iter 4000: dev acc=0.2570\n",
            "new highscore\n",
            "Iter 4250: loss=1311.7873, time=9.29s\n",
            "iter 4250: dev acc=0.2561\n",
            "Iter 4500: loss=1149.0672, time=9.87s\n",
            "iter 4500: dev acc=0.2525\n",
            "Iter 4750: loss=1109.4692, time=10.44s\n",
            "iter 4750: dev acc=0.2589\n",
            "new highscore\n",
            "Iter 5000: loss=1126.2481, time=11.02s\n",
            "iter 5000: dev acc=0.2607\n",
            "new highscore\n",
            "Iter 5250: loss=1070.7843, time=11.59s\n",
            "iter 5250: dev acc=0.2634\n",
            "new highscore\n",
            "Iter 5500: loss=1207.4928, time=12.21s\n",
            "iter 5500: dev acc=0.2607\n",
            "Iter 5750: loss=1148.6961, time=12.78s\n",
            "iter 5750: dev acc=0.2598\n",
            "Iter 6000: loss=1171.8562, time=13.34s\n",
            "iter 6000: dev acc=0.2579\n",
            "Iter 6250: loss=1142.3225, time=13.91s\n",
            "iter 6250: dev acc=0.2607\n",
            "Iter 6500: loss=1057.0566, time=14.47s\n",
            "iter 6500: dev acc=0.2598\n",
            "Iter 6750: loss=1135.0268, time=15.03s\n",
            "iter 6750: dev acc=0.2616\n",
            "Iter 7000: loss=1034.7178, time=15.59s\n",
            "iter 7000: dev acc=0.2607\n",
            "Iter 7250: loss=1117.6380, time=16.15s\n",
            "iter 7250: dev acc=0.2616\n",
            "Iter 7500: loss=1003.3585, time=16.71s\n",
            "iter 7500: dev acc=0.2598\n",
            "Iter 7750: loss=1099.2228, time=17.27s\n",
            "iter 7750: dev acc=0.2607\n",
            "Iter 8000: loss=1038.7390, time=17.83s\n",
            "iter 8000: dev acc=0.2598\n",
            "Iter 8250: loss=1092.7807, time=18.40s\n",
            "iter 8250: dev acc=0.2616\n",
            "Iter 8500: loss=1116.8170, time=18.97s\n",
            "iter 8500: dev acc=0.2625\n",
            "Shuffling training data\n",
            "Iter 8750: loss=1083.0356, time=19.53s\n",
            "iter 8750: dev acc=0.2616\n",
            "Iter 9000: loss=1114.0745, time=20.09s\n",
            "iter 9000: dev acc=0.2679\n",
            "new highscore\n",
            "Iter 9250: loss=1047.5084, time=20.64s\n",
            "iter 9250: dev acc=0.2670\n",
            "Iter 9500: loss=1073.9692, time=21.19s\n",
            "iter 9500: dev acc=0.2688\n",
            "new highscore\n",
            "Iter 9750: loss=1025.6875, time=21.75s\n",
            "iter 9750: dev acc=0.2670\n",
            "Iter 10000: loss=1038.4833, time=22.30s\n",
            "iter 10000: dev acc=0.2652\n",
            "Iter 10250: loss=1036.8101, time=22.85s\n",
            "iter 10250: dev acc=0.2643\n",
            "Iter 10500: loss=946.2384, time=23.40s\n",
            "iter 10500: dev acc=0.2661\n",
            "Iter 10750: loss=968.4420, time=23.95s\n",
            "iter 10750: dev acc=0.2688\n",
            "Iter 11000: loss=1089.4165, time=24.51s\n",
            "iter 11000: dev acc=0.2670\n",
            "Iter 11250: loss=1125.5470, time=25.06s\n",
            "iter 11250: dev acc=0.2670\n",
            "Iter 11500: loss=1069.2048, time=25.60s\n",
            "iter 11500: dev acc=0.2688\n",
            "Iter 11750: loss=1012.2892, time=26.15s\n",
            "iter 11750: dev acc=0.2634\n",
            "Iter 12000: loss=1060.0679, time=26.69s\n",
            "iter 12000: dev acc=0.2607\n",
            "Iter 12250: loss=1114.8802, time=27.24s\n",
            "iter 12250: dev acc=0.2661\n",
            "Iter 12500: loss=946.3486, time=27.78s\n",
            "iter 12500: dev acc=0.2679\n",
            "Iter 12750: loss=875.2045, time=28.33s\n",
            "iter 12750: dev acc=0.2688\n",
            "Iter 13000: loss=999.5349, time=28.89s\n",
            "iter 13000: dev acc=0.2688\n",
            "Iter 13250: loss=956.7555, time=29.44s\n",
            "iter 13250: dev acc=0.2725\n",
            "new highscore\n",
            "Iter 13500: loss=970.7208, time=30.01s\n",
            "iter 13500: dev acc=0.2707\n",
            "Iter 13750: loss=954.9470, time=30.55s\n",
            "iter 13750: dev acc=0.2716\n",
            "Iter 14000: loss=920.5814, time=31.11s\n",
            "iter 14000: dev acc=0.2698\n",
            "Iter 14250: loss=923.9284, time=31.65s\n",
            "iter 14250: dev acc=0.2679\n",
            "Iter 14500: loss=913.6198, time=32.20s\n",
            "iter 14500: dev acc=0.2679\n",
            "Iter 14750: loss=1050.3847, time=32.75s\n",
            "iter 14750: dev acc=0.2679\n",
            "Iter 15000: loss=1076.0020, time=33.30s\n",
            "iter 15000: dev acc=0.2716\n",
            "Iter 15250: loss=946.0018, time=33.86s\n",
            "iter 15250: dev acc=0.2743\n",
            "new highscore\n",
            "Iter 15500: loss=952.6696, time=34.41s\n",
            "iter 15500: dev acc=0.2761\n",
            "new highscore\n",
            "Iter 15750: loss=1040.7191, time=34.97s\n",
            "iter 15750: dev acc=0.2743\n",
            "Iter 16000: loss=1028.7784, time=35.52s\n",
            "iter 16000: dev acc=0.2761\n",
            "Iter 16250: loss=969.2409, time=36.07s\n",
            "iter 16250: dev acc=0.2761\n",
            "Iter 16500: loss=855.1033, time=36.61s\n",
            "iter 16500: dev acc=0.2770\n",
            "new highscore\n",
            "Iter 16750: loss=883.1087, time=37.16s\n",
            "iter 16750: dev acc=0.2743\n",
            "Iter 17000: loss=945.7981, time=37.71s\n",
            "iter 17000: dev acc=0.2743\n",
            "Shuffling training data\n",
            "Iter 17250: loss=925.6604, time=38.27s\n",
            "iter 17250: dev acc=0.2788\n",
            "new highscore\n",
            "Iter 17500: loss=865.0340, time=38.82s\n",
            "iter 17500: dev acc=0.2770\n",
            "Iter 17750: loss=944.9619, time=39.37s\n",
            "iter 17750: dev acc=0.2761\n",
            "Iter 18000: loss=868.4291, time=39.92s\n",
            "iter 18000: dev acc=0.2734\n",
            "Iter 18250: loss=897.9450, time=40.47s\n",
            "iter 18250: dev acc=0.2743\n",
            "Iter 18500: loss=845.8977, time=41.04s\n",
            "iter 18500: dev acc=0.2788\n",
            "Iter 18750: loss=808.7956, time=41.61s\n",
            "iter 18750: dev acc=0.2770\n",
            "Iter 19000: loss=921.3651, time=42.18s\n",
            "iter 19000: dev acc=0.2788\n",
            "Iter 19250: loss=922.3912, time=42.73s\n",
            "iter 19250: dev acc=0.2788\n",
            "Iter 19500: loss=857.2378, time=43.29s\n",
            "iter 19500: dev acc=0.2797\n",
            "new highscore\n",
            "Iter 19750: loss=904.6716, time=43.84s\n",
            "iter 19750: dev acc=0.2807\n",
            "new highscore\n",
            "Iter 20000: loss=931.2605, time=44.39s\n",
            "iter 20000: dev acc=0.2807\n",
            "Iter 20250: loss=796.8213, time=44.95s\n",
            "iter 20250: dev acc=0.2825\n",
            "new highscore\n",
            "Iter 20500: loss=945.3681, time=45.50s\n",
            "iter 20500: dev acc=0.2779\n",
            "Iter 20750: loss=854.9959, time=46.05s\n",
            "iter 20750: dev acc=0.2770\n",
            "Iter 21000: loss=832.0215, time=46.60s\n",
            "iter 21000: dev acc=0.2770\n",
            "Iter 21250: loss=816.0825, time=47.16s\n",
            "iter 21250: dev acc=0.2770\n",
            "Iter 21500: loss=960.8377, time=47.72s\n",
            "iter 21500: dev acc=0.2770\n",
            "Iter 21750: loss=923.7374, time=48.27s\n",
            "iter 21750: dev acc=0.2816\n",
            "Iter 22000: loss=895.3920, time=48.82s\n",
            "iter 22000: dev acc=0.2770\n",
            "Iter 22250: loss=807.5314, time=49.37s\n",
            "iter 22250: dev acc=0.2797\n",
            "Iter 22500: loss=840.1460, time=49.92s\n",
            "iter 22500: dev acc=0.2834\n",
            "new highscore\n",
            "Iter 22750: loss=966.8016, time=50.48s\n",
            "iter 22750: dev acc=0.2807\n",
            "Iter 23000: loss=830.9367, time=51.03s\n",
            "iter 23000: dev acc=0.2825\n",
            "Iter 23250: loss=877.2275, time=51.58s\n",
            "iter 23250: dev acc=0.2825\n",
            "Iter 23500: loss=951.3501, time=52.15s\n",
            "iter 23500: dev acc=0.2834\n",
            "Iter 23750: loss=902.8719, time=52.73s\n",
            "iter 23750: dev acc=0.2843\n",
            "new highscore\n",
            "Iter 24000: loss=774.5704, time=53.32s\n",
            "iter 24000: dev acc=0.2861\n",
            "new highscore\n",
            "Iter 24250: loss=908.8263, time=53.90s\n",
            "iter 24250: dev acc=0.2870\n",
            "new highscore\n",
            "Iter 24500: loss=819.4491, time=54.45s\n",
            "iter 24500: dev acc=0.2843\n",
            "Iter 24750: loss=819.3420, time=55.01s\n",
            "iter 24750: dev acc=0.2870\n",
            "Iter 25000: loss=800.0566, time=55.55s\n",
            "iter 25000: dev acc=0.2888\n",
            "new highscore\n",
            "Iter 25250: loss=837.2139, time=56.11s\n",
            "iter 25250: dev acc=0.2888\n",
            "Iter 25500: loss=925.2074, time=56.65s\n",
            "iter 25500: dev acc=0.2888\n",
            "Shuffling training data\n",
            "Iter 25750: loss=764.3628, time=57.21s\n",
            "iter 25750: dev acc=0.2879\n",
            "Iter 26000: loss=700.9345, time=57.75s\n",
            "iter 26000: dev acc=0.2888\n",
            "Iter 26250: loss=808.1396, time=58.31s\n",
            "iter 26250: dev acc=0.2897\n",
            "new highscore\n",
            "Iter 26500: loss=805.1922, time=58.86s\n",
            "iter 26500: dev acc=0.2897\n",
            "Iter 26750: loss=775.1482, time=59.41s\n",
            "iter 26750: dev acc=0.2897\n",
            "Iter 27000: loss=806.7657, time=59.96s\n",
            "iter 27000: dev acc=0.2888\n",
            "Iter 27250: loss=756.5147, time=60.54s\n",
            "iter 27250: dev acc=0.2952\n",
            "new highscore\n",
            "Iter 27500: loss=795.2802, time=61.10s\n",
            "iter 27500: dev acc=0.2934\n",
            "Iter 27750: loss=845.9132, time=61.65s\n",
            "iter 27750: dev acc=0.2925\n",
            "Iter 28000: loss=729.6395, time=62.20s\n",
            "iter 28000: dev acc=0.2934\n",
            "Iter 28250: loss=777.9705, time=62.75s\n",
            "iter 28250: dev acc=0.2934\n",
            "Iter 28500: loss=822.5448, time=63.30s\n",
            "iter 28500: dev acc=0.2897\n",
            "Iter 28750: loss=730.8343, time=63.87s\n",
            "iter 28750: dev acc=0.2925\n",
            "Iter 29000: loss=673.1041, time=64.42s\n",
            "iter 29000: dev acc=0.2906\n",
            "Iter 29250: loss=863.7984, time=64.99s\n",
            "iter 29250: dev acc=0.2925\n",
            "Iter 29500: loss=825.4306, time=65.56s\n",
            "iter 29500: dev acc=0.2906\n",
            "Iter 29750: loss=804.0749, time=66.12s\n",
            "iter 29750: dev acc=0.2906\n",
            "Iter 30000: loss=804.3895, time=66.67s\n",
            "iter 30000: dev acc=0.2943\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 27250: train acc=0.3136, dev acc=0.2952, test acc=0.2742\n",
            "Best Test Accuracy for seed 999: 0.2742\n"
          ]
        }
      ],
      "source": [
        "seeds = [42, 123, 999]\n",
        "test_accuracies_bow = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_seed(seed)\n",
        "\n",
        "    # If everything is in place we can now train our first model!\n",
        "    bow_model = BOW(len(v.w2i), len(t2i), vocab=v)\n",
        "\n",
        "    bow_model = bow_model.to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(bow_model.parameters(), lr=0.0005)\n",
        "\n",
        "    bow_losses, bow_accuracies, test_acc = train_model(\n",
        "    bow_model, optimizer, num_iterations=30000,\n",
        "    print_every=250, eval_every=250)\n",
        "\n",
        "    print(f\"Best Test Accuracy for seed {seed}: {test_acc:.4f}\")\n",
        "    test_accuracies_bow.append(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and standard deviation across seeds\n",
        "mean_accuracy = np.mean(test_accuracies_bow)\n",
        "std_accuracy = np.std(test_accuracies_bow)\n",
        "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation: {std_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "zZUWl7zayeX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a1ef29-f3c4-4e53-a557-a257c3e34520"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Test Accuracy: 0.2543\n",
            "Standard Deviation: 0.0141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9mB1_XhMPNN"
      },
      "source": [
        "# CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWk78FvNMw4o"
      },
      "source": [
        "We now continue with a **continuous bag-of-words (CBOW)** model. (*This is not the same as the word2vec CBOW model!*)\n",
        "\n",
        "It is similar to the BOW model above, but now embeddings can have a dimension of *arbitrary size*.\n",
        "This means that we can choose a higher dimensionality and learn more aspects of each word. We will still sum word vectors to get a sentence representation, but now the size of the resulting vector will no longer correspond to the number of sentiment classes.\n",
        "\n",
        "So to turn the size of our summed vector into the number of output classes, we can *learn* a parameter matrix $W$ and multiply it by the sum vector $x$: $$Wx$$\n",
        "If the size of $x$ is `d x 1`, we can set $W$ to be `5 x d`, so that the output of the matrix multiplication will be the of the desired size, `5 x 1`. Then, just like for the BOW model, we can obtain a prediction using the argmax function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIjrCPfCwsXI"
      },
      "source": [
        "## Exercise: implement and train the CBOW model\n",
        "\n",
        "Write a class `CBOW` that:\n",
        "\n",
        "- has word embeddings with size 300\n",
        "- sums the word vectors for the input words (just like in `BOW`)\n",
        "- projects the resulting vector down to 5 units using a linear layer and a bias term (check out `nn.Linear`)\n",
        "\n",
        "Train your CBOW model and plot the validation accuracy and training loss over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PEV22aR2MP0Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, num_classes, vocab, embedding_dim=300):\n",
        "        \"\"\"\n",
        "        CBOW Model\n",
        "        \"\"\"\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        # 1. Embedding layer: maps word indices to 300-dimensional vectors\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # 2. Linear layer: projects the sentence vector (embedding_dim) to output logits (num_classes)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the CBOW model\n",
        "        \"\"\"\n",
        "        # 1. Look up embeddings for each word in the input\n",
        "        embeddings = self.embed(x)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # 2. Sum embeddings along the sequence length dimension\n",
        "        sentence_vector = embeddings.sum(dim=1)  # Shape: [batch_size, embedding_dim]\n",
        "\n",
        "        # 3. Project the sentence vector to logits using the linear layer\n",
        "        logits = self.fc(sentence_vector)  # Shape: [batch_size, num_classes]\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8gK-jxAoRXLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92639cac-3808-40f6-edde-8b87e3944161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling training data\n",
            "Iter 100: loss=335.1990, time=0.30s\n",
            "iter 100: dev acc=0.2134\n",
            "new highscore\n",
            "Iter 200: loss=306.0141, time=0.88s\n",
            "iter 200: dev acc=0.2080\n",
            "Iter 300: loss=323.9237, time=1.35s\n",
            "iter 300: dev acc=0.2053\n",
            "Iter 400: loss=239.8090, time=1.83s\n",
            "iter 400: dev acc=0.2153\n",
            "new highscore\n",
            "Iter 500: loss=243.7179, time=2.46s\n",
            "iter 500: dev acc=0.2225\n",
            "new highscore\n",
            "Iter 600: loss=315.6823, time=3.14s\n",
            "iter 600: dev acc=0.2361\n",
            "new highscore\n",
            "Iter 700: loss=247.9935, time=3.77s\n",
            "iter 700: dev acc=0.2343\n",
            "Iter 800: loss=258.4768, time=4.24s\n",
            "iter 800: dev acc=0.2307\n",
            "Iter 900: loss=225.4629, time=4.72s\n",
            "iter 900: dev acc=0.2298\n",
            "Iter 1000: loss=238.0346, time=5.20s\n",
            "iter 1000: dev acc=0.2407\n",
            "new highscore\n",
            "Iter 1100: loss=236.9075, time=5.81s\n",
            "iter 1100: dev acc=0.2480\n",
            "new highscore\n",
            "Iter 1200: loss=208.1920, time=6.44s\n",
            "iter 1200: dev acc=0.2652\n",
            "new highscore\n",
            "Iter 1300: loss=240.5668, time=7.05s\n",
            "iter 1300: dev acc=0.2543\n",
            "Iter 1400: loss=256.0918, time=7.52s\n",
            "iter 1400: dev acc=0.2625\n",
            "Iter 1500: loss=216.2734, time=8.00s\n",
            "iter 1500: dev acc=0.2779\n",
            "new highscore\n",
            "Iter 1600: loss=217.8308, time=8.62s\n",
            "iter 1600: dev acc=0.2661\n",
            "Iter 1700: loss=222.1367, time=9.10s\n",
            "iter 1700: dev acc=0.2679\n",
            "Iter 1800: loss=238.5629, time=9.58s\n",
            "iter 1800: dev acc=0.2552\n",
            "Iter 1900: loss=237.2388, time=10.06s\n",
            "iter 1900: dev acc=0.2589\n",
            "Iter 2000: loss=214.0629, time=10.53s\n",
            "iter 2000: dev acc=0.2361\n",
            "Iter 2100: loss=208.6088, time=11.00s\n",
            "iter 2100: dev acc=0.2452\n",
            "Iter 2200: loss=215.1815, time=11.47s\n",
            "iter 2200: dev acc=0.2525\n",
            "Iter 2300: loss=232.1263, time=11.95s\n",
            "iter 2300: dev acc=0.2425\n",
            "Iter 2400: loss=212.1997, time=12.42s\n",
            "iter 2400: dev acc=0.2398\n",
            "Iter 2500: loss=177.4357, time=12.89s\n",
            "iter 2500: dev acc=0.2480\n",
            "Iter 2600: loss=195.9322, time=13.36s\n",
            "iter 2600: dev acc=0.2470\n",
            "Iter 2700: loss=206.2543, time=13.84s\n",
            "iter 2700: dev acc=0.2407\n",
            "Iter 2800: loss=235.3942, time=14.30s\n",
            "iter 2800: dev acc=0.2234\n",
            "Iter 2900: loss=203.5270, time=14.78s\n",
            "iter 2900: dev acc=0.2271\n",
            "Iter 3000: loss=202.4630, time=15.25s\n",
            "iter 3000: dev acc=0.2498\n",
            "Iter 3100: loss=190.3100, time=15.73s\n",
            "iter 3100: dev acc=0.2398\n",
            "Iter 3200: loss=181.5865, time=16.21s\n",
            "iter 3200: dev acc=0.2552\n",
            "Iter 3300: loss=207.3804, time=16.69s\n",
            "iter 3300: dev acc=0.2489\n",
            "Iter 3400: loss=219.6509, time=17.17s\n",
            "iter 3400: dev acc=0.2434\n",
            "Iter 3500: loss=203.8557, time=17.67s\n",
            "iter 3500: dev acc=0.2543\n",
            "Iter 3600: loss=208.6999, time=18.15s\n",
            "iter 3600: dev acc=0.2316\n",
            "Iter 3700: loss=224.2307, time=18.64s\n",
            "iter 3700: dev acc=0.2625\n",
            "Iter 3800: loss=189.3825, time=19.12s\n",
            "iter 3800: dev acc=0.2698\n",
            "Iter 3900: loss=170.5331, time=19.60s\n",
            "iter 3900: dev acc=0.2643\n",
            "Iter 4000: loss=178.6299, time=20.08s\n",
            "iter 4000: dev acc=0.2625\n",
            "Iter 4100: loss=186.2813, time=20.56s\n",
            "iter 4100: dev acc=0.2516\n",
            "Iter 4200: loss=207.5100, time=21.04s\n",
            "iter 4200: dev acc=0.2389\n",
            "Iter 4300: loss=195.1324, time=21.52s\n",
            "iter 4300: dev acc=0.2507\n",
            "Iter 4400: loss=200.7935, time=22.00s\n",
            "iter 4400: dev acc=0.2325\n",
            "Iter 4500: loss=194.8403, time=22.47s\n",
            "iter 4500: dev acc=0.2334\n",
            "Iter 4600: loss=196.4202, time=22.95s\n",
            "iter 4600: dev acc=0.2688\n",
            "Iter 4700: loss=195.2292, time=23.43s\n",
            "iter 4700: dev acc=0.2598\n",
            "Iter 4800: loss=171.9945, time=23.91s\n",
            "iter 4800: dev acc=0.2561\n",
            "Iter 4900: loss=194.8711, time=24.39s\n",
            "iter 4900: dev acc=0.2625\n",
            "Iter 5000: loss=203.6539, time=24.88s\n",
            "iter 5000: dev acc=0.2888\n",
            "new highscore\n",
            "Iter 5100: loss=188.0690, time=25.52s\n",
            "iter 5100: dev acc=0.2997\n",
            "new highscore\n",
            "Iter 5200: loss=205.2323, time=26.12s\n",
            "iter 5200: dev acc=0.2707\n",
            "Iter 5300: loss=191.5631, time=26.61s\n",
            "iter 5300: dev acc=0.3015\n",
            "new highscore\n",
            "Iter 5400: loss=179.9399, time=27.22s\n",
            "iter 5400: dev acc=0.2925\n",
            "Iter 5500: loss=198.8723, time=27.71s\n",
            "iter 5500: dev acc=0.2498\n",
            "Iter 5600: loss=193.7854, time=28.18s\n",
            "iter 5600: dev acc=0.2979\n",
            "Iter 5700: loss=171.8344, time=28.67s\n",
            "iter 5700: dev acc=0.2861\n",
            "Iter 5800: loss=210.3908, time=29.15s\n",
            "iter 5800: dev acc=0.2661\n",
            "Iter 5900: loss=182.4681, time=29.65s\n",
            "iter 5900: dev acc=0.2970\n",
            "Iter 6000: loss=185.8506, time=30.13s\n",
            "iter 6000: dev acc=0.2788\n",
            "Iter 6100: loss=191.3758, time=30.62s\n",
            "iter 6100: dev acc=0.3106\n",
            "new highscore\n",
            "Iter 6200: loss=193.9792, time=31.27s\n",
            "iter 6200: dev acc=0.2788\n",
            "Iter 6300: loss=203.3291, time=31.76s\n",
            "iter 6300: dev acc=0.2961\n",
            "Iter 6400: loss=203.7976, time=32.23s\n",
            "iter 6400: dev acc=0.2888\n",
            "Iter 6500: loss=187.2680, time=32.71s\n",
            "iter 6500: dev acc=0.3043\n",
            "Iter 6600: loss=191.0782, time=33.19s\n",
            "iter 6600: dev acc=0.3106\n",
            "Iter 6700: loss=193.2722, time=33.66s\n",
            "iter 6700: dev acc=0.2952\n",
            "Iter 6800: loss=181.9324, time=34.15s\n",
            "iter 6800: dev acc=0.2489\n",
            "Iter 6900: loss=189.7597, time=34.62s\n",
            "iter 6900: dev acc=0.2797\n",
            "Iter 7000: loss=170.2371, time=35.09s\n",
            "iter 7000: dev acc=0.3124\n",
            "new highscore\n",
            "Iter 7100: loss=211.0301, time=35.70s\n",
            "iter 7100: dev acc=0.2761\n",
            "Iter 7200: loss=178.1816, time=36.18s\n",
            "iter 7200: dev acc=0.2552\n",
            "Iter 7300: loss=186.6111, time=36.67s\n",
            "iter 7300: dev acc=0.3006\n",
            "Iter 7400: loss=221.4085, time=37.15s\n",
            "iter 7400: dev acc=0.2925\n",
            "Iter 7500: loss=173.1106, time=37.64s\n",
            "iter 7500: dev acc=0.2934\n",
            "Iter 7600: loss=170.7348, time=38.12s\n",
            "iter 7600: dev acc=0.3015\n",
            "Iter 7700: loss=176.2813, time=38.60s\n",
            "iter 7700: dev acc=0.2888\n",
            "Iter 7800: loss=171.7991, time=39.08s\n",
            "iter 7800: dev acc=0.2916\n",
            "Iter 7900: loss=200.9342, time=39.55s\n",
            "iter 7900: dev acc=0.2779\n",
            "Iter 8000: loss=178.5745, time=40.03s\n",
            "iter 8000: dev acc=0.2816\n",
            "Iter 8100: loss=181.6431, time=40.51s\n",
            "iter 8100: dev acc=0.2861\n",
            "Iter 8200: loss=205.5782, time=40.99s\n",
            "iter 8200: dev acc=0.2870\n",
            "Iter 8300: loss=194.4006, time=41.48s\n",
            "iter 8300: dev acc=0.3152\n",
            "new highscore\n",
            "Iter 8400: loss=179.8306, time=42.10s\n",
            "iter 8400: dev acc=0.2961\n",
            "Iter 8500: loss=151.9205, time=42.58s\n",
            "iter 8500: dev acc=0.2925\n",
            "Shuffling training data\n",
            "Iter 8600: loss=188.4052, time=43.06s\n",
            "iter 8600: dev acc=0.2988\n",
            "Iter 8700: loss=172.7599, time=43.54s\n",
            "iter 8700: dev acc=0.3034\n",
            "Iter 8800: loss=152.0237, time=44.02s\n",
            "iter 8800: dev acc=0.3025\n",
            "Iter 8900: loss=160.8961, time=44.49s\n",
            "iter 8900: dev acc=0.2934\n",
            "Iter 9000: loss=152.7384, time=44.96s\n",
            "iter 9000: dev acc=0.2797\n",
            "Iter 9100: loss=167.1568, time=45.44s\n",
            "iter 9100: dev acc=0.2979\n",
            "Iter 9200: loss=129.9841, time=45.92s\n",
            "iter 9200: dev acc=0.2925\n",
            "Iter 9300: loss=160.6924, time=46.40s\n",
            "iter 9300: dev acc=0.3279\n",
            "new highscore\n",
            "Iter 9400: loss=153.5447, time=47.05s\n",
            "iter 9400: dev acc=0.2952\n",
            "Iter 9500: loss=170.7462, time=47.53s\n",
            "iter 9500: dev acc=0.2988\n",
            "Iter 9600: loss=163.0974, time=48.01s\n",
            "iter 9600: dev acc=0.2979\n",
            "Iter 9700: loss=160.2454, time=48.49s\n",
            "iter 9700: dev acc=0.2988\n",
            "Iter 9800: loss=161.5997, time=48.97s\n",
            "iter 9800: dev acc=0.3070\n",
            "Iter 9900: loss=152.7474, time=49.44s\n",
            "iter 9900: dev acc=0.2943\n",
            "Iter 10000: loss=188.2109, time=49.92s\n",
            "iter 10000: dev acc=0.2988\n",
            "Iter 10100: loss=168.3795, time=50.40s\n",
            "iter 10100: dev acc=0.3025\n",
            "Iter 10200: loss=158.9497, time=50.88s\n",
            "iter 10200: dev acc=0.2952\n",
            "Iter 10300: loss=156.1399, time=51.36s\n",
            "iter 10300: dev acc=0.3143\n",
            "Iter 10400: loss=179.6420, time=51.84s\n",
            "iter 10400: dev acc=0.3161\n",
            "Iter 10500: loss=147.9716, time=52.31s\n",
            "iter 10500: dev acc=0.3233\n",
            "Iter 10600: loss=172.6939, time=52.80s\n",
            "iter 10600: dev acc=0.2925\n",
            "Iter 10700: loss=180.3116, time=53.28s\n",
            "iter 10700: dev acc=0.3052\n",
            "Iter 10800: loss=155.6528, time=53.75s\n",
            "iter 10800: dev acc=0.3015\n",
            "Iter 10900: loss=184.0400, time=54.22s\n",
            "iter 10900: dev acc=0.2934\n",
            "Iter 11000: loss=144.0829, time=54.70s\n",
            "iter 11000: dev acc=0.3006\n",
            "Iter 11100: loss=177.6973, time=55.17s\n",
            "iter 11100: dev acc=0.2979\n",
            "Iter 11200: loss=154.0870, time=55.65s\n",
            "iter 11200: dev acc=0.3043\n",
            "Iter 11300: loss=165.7456, time=56.12s\n",
            "iter 11300: dev acc=0.2843\n",
            "Iter 11400: loss=175.3767, time=56.60s\n",
            "iter 11400: dev acc=0.3070\n",
            "Iter 11500: loss=132.8707, time=57.08s\n",
            "iter 11500: dev acc=0.2661\n",
            "Iter 11600: loss=152.5555, time=57.55s\n",
            "iter 11600: dev acc=0.2870\n",
            "Iter 11700: loss=185.4649, time=58.03s\n",
            "iter 11700: dev acc=0.3043\n",
            "Iter 11800: loss=135.4727, time=58.51s\n",
            "iter 11800: dev acc=0.2988\n",
            "Iter 11900: loss=168.4740, time=58.98s\n",
            "iter 11900: dev acc=0.3088\n",
            "Iter 12000: loss=170.2574, time=59.46s\n",
            "iter 12000: dev acc=0.3161\n",
            "Iter 12100: loss=156.0492, time=59.93s\n",
            "iter 12100: dev acc=0.2961\n",
            "Iter 12200: loss=164.1266, time=60.41s\n",
            "iter 12200: dev acc=0.3115\n",
            "Iter 12300: loss=158.4967, time=60.89s\n",
            "iter 12300: dev acc=0.2879\n",
            "Iter 12400: loss=171.6274, time=61.36s\n",
            "iter 12400: dev acc=0.3270\n",
            "Iter 12500: loss=152.7740, time=61.84s\n",
            "iter 12500: dev acc=0.3288\n",
            "new highscore\n",
            "Iter 12600: loss=164.9657, time=62.46s\n",
            "iter 12600: dev acc=0.3143\n",
            "Iter 12700: loss=138.1028, time=62.94s\n",
            "iter 12700: dev acc=0.3315\n",
            "new highscore\n",
            "Iter 12800: loss=147.1649, time=63.58s\n",
            "iter 12800: dev acc=0.3124\n",
            "Iter 12900: loss=151.6274, time=64.06s\n",
            "iter 12900: dev acc=0.3088\n",
            "Iter 13000: loss=155.4740, time=64.58s\n",
            "iter 13000: dev acc=0.3015\n",
            "Iter 13100: loss=171.3839, time=65.05s\n",
            "iter 13100: dev acc=0.2979\n",
            "Iter 13200: loss=168.7438, time=65.52s\n",
            "iter 13200: dev acc=0.3043\n",
            "Iter 13300: loss=167.8199, time=65.99s\n",
            "iter 13300: dev acc=0.3043\n",
            "Iter 13400: loss=166.4234, time=66.46s\n",
            "iter 13400: dev acc=0.3088\n",
            "Iter 13500: loss=155.9060, time=66.95s\n",
            "iter 13500: dev acc=0.2852\n",
            "Iter 13600: loss=173.4841, time=67.43s\n",
            "iter 13600: dev acc=0.3015\n",
            "Iter 13700: loss=140.2762, time=67.91s\n",
            "iter 13700: dev acc=0.2979\n",
            "Iter 13800: loss=162.7037, time=68.38s\n",
            "iter 13800: dev acc=0.2652\n",
            "Iter 13900: loss=152.9645, time=68.86s\n",
            "iter 13900: dev acc=0.2734\n",
            "Iter 14000: loss=142.6398, time=69.34s\n",
            "iter 14000: dev acc=0.2888\n",
            "Iter 14100: loss=160.5657, time=69.82s\n",
            "iter 14100: dev acc=0.3015\n",
            "Iter 14200: loss=169.4635, time=70.30s\n",
            "iter 14200: dev acc=0.2834\n",
            "Iter 14300: loss=166.9468, time=70.78s\n",
            "iter 14300: dev acc=0.2934\n",
            "Iter 14400: loss=156.8433, time=71.26s\n",
            "iter 14400: dev acc=0.3124\n",
            "Iter 14500: loss=144.1072, time=71.74s\n",
            "iter 14500: dev acc=0.3188\n",
            "Iter 14600: loss=177.2668, time=72.21s\n",
            "iter 14600: dev acc=0.2952\n",
            "Iter 14700: loss=144.1953, time=72.69s\n",
            "iter 14700: dev acc=0.2834\n",
            "Iter 14800: loss=176.0385, time=73.16s\n",
            "iter 14800: dev acc=0.3079\n",
            "Iter 14900: loss=143.9138, time=73.64s\n",
            "iter 14900: dev acc=0.2979\n",
            "Iter 15000: loss=157.6857, time=74.11s\n",
            "iter 15000: dev acc=0.2925\n",
            "Iter 15100: loss=182.2952, time=74.58s\n",
            "iter 15100: dev acc=0.2825\n",
            "Iter 15200: loss=162.3907, time=75.07s\n",
            "iter 15200: dev acc=0.2970\n",
            "Iter 15300: loss=158.3632, time=75.55s\n",
            "iter 15300: dev acc=0.2997\n",
            "Iter 15400: loss=164.7217, time=76.03s\n",
            "iter 15400: dev acc=0.3079\n",
            "Iter 15500: loss=152.9980, time=76.50s\n",
            "iter 15500: dev acc=0.3088\n",
            "Iter 15600: loss=175.9267, time=76.98s\n",
            "iter 15600: dev acc=0.3052\n",
            "Iter 15700: loss=158.9310, time=77.46s\n",
            "iter 15700: dev acc=0.3088\n",
            "Iter 15800: loss=156.5455, time=77.93s\n",
            "iter 15800: dev acc=0.3161\n",
            "Iter 15900: loss=168.1294, time=78.40s\n",
            "iter 15900: dev acc=0.3124\n",
            "Iter 16000: loss=166.8062, time=78.88s\n",
            "iter 16000: dev acc=0.2997\n",
            "Iter 16100: loss=164.4956, time=79.36s\n",
            "iter 16100: dev acc=0.3115\n",
            "Iter 16200: loss=170.8098, time=79.84s\n",
            "iter 16200: dev acc=0.3034\n",
            "Iter 16300: loss=140.6130, time=80.31s\n",
            "iter 16300: dev acc=0.2934\n",
            "Iter 16400: loss=152.2672, time=80.79s\n",
            "iter 16400: dev acc=0.3097\n",
            "Iter 16500: loss=158.6684, time=81.27s\n",
            "iter 16500: dev acc=0.3061\n",
            "Iter 16600: loss=161.0714, time=81.76s\n",
            "iter 16600: dev acc=0.3106\n",
            "Iter 16700: loss=188.6812, time=82.23s\n",
            "iter 16700: dev acc=0.3052\n",
            "Iter 16800: loss=159.4600, time=82.71s\n",
            "iter 16800: dev acc=0.3043\n",
            "Iter 16900: loss=165.3215, time=83.20s\n",
            "iter 16900: dev acc=0.3106\n",
            "Iter 17000: loss=155.1861, time=83.68s\n",
            "iter 17000: dev acc=0.3215\n",
            "Shuffling training data\n",
            "Iter 17100: loss=142.6512, time=84.15s\n",
            "iter 17100: dev acc=0.3215\n",
            "Iter 17200: loss=120.3365, time=84.63s\n",
            "iter 17200: dev acc=0.3188\n",
            "Iter 17300: loss=122.7689, time=85.10s\n",
            "iter 17300: dev acc=0.3179\n",
            "Iter 17400: loss=122.2364, time=85.58s\n",
            "iter 17400: dev acc=0.3170\n",
            "Iter 17500: loss=119.7032, time=86.06s\n",
            "iter 17500: dev acc=0.3270\n",
            "Iter 17600: loss=135.1747, time=86.54s\n",
            "iter 17600: dev acc=0.2997\n",
            "Iter 17700: loss=117.5135, time=87.01s\n",
            "iter 17700: dev acc=0.3206\n",
            "Iter 17800: loss=136.8573, time=87.49s\n",
            "iter 17800: dev acc=0.3152\n",
            "Iter 17900: loss=90.6195, time=87.98s\n",
            "iter 17900: dev acc=0.3279\n",
            "Iter 18000: loss=114.7166, time=88.46s\n",
            "iter 18000: dev acc=0.3270\n",
            "Iter 18100: loss=130.0018, time=88.93s\n",
            "iter 18100: dev acc=0.3134\n",
            "Iter 18200: loss=125.1783, time=89.41s\n",
            "iter 18200: dev acc=0.3115\n",
            "Iter 18300: loss=118.5531, time=89.87s\n",
            "iter 18300: dev acc=0.3261\n",
            "Iter 18400: loss=140.6921, time=90.34s\n",
            "iter 18400: dev acc=0.2879\n",
            "Iter 18500: loss=108.4218, time=90.82s\n",
            "iter 18500: dev acc=0.3243\n",
            "Iter 18600: loss=148.5962, time=91.29s\n",
            "iter 18600: dev acc=0.3079\n",
            "Iter 18700: loss=117.5707, time=91.77s\n",
            "iter 18700: dev acc=0.2834\n",
            "Iter 18800: loss=126.7834, time=92.24s\n",
            "iter 18800: dev acc=0.3052\n",
            "Iter 18900: loss=116.3995, time=92.71s\n",
            "iter 18900: dev acc=0.3243\n",
            "Iter 19000: loss=148.3411, time=93.19s\n",
            "iter 19000: dev acc=0.3025\n",
            "Iter 19100: loss=148.1708, time=93.67s\n",
            "iter 19100: dev acc=0.2916\n",
            "Iter 19200: loss=147.7127, time=94.14s\n",
            "iter 19200: dev acc=0.3025\n",
            "Iter 19300: loss=135.3005, time=94.62s\n",
            "iter 19300: dev acc=0.3061\n",
            "Iter 19400: loss=145.9867, time=95.09s\n",
            "iter 19400: dev acc=0.3315\n",
            "Iter 19500: loss=146.2087, time=95.57s\n",
            "iter 19500: dev acc=0.3370\n",
            "new highscore\n",
            "Iter 19600: loss=119.7024, time=96.19s\n",
            "iter 19600: dev acc=0.3143\n",
            "Iter 19700: loss=141.0487, time=96.67s\n",
            "iter 19700: dev acc=0.3188\n",
            "Iter 19800: loss=150.0389, time=97.14s\n",
            "iter 19800: dev acc=0.3315\n",
            "Iter 19900: loss=132.3496, time=97.62s\n",
            "iter 19900: dev acc=0.3197\n",
            "Iter 20000: loss=124.0005, time=98.11s\n",
            "iter 20000: dev acc=0.2970\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-07f34c5b57a1>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 19500: train acc=0.5652, dev acc=0.3370, test acc=0.3561\n",
            "Best Test Accuracy for seed 42: 0.3561\n",
            "Shuffling training data\n",
            "Iter 100: loss=340.0605, time=0.14s\n",
            "iter 100: dev acc=0.2144\n",
            "new highscore\n",
            "Iter 200: loss=283.7693, time=0.76s\n",
            "iter 200: dev acc=0.2035\n",
            "Iter 300: loss=266.3839, time=1.24s\n",
            "iter 300: dev acc=0.2089\n",
            "Iter 400: loss=312.4178, time=1.71s\n",
            "iter 400: dev acc=0.2035\n",
            "Iter 500: loss=285.2600, time=2.19s\n",
            "iter 500: dev acc=0.2225\n",
            "new highscore\n",
            "Iter 600: loss=226.9294, time=2.84s\n",
            "iter 600: dev acc=0.2080\n",
            "Iter 700: loss=220.8995, time=3.31s\n",
            "iter 700: dev acc=0.2271\n",
            "new highscore\n",
            "Iter 800: loss=285.7745, time=3.95s\n",
            "iter 800: dev acc=0.2062\n",
            "Iter 900: loss=256.4001, time=4.43s\n",
            "iter 900: dev acc=0.2325\n",
            "new highscore\n",
            "Iter 1000: loss=258.4729, time=5.08s\n",
            "iter 1000: dev acc=0.2371\n",
            "new highscore\n",
            "Iter 1100: loss=255.0116, time=5.71s\n",
            "iter 1100: dev acc=0.2316\n",
            "Iter 1200: loss=256.5278, time=6.19s\n",
            "iter 1200: dev acc=0.2461\n",
            "new highscore\n",
            "Iter 1300: loss=203.2188, time=6.81s\n",
            "iter 1300: dev acc=0.2725\n",
            "new highscore\n",
            "Iter 1400: loss=204.4021, time=7.44s\n",
            "iter 1400: dev acc=0.2552\n",
            "Iter 1500: loss=236.2385, time=7.92s\n",
            "iter 1500: dev acc=0.2189\n",
            "Iter 1600: loss=230.2265, time=8.40s\n",
            "iter 1600: dev acc=0.2334\n",
            "Iter 1700: loss=229.9900, time=8.88s\n",
            "iter 1700: dev acc=0.2380\n",
            "Iter 1800: loss=206.1899, time=9.36s\n",
            "iter 1800: dev acc=0.2579\n",
            "Iter 1900: loss=209.2705, time=9.84s\n",
            "iter 1900: dev acc=0.2743\n",
            "new highscore\n",
            "Iter 2000: loss=208.5092, time=10.45s\n",
            "iter 2000: dev acc=0.2380\n",
            "Iter 2100: loss=214.7223, time=10.93s\n",
            "iter 2100: dev acc=0.2698\n",
            "Iter 2200: loss=231.2222, time=11.41s\n",
            "iter 2200: dev acc=0.2507\n",
            "Iter 2300: loss=213.6451, time=11.90s\n",
            "iter 2300: dev acc=0.2634\n",
            "Iter 2400: loss=185.4891, time=12.38s\n",
            "iter 2400: dev acc=0.2698\n",
            "Iter 2500: loss=217.3980, time=12.85s\n",
            "iter 2500: dev acc=0.2634\n",
            "Iter 2600: loss=205.1176, time=13.33s\n",
            "iter 2600: dev acc=0.2589\n",
            "Iter 2700: loss=217.3809, time=13.81s\n",
            "iter 2700: dev acc=0.2398\n",
            "Iter 2800: loss=211.6901, time=14.29s\n",
            "iter 2800: dev acc=0.2352\n",
            "Iter 2900: loss=221.1407, time=14.76s\n",
            "iter 2900: dev acc=0.2652\n",
            "Iter 3000: loss=200.8084, time=15.24s\n",
            "iter 3000: dev acc=0.2098\n",
            "Iter 3100: loss=198.7020, time=15.72s\n",
            "iter 3100: dev acc=0.2198\n",
            "Iter 3200: loss=205.7882, time=16.19s\n",
            "iter 3200: dev acc=0.2489\n",
            "Iter 3300: loss=189.3529, time=16.67s\n",
            "iter 3300: dev acc=0.2534\n",
            "Iter 3400: loss=209.6851, time=17.14s\n",
            "iter 3400: dev acc=0.2470\n",
            "Iter 3500: loss=211.7819, time=17.63s\n",
            "iter 3500: dev acc=0.2461\n",
            "Iter 3600: loss=209.1351, time=18.11s\n",
            "iter 3600: dev acc=0.2643\n",
            "Iter 3700: loss=211.9526, time=18.59s\n",
            "iter 3700: dev acc=0.2770\n",
            "new highscore\n",
            "Iter 3800: loss=222.7208, time=19.24s\n",
            "iter 3800: dev acc=0.2725\n",
            "Iter 3900: loss=181.9085, time=19.72s\n",
            "iter 3900: dev acc=0.2498\n",
            "Iter 4000: loss=177.9197, time=20.21s\n",
            "iter 4000: dev acc=0.3006\n",
            "new highscore\n",
            "Iter 4100: loss=188.2427, time=20.83s\n",
            "iter 4100: dev acc=0.2734\n",
            "Iter 4200: loss=216.9991, time=21.32s\n",
            "iter 4200: dev acc=0.2688\n",
            "Iter 4300: loss=197.4101, time=21.80s\n",
            "iter 4300: dev acc=0.2725\n",
            "Iter 4400: loss=193.2575, time=22.28s\n",
            "iter 4400: dev acc=0.2652\n",
            "Iter 4500: loss=197.6924, time=22.76s\n",
            "iter 4500: dev acc=0.2825\n",
            "Iter 4600: loss=199.7090, time=23.23s\n",
            "iter 4600: dev acc=0.2852\n",
            "Iter 4700: loss=198.7135, time=23.75s\n",
            "iter 4700: dev acc=0.2779\n",
            "Iter 4800: loss=187.8933, time=24.22s\n",
            "iter 4800: dev acc=0.2816\n",
            "Iter 4900: loss=193.3597, time=24.69s\n",
            "iter 4900: dev acc=0.2825\n",
            "Iter 5000: loss=194.5622, time=25.17s\n",
            "iter 5000: dev acc=0.2589\n",
            "Iter 5100: loss=183.1315, time=25.65s\n",
            "iter 5100: dev acc=0.3006\n",
            "Iter 5200: loss=219.5929, time=26.12s\n",
            "iter 5200: dev acc=0.2825\n",
            "Iter 5300: loss=195.5716, time=26.60s\n",
            "iter 5300: dev acc=0.2652\n",
            "Iter 5400: loss=211.6993, time=27.07s\n",
            "iter 5400: dev acc=0.2661\n",
            "Iter 5500: loss=216.5645, time=27.56s\n",
            "iter 5500: dev acc=0.2807\n",
            "Iter 5600: loss=165.6847, time=28.04s\n",
            "iter 5600: dev acc=0.2897\n",
            "Iter 5700: loss=190.1441, time=28.51s\n",
            "iter 5700: dev acc=0.2816\n",
            "Iter 5800: loss=194.2236, time=28.99s\n",
            "iter 5800: dev acc=0.2534\n",
            "Iter 5900: loss=169.2248, time=29.47s\n",
            "iter 5900: dev acc=0.2661\n",
            "Iter 6000: loss=196.1466, time=29.95s\n",
            "iter 6000: dev acc=0.2825\n",
            "Iter 6100: loss=182.9390, time=30.43s\n",
            "iter 6100: dev acc=0.2716\n",
            "Iter 6200: loss=207.2197, time=30.92s\n",
            "iter 6200: dev acc=0.2897\n",
            "Iter 6300: loss=188.0728, time=31.41s\n",
            "iter 6300: dev acc=0.2825\n",
            "Iter 6400: loss=183.5247, time=31.91s\n",
            "iter 6400: dev acc=0.2770\n",
            "Iter 6500: loss=210.4034, time=32.39s\n",
            "iter 6500: dev acc=0.2925\n",
            "Iter 6600: loss=179.5654, time=32.87s\n",
            "iter 6600: dev acc=0.2652\n",
            "Iter 6700: loss=198.1503, time=33.35s\n",
            "iter 6700: dev acc=0.2934\n",
            "Iter 6800: loss=173.6591, time=33.83s\n",
            "iter 6800: dev acc=0.2725\n",
            "Iter 6900: loss=191.3426, time=34.31s\n",
            "iter 6900: dev acc=0.2897\n",
            "Iter 7000: loss=187.9872, time=34.78s\n",
            "iter 7000: dev acc=0.2643\n",
            "Iter 7100: loss=174.9649, time=35.26s\n",
            "iter 7100: dev acc=0.2743\n",
            "Iter 7200: loss=148.5035, time=35.73s\n",
            "iter 7200: dev acc=0.2670\n",
            "Iter 7300: loss=189.5285, time=36.20s\n",
            "iter 7300: dev acc=0.2734\n",
            "Iter 7400: loss=144.8550, time=36.68s\n",
            "iter 7400: dev acc=0.2816\n",
            "Iter 7500: loss=193.8668, time=37.16s\n",
            "iter 7500: dev acc=0.2906\n",
            "Iter 7600: loss=201.7920, time=37.63s\n",
            "iter 7600: dev acc=0.2825\n",
            "Iter 7700: loss=188.5015, time=38.11s\n",
            "iter 7700: dev acc=0.3006\n",
            "Iter 7800: loss=184.1319, time=38.58s\n",
            "iter 7800: dev acc=0.2698\n",
            "Iter 7900: loss=188.0356, time=39.07s\n",
            "iter 7900: dev acc=0.2843\n",
            "Iter 8000: loss=194.8962, time=39.55s\n",
            "iter 8000: dev acc=0.3015\n",
            "new highscore\n",
            "Iter 8100: loss=178.9224, time=40.18s\n",
            "iter 8100: dev acc=0.3179\n",
            "new highscore\n",
            "Iter 8200: loss=182.5615, time=40.81s\n",
            "iter 8200: dev acc=0.2816\n",
            "Iter 8300: loss=184.4047, time=41.30s\n",
            "iter 8300: dev acc=0.2916\n",
            "Iter 8400: loss=186.3541, time=41.78s\n",
            "iter 8400: dev acc=0.2879\n",
            "Iter 8500: loss=183.0047, time=42.27s\n",
            "iter 8500: dev acc=0.2797\n",
            "Shuffling training data\n",
            "Iter 8600: loss=191.4168, time=42.74s\n",
            "iter 8600: dev acc=0.2843\n",
            "Iter 8700: loss=139.0279, time=43.23s\n",
            "iter 8700: dev acc=0.2897\n",
            "Iter 8800: loss=168.0486, time=43.72s\n",
            "iter 8800: dev acc=0.2934\n",
            "Iter 8900: loss=160.0363, time=44.19s\n",
            "iter 8900: dev acc=0.2643\n",
            "Iter 9000: loss=168.6603, time=44.67s\n",
            "iter 9000: dev acc=0.2961\n",
            "Iter 9100: loss=164.5931, time=45.14s\n",
            "iter 9100: dev acc=0.2834\n",
            "Iter 9200: loss=155.4081, time=45.63s\n",
            "iter 9200: dev acc=0.2879\n",
            "Iter 9300: loss=165.4472, time=46.11s\n",
            "iter 9300: dev acc=0.2916\n",
            "Iter 9400: loss=167.1598, time=46.58s\n",
            "iter 9400: dev acc=0.2897\n",
            "Iter 9500: loss=165.4378, time=47.06s\n",
            "iter 9500: dev acc=0.2716\n",
            "Iter 9600: loss=178.5636, time=47.54s\n",
            "iter 9600: dev acc=0.2834\n",
            "Iter 9700: loss=143.9641, time=48.01s\n",
            "iter 9700: dev acc=0.2970\n",
            "Iter 9800: loss=172.9799, time=48.48s\n",
            "iter 9800: dev acc=0.2879\n",
            "Iter 9900: loss=185.4205, time=48.96s\n",
            "iter 9900: dev acc=0.2897\n",
            "Iter 10000: loss=154.3048, time=49.43s\n",
            "iter 10000: dev acc=0.3043\n",
            "Iter 10100: loss=166.5304, time=49.91s\n",
            "iter 10100: dev acc=0.2807\n",
            "Iter 10200: loss=179.5859, time=50.39s\n",
            "iter 10200: dev acc=0.2634\n",
            "Iter 10300: loss=162.6272, time=50.86s\n",
            "iter 10300: dev acc=0.2807\n",
            "Iter 10400: loss=153.8709, time=51.34s\n",
            "iter 10400: dev acc=0.2970\n",
            "Iter 10500: loss=167.4172, time=51.82s\n",
            "iter 10500: dev acc=0.3034\n",
            "Iter 10600: loss=161.6195, time=52.30s\n",
            "iter 10600: dev acc=0.2879\n",
            "Iter 10700: loss=143.1309, time=52.78s\n",
            "iter 10700: dev acc=0.3006\n",
            "Iter 10800: loss=131.6617, time=53.25s\n",
            "iter 10800: dev acc=0.2916\n",
            "Iter 10900: loss=148.5502, time=53.75s\n",
            "iter 10900: dev acc=0.3179\n",
            "Iter 11000: loss=166.9223, time=54.23s\n",
            "iter 11000: dev acc=0.2734\n",
            "Iter 11100: loss=166.6272, time=54.72s\n",
            "iter 11100: dev acc=0.2797\n",
            "Iter 11200: loss=183.7886, time=55.22s\n",
            "iter 11200: dev acc=0.2634\n",
            "Iter 11300: loss=185.7352, time=55.70s\n",
            "iter 11300: dev acc=0.2779\n",
            "Iter 11400: loss=177.0331, time=56.19s\n",
            "iter 11400: dev acc=0.2925\n",
            "Iter 11500: loss=139.5887, time=56.68s\n",
            "iter 11500: dev acc=0.3070\n",
            "Iter 11600: loss=156.3271, time=57.16s\n",
            "iter 11600: dev acc=0.2988\n",
            "Iter 11700: loss=168.3138, time=57.65s\n",
            "iter 11700: dev acc=0.2888\n",
            "Iter 11800: loss=194.2694, time=58.13s\n",
            "iter 11800: dev acc=0.2906\n",
            "Iter 11900: loss=174.5886, time=58.62s\n",
            "iter 11900: dev acc=0.3034\n",
            "Iter 12000: loss=149.8912, time=59.11s\n",
            "iter 12000: dev acc=0.2816\n",
            "Iter 12100: loss=180.8093, time=59.59s\n",
            "iter 12100: dev acc=0.2934\n",
            "Iter 12200: loss=168.1084, time=60.07s\n",
            "iter 12200: dev acc=0.2961\n",
            "Iter 12300: loss=164.8847, time=60.55s\n",
            "iter 12300: dev acc=0.2761\n",
            "Iter 12400: loss=159.6685, time=61.03s\n",
            "iter 12400: dev acc=0.2943\n",
            "Iter 12500: loss=175.7740, time=61.51s\n",
            "iter 12500: dev acc=0.2743\n",
            "Iter 12600: loss=145.1234, time=61.99s\n",
            "iter 12600: dev acc=0.3124\n",
            "Iter 12700: loss=151.4665, time=62.48s\n",
            "iter 12700: dev acc=0.2698\n",
            "Iter 12800: loss=156.0520, time=62.96s\n",
            "iter 12800: dev acc=0.2716\n",
            "Iter 12900: loss=155.3343, time=63.45s\n",
            "iter 12900: dev acc=0.2707\n",
            "Iter 13000: loss=165.5671, time=63.93s\n",
            "iter 13000: dev acc=0.2897\n",
            "Iter 13100: loss=166.8440, time=64.40s\n",
            "iter 13100: dev acc=0.2852\n",
            "Iter 13200: loss=144.7668, time=64.88s\n",
            "iter 13200: dev acc=0.2797\n",
            "Iter 13300: loss=154.5134, time=65.37s\n",
            "iter 13300: dev acc=0.2834\n",
            "Iter 13400: loss=176.4008, time=65.85s\n",
            "iter 13400: dev acc=0.2888\n",
            "Iter 13500: loss=154.1513, time=66.33s\n",
            "iter 13500: dev acc=0.2916\n",
            "Iter 13600: loss=158.7029, time=66.81s\n",
            "iter 13600: dev acc=0.2943\n",
            "Iter 13700: loss=186.9202, time=67.29s\n",
            "iter 13700: dev acc=0.3079\n",
            "Iter 13800: loss=143.1321, time=67.77s\n",
            "iter 13800: dev acc=0.2870\n",
            "Iter 13900: loss=149.7578, time=68.24s\n",
            "iter 13900: dev acc=0.2925\n",
            "Iter 14000: loss=157.9520, time=68.72s\n",
            "iter 14000: dev acc=0.3015\n",
            "Iter 14100: loss=160.6538, time=69.19s\n",
            "iter 14100: dev acc=0.3034\n",
            "Iter 14200: loss=146.2080, time=69.68s\n",
            "iter 14200: dev acc=0.3079\n",
            "Iter 14300: loss=160.9510, time=70.16s\n",
            "iter 14300: dev acc=0.2870\n",
            "Iter 14400: loss=173.7020, time=70.64s\n",
            "iter 14400: dev acc=0.2816\n",
            "Iter 14500: loss=150.1231, time=71.12s\n",
            "iter 14500: dev acc=0.2970\n",
            "Iter 14600: loss=168.1314, time=71.60s\n",
            "iter 14600: dev acc=0.3143\n",
            "Iter 14700: loss=174.1152, time=72.08s\n",
            "iter 14700: dev acc=0.3115\n",
            "Iter 14800: loss=150.2493, time=72.55s\n",
            "iter 14800: dev acc=0.2988\n",
            "Iter 14900: loss=171.0203, time=73.03s\n",
            "iter 14900: dev acc=0.3161\n",
            "Iter 15000: loss=149.7870, time=73.51s\n",
            "iter 15000: dev acc=0.2861\n",
            "Iter 15100: loss=138.0860, time=73.99s\n",
            "iter 15100: dev acc=0.2961\n",
            "Iter 15200: loss=130.9379, time=74.47s\n",
            "iter 15200: dev acc=0.3043\n",
            "Iter 15300: loss=148.0065, time=74.95s\n",
            "iter 15300: dev acc=0.2943\n",
            "Iter 15400: loss=177.5944, time=75.43s\n",
            "iter 15400: dev acc=0.2788\n",
            "Iter 15500: loss=201.9173, time=75.90s\n",
            "iter 15500: dev acc=0.2861\n",
            "Iter 15600: loss=146.7927, time=76.37s\n",
            "iter 15600: dev acc=0.3025\n",
            "Iter 15700: loss=166.6579, time=76.86s\n",
            "iter 15700: dev acc=0.3261\n",
            "new highscore\n",
            "Iter 15800: loss=172.6204, time=77.49s\n",
            "iter 15800: dev acc=0.3124\n",
            "Iter 15900: loss=170.1522, time=77.99s\n",
            "iter 15900: dev acc=0.2961\n",
            "Iter 16000: loss=143.7441, time=78.47s\n",
            "iter 16000: dev acc=0.3015\n",
            "Iter 16100: loss=163.2325, time=78.95s\n",
            "iter 16100: dev acc=0.3006\n",
            "Iter 16200: loss=181.7628, time=79.43s\n",
            "iter 16200: dev acc=0.2979\n",
            "Iter 16300: loss=146.4187, time=79.90s\n",
            "iter 16300: dev acc=0.2834\n",
            "Iter 16400: loss=157.7838, time=80.38s\n",
            "iter 16400: dev acc=0.2852\n",
            "Iter 16500: loss=191.8488, time=80.86s\n",
            "iter 16500: dev acc=0.2906\n",
            "Iter 16600: loss=178.2432, time=81.32s\n",
            "iter 16600: dev acc=0.3061\n",
            "Iter 16700: loss=176.7658, time=81.80s\n",
            "iter 16700: dev acc=0.2779\n",
            "Iter 16800: loss=186.6155, time=82.27s\n",
            "iter 16800: dev acc=0.2925\n",
            "Iter 16900: loss=176.8569, time=82.75s\n",
            "iter 16900: dev acc=0.3070\n",
            "Iter 17000: loss=151.8116, time=83.22s\n",
            "iter 17000: dev acc=0.2979\n",
            "Shuffling training data\n",
            "Iter 17100: loss=163.7332, time=83.71s\n",
            "iter 17100: dev acc=0.3124\n",
            "Iter 17200: loss=115.6382, time=84.18s\n",
            "iter 17200: dev acc=0.3233\n",
            "Iter 17300: loss=133.8020, time=84.66s\n",
            "iter 17300: dev acc=0.3124\n",
            "Iter 17400: loss=131.7503, time=85.18s\n",
            "iter 17400: dev acc=0.3061\n",
            "Iter 17500: loss=128.1404, time=85.65s\n",
            "iter 17500: dev acc=0.2925\n",
            "Iter 17600: loss=122.7530, time=86.13s\n",
            "iter 17600: dev acc=0.3061\n",
            "Iter 17700: loss=143.4642, time=86.60s\n",
            "iter 17700: dev acc=0.3097\n",
            "Iter 17800: loss=152.5680, time=87.08s\n",
            "iter 17800: dev acc=0.2925\n",
            "Iter 17900: loss=120.6447, time=87.56s\n",
            "iter 17900: dev acc=0.3152\n",
            "Iter 18000: loss=122.3429, time=88.04s\n",
            "iter 18000: dev acc=0.2997\n",
            "Iter 18100: loss=119.8490, time=88.53s\n",
            "iter 18100: dev acc=0.3152\n",
            "Iter 18200: loss=140.6097, time=89.00s\n",
            "iter 18200: dev acc=0.3088\n",
            "Iter 18300: loss=150.3997, time=89.48s\n",
            "iter 18300: dev acc=0.3197\n",
            "Iter 18400: loss=129.1011, time=89.98s\n",
            "iter 18400: dev acc=0.3179\n",
            "Iter 18500: loss=120.7519, time=90.46s\n",
            "iter 18500: dev acc=0.3143\n",
            "Iter 18600: loss=105.3766, time=90.94s\n",
            "iter 18600: dev acc=0.3252\n",
            "Iter 18700: loss=109.2444, time=91.42s\n",
            "iter 18700: dev acc=0.3342\n",
            "new highscore\n",
            "Iter 18800: loss=115.1011, time=92.04s\n",
            "iter 18800: dev acc=0.3124\n",
            "Iter 18900: loss=147.0352, time=92.52s\n",
            "iter 18900: dev acc=0.3088\n",
            "Iter 19000: loss=121.1227, time=93.00s\n",
            "iter 19000: dev acc=0.3070\n",
            "Iter 19100: loss=131.7938, time=93.48s\n",
            "iter 19100: dev acc=0.3070\n",
            "Iter 19200: loss=141.6506, time=93.97s\n",
            "iter 19200: dev acc=0.3215\n",
            "Iter 19300: loss=117.3294, time=94.44s\n",
            "iter 19300: dev acc=0.3215\n",
            "Iter 19400: loss=152.3370, time=94.91s\n",
            "iter 19400: dev acc=0.3124\n",
            "Iter 19500: loss=126.5882, time=95.39s\n",
            "iter 19500: dev acc=0.3406\n",
            "new highscore\n",
            "Iter 19600: loss=111.2507, time=96.03s\n",
            "iter 19600: dev acc=0.3179\n",
            "Iter 19700: loss=128.0290, time=96.51s\n",
            "iter 19700: dev acc=0.3115\n",
            "Iter 19800: loss=141.8174, time=96.99s\n",
            "iter 19800: dev acc=0.3243\n",
            "Iter 19900: loss=114.4549, time=97.47s\n",
            "iter 19900: dev acc=0.3215\n",
            "Iter 20000: loss=129.5278, time=97.95s\n",
            "iter 20000: dev acc=0.3252\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 19500: train acc=0.5756, dev acc=0.3406, test acc=0.3262\n",
            "Best Test Accuracy for seed 123: 0.3262\n",
            "Shuffling training data\n",
            "Iter 100: loss=327.9038, time=0.14s\n",
            "iter 100: dev acc=0.2198\n",
            "new highscore\n",
            "Iter 200: loss=296.2119, time=0.75s\n",
            "iter 200: dev acc=0.2216\n",
            "new highscore\n",
            "Iter 300: loss=301.1189, time=1.37s\n",
            "iter 300: dev acc=0.2316\n",
            "new highscore\n",
            "Iter 400: loss=274.8144, time=1.99s\n",
            "iter 400: dev acc=0.2343\n",
            "new highscore\n",
            "Iter 500: loss=286.7929, time=2.60s\n",
            "iter 500: dev acc=0.2498\n",
            "new highscore\n",
            "Iter 600: loss=262.3672, time=3.21s\n",
            "iter 600: dev acc=0.2380\n",
            "Iter 700: loss=267.8502, time=3.69s\n",
            "iter 700: dev acc=0.2461\n",
            "Iter 800: loss=263.3237, time=4.17s\n",
            "iter 800: dev acc=0.2289\n",
            "Iter 900: loss=227.9012, time=4.65s\n",
            "iter 900: dev acc=0.2570\n",
            "new highscore\n",
            "Iter 1000: loss=238.8843, time=5.28s\n",
            "iter 1000: dev acc=0.2452\n",
            "Iter 1100: loss=256.8103, time=5.76s\n",
            "iter 1100: dev acc=0.2779\n",
            "new highscore\n",
            "Iter 1200: loss=229.2575, time=6.40s\n",
            "iter 1200: dev acc=0.2525\n",
            "Iter 1300: loss=219.1369, time=6.87s\n",
            "iter 1300: dev acc=0.2788\n",
            "new highscore\n",
            "Iter 1400: loss=200.2365, time=7.51s\n",
            "iter 1400: dev acc=0.2570\n",
            "Iter 1500: loss=231.5285, time=7.98s\n",
            "iter 1500: dev acc=0.2607\n",
            "Iter 1600: loss=178.9611, time=8.45s\n",
            "iter 1600: dev acc=0.2807\n",
            "new highscore\n",
            "Iter 1700: loss=227.2254, time=9.08s\n",
            "iter 1700: dev acc=0.2534\n",
            "Iter 1800: loss=227.2037, time=9.56s\n",
            "iter 1800: dev acc=0.2779\n",
            "Iter 1900: loss=217.4825, time=10.05s\n",
            "iter 1900: dev acc=0.2725\n",
            "Iter 2000: loss=228.2260, time=10.53s\n",
            "iter 2000: dev acc=0.2843\n",
            "new highscore\n",
            "Iter 2100: loss=201.4892, time=11.14s\n",
            "iter 2100: dev acc=0.2761\n",
            "Iter 2200: loss=244.0873, time=11.64s\n",
            "iter 2200: dev acc=0.2743\n",
            "Iter 2300: loss=222.6698, time=12.13s\n",
            "iter 2300: dev acc=0.2643\n",
            "Iter 2400: loss=199.0314, time=12.63s\n",
            "iter 2400: dev acc=0.2707\n",
            "Iter 2500: loss=203.5473, time=13.12s\n",
            "iter 2500: dev acc=0.2670\n",
            "Iter 2600: loss=227.0461, time=13.63s\n",
            "iter 2600: dev acc=0.2679\n",
            "Iter 2700: loss=185.6865, time=14.12s\n",
            "iter 2700: dev acc=0.2707\n",
            "Iter 2800: loss=208.4507, time=14.61s\n",
            "iter 2800: dev acc=0.2670\n",
            "Iter 2900: loss=197.0831, time=15.10s\n",
            "iter 2900: dev acc=0.2879\n",
            "new highscore\n",
            "Iter 3000: loss=203.7468, time=15.74s\n",
            "iter 3000: dev acc=0.2707\n",
            "Iter 3100: loss=203.2153, time=16.23s\n",
            "iter 3100: dev acc=0.2979\n",
            "new highscore\n",
            "Iter 3200: loss=194.7086, time=16.87s\n",
            "iter 3200: dev acc=0.2916\n",
            "Iter 3300: loss=180.6603, time=17.37s\n",
            "iter 3300: dev acc=0.2979\n",
            "Iter 3400: loss=204.8016, time=17.86s\n",
            "iter 3400: dev acc=0.2934\n",
            "Iter 3500: loss=185.5469, time=18.35s\n",
            "iter 3500: dev acc=0.3279\n",
            "new highscore\n",
            "Iter 3600: loss=233.3197, time=18.99s\n",
            "iter 3600: dev acc=0.2897\n",
            "Iter 3700: loss=211.2782, time=19.47s\n",
            "iter 3700: dev acc=0.2879\n",
            "Iter 3800: loss=201.4035, time=19.95s\n",
            "iter 3800: dev acc=0.3052\n",
            "Iter 3900: loss=258.9836, time=20.43s\n",
            "iter 3900: dev acc=0.3170\n",
            "Iter 4000: loss=166.4266, time=20.91s\n",
            "iter 4000: dev acc=0.3188\n",
            "Iter 4100: loss=162.4939, time=21.40s\n",
            "iter 4100: dev acc=0.3233\n",
            "Iter 4200: loss=195.2516, time=21.89s\n",
            "iter 4200: dev acc=0.2870\n",
            "Iter 4300: loss=212.2842, time=22.38s\n",
            "iter 4300: dev acc=0.2734\n",
            "Iter 4400: loss=151.2086, time=22.85s\n",
            "iter 4400: dev acc=0.3043\n",
            "Iter 4500: loss=177.7954, time=23.34s\n",
            "iter 4500: dev acc=0.2834\n",
            "Iter 4600: loss=210.8155, time=23.82s\n",
            "iter 4600: dev acc=0.2943\n",
            "Iter 4700: loss=197.5864, time=24.30s\n",
            "iter 4700: dev acc=0.2788\n",
            "Iter 4800: loss=200.7474, time=24.78s\n",
            "iter 4800: dev acc=0.2543\n",
            "Iter 4900: loss=185.2984, time=25.25s\n",
            "iter 4900: dev acc=0.2834\n",
            "Iter 5000: loss=191.3095, time=25.73s\n",
            "iter 5000: dev acc=0.2870\n",
            "Iter 5100: loss=190.0629, time=26.21s\n",
            "iter 5100: dev acc=0.2716\n",
            "Iter 5200: loss=187.9343, time=26.70s\n",
            "iter 5200: dev acc=0.2888\n",
            "Iter 5300: loss=219.7067, time=27.18s\n",
            "iter 5300: dev acc=0.3088\n",
            "Iter 5400: loss=207.8896, time=27.66s\n",
            "iter 5400: dev acc=0.2807\n",
            "Iter 5500: loss=195.5567, time=28.14s\n",
            "iter 5500: dev acc=0.2734\n",
            "Iter 5600: loss=203.3375, time=28.62s\n",
            "iter 5600: dev acc=0.2634\n",
            "Iter 5700: loss=175.8119, time=29.09s\n",
            "iter 5700: dev acc=0.2943\n",
            "Iter 5800: loss=194.2153, time=29.57s\n",
            "iter 5800: dev acc=0.2879\n",
            "Iter 5900: loss=190.0713, time=30.04s\n",
            "iter 5900: dev acc=0.2852\n",
            "Iter 6000: loss=179.9254, time=30.51s\n",
            "iter 6000: dev acc=0.2698\n",
            "Iter 6100: loss=185.9565, time=30.98s\n",
            "iter 6100: dev acc=0.2952\n",
            "Iter 6200: loss=200.1551, time=31.46s\n",
            "iter 6200: dev acc=0.2925\n",
            "Iter 6300: loss=176.7610, time=31.93s\n",
            "iter 6300: dev acc=0.2843\n",
            "Iter 6400: loss=201.6076, time=32.41s\n",
            "iter 6400: dev acc=0.2888\n",
            "Iter 6500: loss=166.7423, time=32.89s\n",
            "iter 6500: dev acc=0.2916\n",
            "Iter 6600: loss=179.3267, time=33.38s\n",
            "iter 6600: dev acc=0.3025\n",
            "Iter 6700: loss=210.2432, time=33.86s\n",
            "iter 6700: dev acc=0.2725\n",
            "Iter 6800: loss=182.9814, time=34.35s\n",
            "iter 6800: dev acc=0.2707\n",
            "Iter 6900: loss=173.6380, time=34.84s\n",
            "iter 6900: dev acc=0.2934\n",
            "Iter 7000: loss=186.2154, time=35.31s\n",
            "iter 7000: dev acc=0.2870\n",
            "Iter 7100: loss=203.9341, time=35.79s\n",
            "iter 7100: dev acc=0.2716\n",
            "Iter 7200: loss=197.1867, time=36.28s\n",
            "iter 7200: dev acc=0.2807\n",
            "Iter 7300: loss=185.5909, time=36.76s\n",
            "iter 7300: dev acc=0.2925\n",
            "Iter 7400: loss=176.2914, time=37.24s\n",
            "iter 7400: dev acc=0.2716\n",
            "Iter 7500: loss=203.5768, time=37.72s\n",
            "iter 7500: dev acc=0.2879\n",
            "Iter 7600: loss=182.7372, time=38.20s\n",
            "iter 7600: dev acc=0.2670\n",
            "Iter 7700: loss=185.5439, time=38.68s\n",
            "iter 7700: dev acc=0.3215\n",
            "Iter 7800: loss=213.9219, time=39.16s\n",
            "iter 7800: dev acc=0.3115\n",
            "Iter 7900: loss=190.0845, time=39.64s\n",
            "iter 7900: dev acc=0.3152\n",
            "Iter 8000: loss=198.5848, time=40.12s\n",
            "iter 8000: dev acc=0.2734\n",
            "Iter 8100: loss=178.1218, time=40.62s\n",
            "iter 8100: dev acc=0.2752\n",
            "Iter 8200: loss=197.9482, time=41.10s\n",
            "iter 8200: dev acc=0.3252\n",
            "Iter 8300: loss=184.8020, time=41.59s\n",
            "iter 8300: dev acc=0.3179\n",
            "Iter 8400: loss=194.7776, time=42.08s\n",
            "iter 8400: dev acc=0.3015\n",
            "Iter 8500: loss=184.1063, time=42.56s\n",
            "iter 8500: dev acc=0.3188\n",
            "Shuffling training data\n",
            "Iter 8600: loss=169.2868, time=43.04s\n",
            "iter 8600: dev acc=0.3197\n",
            "Iter 8700: loss=127.1672, time=43.53s\n",
            "iter 8700: dev acc=0.3124\n",
            "Iter 8800: loss=148.6784, time=44.02s\n",
            "iter 8800: dev acc=0.3124\n",
            "Iter 8900: loss=164.6928, time=44.56s\n",
            "iter 8900: dev acc=0.3279\n",
            "Iter 9000: loss=166.5051, time=45.04s\n",
            "iter 9000: dev acc=0.3170\n",
            "Iter 9100: loss=144.6343, time=45.53s\n",
            "iter 9100: dev acc=0.3224\n",
            "Iter 9200: loss=157.6657, time=46.02s\n",
            "iter 9200: dev acc=0.3015\n",
            "Iter 9300: loss=142.5863, time=46.52s\n",
            "iter 9300: dev acc=0.3306\n",
            "new highscore\n",
            "Iter 9400: loss=150.4939, time=47.15s\n",
            "iter 9400: dev acc=0.3143\n",
            "Iter 9500: loss=144.7844, time=47.63s\n",
            "iter 9500: dev acc=0.3115\n",
            "Iter 9600: loss=153.1436, time=48.11s\n",
            "iter 9600: dev acc=0.3261\n",
            "Iter 9700: loss=165.7091, time=48.59s\n",
            "iter 9700: dev acc=0.3370\n",
            "new highscore\n",
            "Iter 9800: loss=171.2449, time=49.22s\n",
            "iter 9800: dev acc=0.3297\n",
            "Iter 9900: loss=175.6277, time=49.70s\n",
            "iter 9900: dev acc=0.3097\n",
            "Iter 10000: loss=157.1793, time=50.18s\n",
            "iter 10000: dev acc=0.3197\n",
            "Iter 10100: loss=175.5678, time=50.66s\n",
            "iter 10100: dev acc=0.3288\n",
            "Iter 10200: loss=167.7837, time=51.14s\n",
            "iter 10200: dev acc=0.3533\n",
            "new highscore\n",
            "Iter 10300: loss=168.6864, time=51.77s\n",
            "iter 10300: dev acc=0.3206\n",
            "Iter 10400: loss=151.9378, time=52.26s\n",
            "iter 10400: dev acc=0.3197\n",
            "Iter 10500: loss=164.8797, time=52.74s\n",
            "iter 10500: dev acc=0.3197\n",
            "Iter 10600: loss=145.2428, time=53.22s\n",
            "iter 10600: dev acc=0.3206\n",
            "Iter 10700: loss=151.3171, time=53.70s\n",
            "iter 10700: dev acc=0.3188\n",
            "Iter 10800: loss=178.5177, time=54.17s\n",
            "iter 10800: dev acc=0.3152\n",
            "Iter 10900: loss=174.9337, time=54.65s\n",
            "iter 10900: dev acc=0.3315\n",
            "Iter 11000: loss=166.0601, time=55.13s\n",
            "iter 11000: dev acc=0.3243\n",
            "Iter 11100: loss=154.6328, time=55.60s\n",
            "iter 11100: dev acc=0.3124\n",
            "Iter 11200: loss=170.7726, time=56.09s\n",
            "iter 11200: dev acc=0.3197\n",
            "Iter 11300: loss=182.9119, time=56.57s\n",
            "iter 11300: dev acc=0.3243\n",
            "Iter 11400: loss=174.8031, time=57.06s\n",
            "iter 11400: dev acc=0.3379\n",
            "Iter 11500: loss=135.8452, time=57.55s\n",
            "iter 11500: dev acc=0.3279\n",
            "Iter 11600: loss=163.1826, time=58.04s\n",
            "iter 11600: dev acc=0.3124\n",
            "Iter 11700: loss=156.0725, time=58.51s\n",
            "iter 11700: dev acc=0.3161\n",
            "Iter 11800: loss=156.3976, time=58.99s\n",
            "iter 11800: dev acc=0.3479\n",
            "Iter 11900: loss=158.3328, time=59.48s\n",
            "iter 11900: dev acc=0.3079\n",
            "Iter 12000: loss=158.1218, time=59.95s\n",
            "iter 12000: dev acc=0.3070\n",
            "Iter 12100: loss=148.3020, time=60.44s\n",
            "iter 12100: dev acc=0.3079\n",
            "Iter 12200: loss=163.2654, time=60.92s\n",
            "iter 12200: dev acc=0.3170\n",
            "Iter 12300: loss=166.6917, time=61.40s\n",
            "iter 12300: dev acc=0.3170\n",
            "Iter 12400: loss=151.7530, time=61.88s\n",
            "iter 12400: dev acc=0.3333\n",
            "Iter 12500: loss=151.4628, time=62.36s\n",
            "iter 12500: dev acc=0.3143\n",
            "Iter 12600: loss=162.0299, time=62.84s\n",
            "iter 12600: dev acc=0.3215\n",
            "Iter 12700: loss=170.8573, time=63.32s\n",
            "iter 12700: dev acc=0.3324\n",
            "Iter 12800: loss=155.6517, time=63.81s\n",
            "iter 12800: dev acc=0.3143\n",
            "Iter 12900: loss=158.5766, time=64.29s\n",
            "iter 12900: dev acc=0.3097\n",
            "Iter 13000: loss=166.7470, time=64.77s\n",
            "iter 13000: dev acc=0.3134\n",
            "Iter 13100: loss=157.9618, time=65.25s\n",
            "iter 13100: dev acc=0.3424\n",
            "Iter 13200: loss=160.9489, time=65.74s\n",
            "iter 13200: dev acc=0.3470\n",
            "Iter 13300: loss=164.8917, time=66.22s\n",
            "iter 13300: dev acc=0.3106\n",
            "Iter 13400: loss=158.1981, time=66.69s\n",
            "iter 13400: dev acc=0.3197\n",
            "Iter 13500: loss=137.5997, time=67.17s\n",
            "iter 13500: dev acc=0.3433\n",
            "Iter 13600: loss=163.2322, time=67.66s\n",
            "iter 13600: dev acc=0.3197\n",
            "Iter 13700: loss=175.9500, time=68.13s\n",
            "iter 13700: dev acc=0.3342\n",
            "Iter 13800: loss=171.0306, time=68.62s\n",
            "iter 13800: dev acc=0.3224\n",
            "Iter 13900: loss=158.6632, time=69.10s\n",
            "iter 13900: dev acc=0.3351\n",
            "Iter 14000: loss=151.9700, time=69.58s\n",
            "iter 14000: dev acc=0.3288\n",
            "Iter 14100: loss=173.0705, time=70.06s\n",
            "iter 14100: dev acc=0.3533\n",
            "Iter 14200: loss=161.7083, time=70.54s\n",
            "iter 14200: dev acc=0.3324\n",
            "Iter 14300: loss=165.1723, time=71.02s\n",
            "iter 14300: dev acc=0.3342\n",
            "Iter 14400: loss=167.2497, time=71.50s\n",
            "iter 14400: dev acc=0.3406\n",
            "Iter 14500: loss=145.3218, time=71.98s\n",
            "iter 14500: dev acc=0.3370\n",
            "Iter 14600: loss=159.2897, time=72.46s\n",
            "iter 14600: dev acc=0.3188\n",
            "Iter 14700: loss=154.5126, time=72.93s\n",
            "iter 14700: dev acc=0.3315\n",
            "Iter 14800: loss=170.2966, time=73.41s\n",
            "iter 14800: dev acc=0.3688\n",
            "new highscore\n",
            "Iter 14900: loss=164.3553, time=74.03s\n",
            "iter 14900: dev acc=0.3442\n",
            "Iter 15000: loss=147.6560, time=74.51s\n",
            "iter 15000: dev acc=0.3333\n",
            "Iter 15100: loss=135.9641, time=74.98s\n",
            "iter 15100: dev acc=0.3270\n",
            "Iter 15200: loss=164.0011, time=75.45s\n",
            "iter 15200: dev acc=0.3297\n",
            "Iter 15300: loss=160.4784, time=75.92s\n",
            "iter 15300: dev acc=0.3470\n",
            "Iter 15400: loss=177.7890, time=76.39s\n",
            "iter 15400: dev acc=0.3161\n",
            "Iter 15500: loss=165.0403, time=76.86s\n",
            "iter 15500: dev acc=0.3279\n",
            "Iter 15600: loss=170.0678, time=77.33s\n",
            "iter 15600: dev acc=0.3406\n",
            "Iter 15700: loss=178.8794, time=77.80s\n",
            "iter 15700: dev acc=0.3551\n",
            "Iter 15800: loss=157.3614, time=78.27s\n",
            "iter 15800: dev acc=0.3415\n",
            "Iter 15900: loss=184.8306, time=78.74s\n",
            "iter 15900: dev acc=0.3460\n",
            "Iter 16000: loss=158.7491, time=79.22s\n",
            "iter 16000: dev acc=0.3588\n",
            "Iter 16100: loss=179.4550, time=79.70s\n",
            "iter 16100: dev acc=0.3415\n",
            "Iter 16200: loss=159.1742, time=80.18s\n",
            "iter 16200: dev acc=0.3397\n",
            "Iter 16300: loss=159.0812, time=80.71s\n",
            "iter 16300: dev acc=0.3152\n",
            "Iter 16400: loss=164.2647, time=81.19s\n",
            "iter 16400: dev acc=0.3179\n",
            "Iter 16500: loss=138.8966, time=81.67s\n",
            "iter 16500: dev acc=0.3170\n",
            "Iter 16600: loss=173.4187, time=82.14s\n",
            "iter 16600: dev acc=0.3306\n",
            "Iter 16700: loss=174.4194, time=82.62s\n",
            "iter 16700: dev acc=0.3088\n",
            "Iter 16800: loss=160.1757, time=83.09s\n",
            "iter 16800: dev acc=0.3569\n",
            "Iter 16900: loss=175.8568, time=83.57s\n",
            "iter 16900: dev acc=0.3152\n",
            "Iter 17000: loss=184.8582, time=84.05s\n",
            "iter 17000: dev acc=0.3215\n",
            "Shuffling training data\n",
            "Iter 17100: loss=157.2154, time=84.53s\n",
            "iter 17100: dev acc=0.3061\n",
            "Iter 17200: loss=123.7434, time=85.00s\n",
            "iter 17200: dev acc=0.3188\n",
            "Iter 17300: loss=136.5702, time=85.48s\n",
            "iter 17300: dev acc=0.3488\n",
            "Iter 17400: loss=122.3020, time=85.96s\n",
            "iter 17400: dev acc=0.3424\n",
            "Iter 17500: loss=104.8590, time=86.44s\n",
            "iter 17500: dev acc=0.3397\n",
            "Iter 17600: loss=110.0602, time=86.92s\n",
            "iter 17600: dev acc=0.3361\n",
            "Iter 17700: loss=111.3011, time=87.39s\n",
            "iter 17700: dev acc=0.3342\n",
            "Iter 17800: loss=112.3312, time=87.87s\n",
            "iter 17800: dev acc=0.3206\n",
            "Iter 17900: loss=131.5736, time=88.35s\n",
            "iter 17900: dev acc=0.3324\n",
            "Iter 18000: loss=116.1990, time=88.82s\n",
            "iter 18000: dev acc=0.3106\n",
            "Iter 18100: loss=104.5530, time=89.30s\n",
            "iter 18100: dev acc=0.3252\n",
            "Iter 18200: loss=131.1880, time=89.77s\n",
            "iter 18200: dev acc=0.3270\n",
            "Iter 18300: loss=129.0718, time=90.24s\n",
            "iter 18300: dev acc=0.3324\n",
            "Iter 18400: loss=114.9224, time=90.73s\n",
            "iter 18400: dev acc=0.3333\n",
            "Iter 18500: loss=146.1797, time=91.22s\n",
            "iter 18500: dev acc=0.3179\n",
            "Iter 18600: loss=128.4677, time=91.71s\n",
            "iter 18600: dev acc=0.3115\n",
            "Iter 18700: loss=140.6419, time=92.20s\n",
            "iter 18700: dev acc=0.3479\n",
            "Iter 18800: loss=119.3857, time=92.69s\n",
            "iter 18800: dev acc=0.3306\n",
            "Iter 18900: loss=126.1200, time=93.16s\n",
            "iter 18900: dev acc=0.3415\n",
            "Iter 19000: loss=111.0160, time=93.65s\n",
            "iter 19000: dev acc=0.3415\n",
            "Iter 19100: loss=107.5963, time=94.13s\n",
            "iter 19100: dev acc=0.3324\n",
            "Iter 19200: loss=114.3405, time=94.61s\n",
            "iter 19200: dev acc=0.3288\n",
            "Iter 19300: loss=145.7121, time=95.09s\n",
            "iter 19300: dev acc=0.3460\n",
            "Iter 19400: loss=124.4774, time=95.57s\n",
            "iter 19400: dev acc=0.3406\n",
            "Iter 19500: loss=142.8795, time=96.05s\n",
            "iter 19500: dev acc=0.3415\n",
            "Iter 19600: loss=131.9918, time=96.52s\n",
            "iter 19600: dev acc=0.3333\n",
            "Iter 19700: loss=138.6980, time=97.00s\n",
            "iter 19700: dev acc=0.3324\n",
            "Iter 19800: loss=120.5499, time=97.48s\n",
            "iter 19800: dev acc=0.3279\n",
            "Iter 19900: loss=144.6451, time=97.96s\n",
            "iter 19900: dev acc=0.3233\n",
            "Iter 20000: loss=135.3172, time=98.45s\n",
            "iter 20000: dev acc=0.3233\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 14800: train acc=0.5122, dev acc=0.3688, test acc=0.3317\n",
            "Best Test Accuracy for seed 999: 0.3317\n"
          ]
        }
      ],
      "source": [
        "seeds = [42, 123, 999]\n",
        "test_accuracies_cbow = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_seed(seed)\n",
        "\n",
        "    # If everything is in place we can now train our first model!\n",
        "    cbow_model = CBOW(len(v.w2i), len(t2i), vocab=v)\n",
        "\n",
        "    cbow_model = cbow_model.to(device)\n",
        "    optimizer = optim.Adam(cbow_model.parameters(), lr=0.0005)\n",
        "\n",
        "    losses, accuracies, test_acc = train_model(\n",
        "    cbow_model, optimizer, num_iterations=20000,\n",
        "    print_every=100, eval_every=100)\n",
        "\n",
        "    print(f\"Best Test Accuracy for seed {seed}: {test_acc:.4f}\")\n",
        "    test_accuracies_cbow.append(test_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and standard deviation across seeds\n",
        "mean_accuracy = np.mean(test_accuracies_cbow)\n",
        "std_accuracy = np.std(test_accuracies_cbow)\n",
        "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation: {std_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "m_FDLQfAykCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4eb1e2e-4676-4b7f-b611-52804f2e8090"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Test Accuracy: 0.3380\n",
            "Standard Deviation: 0.0130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpFt_Fo2TdN0"
      },
      "source": [
        "# Deep CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZanOMesTfEZ"
      },
      "source": [
        "To see if we can squeeze some more performance out of the CBOW model, we can make it deeper and non-linear by adding more layers and, e.g., tanh-activations.\n",
        "By using more parameters we can learn more aspects of the data, and by using more layers and non-linearities, we can try to learn a more complex function.\n",
        "This is not something that always works. If the input-output mapping of your data is simple, then a complicated function could easily overfit on your training set, thereby leading to poor generalization.\n",
        "\n",
        "#### Exercise: write Deep CBOW class and train it\n",
        "\n",
        "Write a class `DeepCBOW`.\n",
        "\n",
        "In your code, make sure that your `output_layer` consists of the following:\n",
        "- A linear transformation from E units to D units.\n",
        "- A Tanh activation\n",
        "- A linear transformation from D units to D units\n",
        "- A Tanh activation\n",
        "- A linear transformation from D units to 5 units (our output classes).\n",
        "\n",
        "E is the size of the word embeddings (please use E=300) and D for the size of a hidden layer (please use D=100).\n",
        "\n",
        "We recommend using [nn.Sequential](https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential) to implement this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4vn369NkRfLw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DeepCBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, num_classes, vocab, embedding_dim=300, hidden_dim=100):\n",
        "        \"\"\"\n",
        "        Deep CBOW Model\n",
        "        :param vocab_size: Size of the vocabulary\n",
        "        :param num_classes: Number of output classes\n",
        "        :param embedding_dim: Size of the word embeddings (E)\n",
        "        :param hidden_dim: Size of the hidden layers (D)\n",
        "        \"\"\"\n",
        "        super(DeepCBOW, self).__init__()\n",
        "        self.vocab = vocab # Add this line to store the vocabulary\n",
        "\n",
        "        # Embedding layer: Map word indices to embedding vectors\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Output layer: Stack of linear layers + tanh activations\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),  # E -> D\n",
        "            nn.Tanh(),                            # Non-linearity\n",
        "            nn.Linear(hidden_dim, hidden_dim),    # D -> D\n",
        "            nn.Tanh(),                            # Non-linearity\n",
        "            nn.Linear(hidden_dim, num_classes)    # D -> 5\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Deep CBOW model\n",
        "        :param x: Input word indices (batch_size, seq_len)\n",
        "        :return: Logits (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # 1. Look up embeddings for each word in the input\n",
        "        embeddings = self.embed(x)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # 2. Sum embeddings along the sequence length dimension\n",
        "        sentence_vector = embeddings.sum(dim=1)  # Shape: [batch_size, embedding_dim]\n",
        "\n",
        "        # 3. Pass through the output layer\n",
        "        logits = self.output_layer(sentence_vector)  # Shape: [batch_size, num_classes]\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [42, 123, 999]\n",
        "test_accuracies_dcbow = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_seed(seed)\n",
        "\n",
        "    # If everything is in place we can now train our first model!\n",
        "    dcbow_model = DeepCBOW(len(v.w2i), num_classes = 5, vocab = v)\n",
        "\n",
        "    dcbow_model = dcbow_model.to(device)\n",
        "    optimizer = optim.Adam(dcbow_model.parameters(), lr=0.0005)\n",
        "\n",
        "    losses, accuracies, test_acc = train_model(\n",
        "    dcbow_model, optimizer, num_iterations=13000,\n",
        "    print_every=100, eval_every=100)\n",
        "\n",
        "    print(f\"Best Test Accuracy for seed {seed}: {test_acc:.4f}\")\n",
        "    test_accuracies_dcbow.append(test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd8ZN4g0RsFI",
        "outputId": "b11be4e7-2069-443b-b241-e666aa0d2b6a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling training data\n",
            "Iter 100: loss=157.8315, time=0.18s\n",
            "iter 100: dev acc=0.2598\n",
            "new highscore\n",
            "Iter 200: loss=156.2704, time=1.01s\n",
            "iter 200: dev acc=0.2489\n",
            "Iter 300: loss=160.0996, time=1.69s\n",
            "iter 300: dev acc=0.2534\n",
            "Iter 400: loss=155.9718, time=2.32s\n",
            "iter 400: dev acc=0.2661\n",
            "new highscore\n",
            "Iter 500: loss=159.3816, time=3.12s\n",
            "iter 500: dev acc=0.2507\n",
            "Iter 600: loss=163.3096, time=3.76s\n",
            "iter 600: dev acc=0.2425\n",
            "Iter 700: loss=156.3889, time=4.39s\n",
            "iter 700: dev acc=0.2679\n",
            "new highscore\n",
            "Iter 800: loss=157.5715, time=5.17s\n",
            "iter 800: dev acc=0.2843\n",
            "new highscore\n",
            "Iter 900: loss=156.9384, time=5.99s\n",
            "iter 900: dev acc=0.2861\n",
            "new highscore\n",
            "Iter 1000: loss=168.1795, time=6.80s\n",
            "iter 1000: dev acc=0.2552\n",
            "Iter 1100: loss=163.5461, time=7.43s\n",
            "iter 1100: dev acc=0.2616\n",
            "Iter 1200: loss=162.5178, time=8.06s\n",
            "iter 1200: dev acc=0.2616\n",
            "Iter 1300: loss=155.8191, time=8.69s\n",
            "iter 1300: dev acc=0.2525\n",
            "Iter 1400: loss=160.4076, time=9.32s\n",
            "iter 1400: dev acc=0.2607\n",
            "Iter 1500: loss=151.0670, time=9.96s\n",
            "iter 1500: dev acc=0.2879\n",
            "new highscore\n",
            "Iter 1600: loss=160.4344, time=10.76s\n",
            "iter 1600: dev acc=0.2979\n",
            "new highscore\n",
            "Iter 1700: loss=150.7229, time=11.55s\n",
            "iter 1700: dev acc=0.3097\n",
            "new highscore\n",
            "Iter 1800: loss=150.9678, time=12.35s\n",
            "iter 1800: dev acc=0.3252\n",
            "new highscore\n",
            "Iter 1900: loss=157.1181, time=13.14s\n",
            "iter 1900: dev acc=0.3106\n",
            "Iter 2000: loss=155.9082, time=13.78s\n",
            "iter 2000: dev acc=0.2916\n",
            "Iter 2100: loss=155.0745, time=14.41s\n",
            "iter 2100: dev acc=0.2925\n",
            "Iter 2200: loss=155.2762, time=15.05s\n",
            "iter 2200: dev acc=0.2707\n",
            "Iter 2300: loss=161.6190, time=15.70s\n",
            "iter 2300: dev acc=0.3115\n",
            "Iter 2400: loss=163.3176, time=16.34s\n",
            "iter 2400: dev acc=0.3015\n",
            "Iter 2500: loss=151.0069, time=16.98s\n",
            "iter 2500: dev acc=0.2788\n",
            "Iter 2600: loss=151.6751, time=17.63s\n",
            "iter 2600: dev acc=0.3043\n",
            "Iter 2700: loss=155.1304, time=18.27s\n",
            "iter 2700: dev acc=0.2807\n",
            "Iter 2800: loss=154.6683, time=18.90s\n",
            "iter 2800: dev acc=0.2861\n",
            "Iter 2900: loss=158.0846, time=19.53s\n",
            "iter 2900: dev acc=0.3261\n",
            "new highscore\n",
            "Iter 3000: loss=153.3049, time=20.31s\n",
            "iter 3000: dev acc=0.2952\n",
            "Iter 3100: loss=158.2529, time=20.95s\n",
            "iter 3100: dev acc=0.2725\n",
            "Iter 3200: loss=159.3746, time=21.59s\n",
            "iter 3200: dev acc=0.3097\n",
            "Iter 3300: loss=155.2558, time=22.22s\n",
            "iter 3300: dev acc=0.2943\n",
            "Iter 3400: loss=150.7166, time=22.85s\n",
            "iter 3400: dev acc=0.2961\n",
            "Iter 3500: loss=156.6919, time=23.49s\n",
            "iter 3500: dev acc=0.3143\n",
            "Iter 3600: loss=163.1148, time=24.12s\n",
            "iter 3600: dev acc=0.3052\n",
            "Iter 3700: loss=153.1807, time=24.74s\n",
            "iter 3700: dev acc=0.3097\n",
            "Iter 3800: loss=151.5557, time=25.37s\n",
            "iter 3800: dev acc=0.2825\n",
            "Iter 3900: loss=152.2732, time=26.00s\n",
            "iter 3900: dev acc=0.3106\n",
            "Iter 4000: loss=151.2547, time=26.64s\n",
            "iter 4000: dev acc=0.3152\n",
            "Iter 4100: loss=149.9140, time=27.27s\n",
            "iter 4100: dev acc=0.3061\n",
            "Iter 4200: loss=158.2242, time=27.90s\n",
            "iter 4200: dev acc=0.2716\n",
            "Iter 4300: loss=158.7248, time=28.54s\n",
            "iter 4300: dev acc=0.3124\n",
            "Iter 4400: loss=147.6418, time=29.18s\n",
            "iter 4400: dev acc=0.3306\n",
            "new highscore\n",
            "Iter 4500: loss=152.4971, time=29.95s\n",
            "iter 4500: dev acc=0.3115\n",
            "Iter 4600: loss=147.7098, time=30.59s\n",
            "iter 4600: dev acc=0.2852\n",
            "Iter 4700: loss=153.2143, time=31.23s\n",
            "iter 4700: dev acc=0.3161\n",
            "Iter 4800: loss=157.2932, time=31.87s\n",
            "iter 4800: dev acc=0.2952\n",
            "Iter 4900: loss=148.2463, time=32.52s\n",
            "iter 4900: dev acc=0.3015\n",
            "Iter 5000: loss=152.9968, time=33.15s\n",
            "iter 5000: dev acc=0.2797\n",
            "Iter 5100: loss=150.4520, time=33.80s\n",
            "iter 5100: dev acc=0.3052\n",
            "Iter 5200: loss=158.0580, time=34.44s\n",
            "iter 5200: dev acc=0.3097\n",
            "Iter 5300: loss=145.1050, time=35.08s\n",
            "iter 5300: dev acc=0.2988\n",
            "Iter 5400: loss=154.4144, time=35.72s\n",
            "iter 5400: dev acc=0.3188\n",
            "Iter 5500: loss=158.7578, time=36.36s\n",
            "iter 5500: dev acc=0.3152\n",
            "Iter 5600: loss=148.8799, time=37.00s\n",
            "iter 5600: dev acc=0.3351\n",
            "new highscore\n",
            "Iter 5700: loss=142.8169, time=37.81s\n",
            "iter 5700: dev acc=0.3088\n",
            "Iter 5800: loss=148.0562, time=38.44s\n",
            "iter 5800: dev acc=0.3034\n",
            "Iter 5900: loss=152.7979, time=39.07s\n",
            "iter 5900: dev acc=0.3088\n",
            "Iter 6000: loss=157.7003, time=39.70s\n",
            "iter 6000: dev acc=0.3297\n",
            "Iter 6100: loss=149.1630, time=40.34s\n",
            "iter 6100: dev acc=0.3115\n",
            "Iter 6200: loss=153.2819, time=40.98s\n",
            "iter 6200: dev acc=0.3279\n",
            "Iter 6300: loss=147.4872, time=41.63s\n",
            "iter 6300: dev acc=0.3370\n",
            "new highscore\n",
            "Iter 6400: loss=154.7938, time=42.44s\n",
            "iter 6400: dev acc=0.2952\n",
            "Iter 6500: loss=146.7120, time=43.07s\n",
            "iter 6500: dev acc=0.3433\n",
            "new highscore\n",
            "Iter 6600: loss=151.8539, time=43.86s\n",
            "iter 6600: dev acc=0.3333\n",
            "Iter 6700: loss=149.7688, time=44.50s\n",
            "iter 6700: dev acc=0.3361\n",
            "Iter 6800: loss=155.6003, time=45.14s\n",
            "iter 6800: dev acc=0.3361\n",
            "Iter 6900: loss=158.1430, time=45.78s\n",
            "iter 6900: dev acc=0.3351\n",
            "Iter 7000: loss=154.4299, time=46.41s\n",
            "iter 7000: dev acc=0.3306\n",
            "Iter 7100: loss=144.8376, time=47.03s\n",
            "iter 7100: dev acc=0.3197\n",
            "Iter 7200: loss=150.0649, time=47.67s\n",
            "iter 7200: dev acc=0.3143\n",
            "Iter 7300: loss=148.7586, time=48.30s\n",
            "iter 7300: dev acc=0.3052\n",
            "Iter 7400: loss=143.7786, time=48.93s\n",
            "iter 7400: dev acc=0.3161\n",
            "Iter 7500: loss=156.9907, time=49.56s\n",
            "iter 7500: dev acc=0.3124\n",
            "Iter 7600: loss=154.0812, time=50.20s\n",
            "iter 7600: dev acc=0.3179\n",
            "Iter 7700: loss=152.4476, time=50.83s\n",
            "iter 7700: dev acc=0.3470\n",
            "new highscore\n",
            "Iter 7800: loss=149.6207, time=51.62s\n",
            "iter 7800: dev acc=0.3252\n",
            "Iter 7900: loss=163.0619, time=52.26s\n",
            "iter 7900: dev acc=0.2688\n",
            "Iter 8000: loss=148.6423, time=52.92s\n",
            "iter 8000: dev acc=0.3106\n",
            "Iter 8100: loss=148.1612, time=53.57s\n",
            "iter 8100: dev acc=0.3270\n",
            "Iter 8200: loss=153.0293, time=54.22s\n",
            "iter 8200: dev acc=0.3179\n",
            "Iter 8300: loss=145.2400, time=54.87s\n",
            "iter 8300: dev acc=0.3115\n",
            "Iter 8400: loss=149.7316, time=55.52s\n",
            "iter 8400: dev acc=0.3370\n",
            "Iter 8500: loss=150.9459, time=56.16s\n",
            "iter 8500: dev acc=0.3288\n",
            "Shuffling training data\n",
            "Iter 8600: loss=148.7495, time=56.81s\n",
            "iter 8600: dev acc=0.3297\n",
            "Iter 8700: loss=139.4007, time=57.46s\n",
            "iter 8700: dev acc=0.3288\n",
            "Iter 8800: loss=136.3402, time=58.11s\n",
            "iter 8800: dev acc=0.3124\n",
            "Iter 8900: loss=139.4270, time=58.76s\n",
            "iter 8900: dev acc=0.3152\n",
            "Iter 9000: loss=132.3412, time=59.41s\n",
            "iter 9000: dev acc=0.3470\n",
            "Iter 9100: loss=133.7170, time=60.06s\n",
            "iter 9100: dev acc=0.3134\n",
            "Iter 9200: loss=132.7407, time=60.70s\n",
            "iter 9200: dev acc=0.2970\n",
            "Iter 9300: loss=139.1076, time=61.34s\n",
            "iter 9300: dev acc=0.2906\n",
            "Iter 9400: loss=133.2570, time=61.98s\n",
            "iter 9400: dev acc=0.3179\n",
            "Iter 9500: loss=138.0554, time=62.62s\n",
            "iter 9500: dev acc=0.3124\n",
            "Iter 9600: loss=141.0464, time=63.30s\n",
            "iter 9600: dev acc=0.3324\n",
            "Iter 9700: loss=135.5363, time=63.93s\n",
            "iter 9700: dev acc=0.3333\n",
            "Iter 9800: loss=138.7102, time=64.56s\n",
            "iter 9800: dev acc=0.3460\n",
            "Iter 9900: loss=137.2645, time=65.20s\n",
            "iter 9900: dev acc=0.3379\n",
            "Iter 10000: loss=133.1420, time=65.83s\n",
            "iter 10000: dev acc=0.3506\n",
            "new highscore\n",
            "Iter 10100: loss=137.0087, time=66.63s\n",
            "iter 10100: dev acc=0.3433\n",
            "Iter 10200: loss=132.7115, time=67.27s\n",
            "iter 10200: dev acc=0.3451\n",
            "Iter 10300: loss=132.2276, time=67.90s\n",
            "iter 10300: dev acc=0.3388\n",
            "Iter 10400: loss=130.5530, time=68.53s\n",
            "iter 10400: dev acc=0.3397\n",
            "Iter 10500: loss=143.1364, time=69.17s\n",
            "iter 10500: dev acc=0.3315\n",
            "Iter 10600: loss=140.2046, time=69.83s\n",
            "iter 10600: dev acc=0.3324\n",
            "Iter 10700: loss=130.2992, time=70.46s\n",
            "iter 10700: dev acc=0.3351\n",
            "Iter 10800: loss=136.8883, time=71.09s\n",
            "iter 10800: dev acc=0.3297\n",
            "Iter 10900: loss=149.9590, time=71.72s\n",
            "iter 10900: dev acc=0.3306\n",
            "Iter 11000: loss=147.9605, time=72.36s\n",
            "iter 11000: dev acc=0.3306\n",
            "Iter 11100: loss=133.5061, time=72.99s\n",
            "iter 11100: dev acc=0.3315\n",
            "Iter 11200: loss=138.7829, time=73.63s\n",
            "iter 11200: dev acc=0.3143\n",
            "Iter 11300: loss=147.6287, time=74.26s\n",
            "iter 11300: dev acc=0.3034\n",
            "Iter 11400: loss=129.1755, time=74.89s\n",
            "iter 11400: dev acc=0.3460\n",
            "Iter 11500: loss=147.7309, time=75.53s\n",
            "iter 11500: dev acc=0.3415\n",
            "Iter 11600: loss=139.5639, time=76.17s\n",
            "iter 11600: dev acc=0.3288\n",
            "Iter 11700: loss=127.5595, time=76.82s\n",
            "iter 11700: dev acc=0.3515\n",
            "new highscore\n",
            "Iter 11800: loss=146.2629, time=77.61s\n",
            "iter 11800: dev acc=0.3215\n",
            "Iter 11900: loss=140.6002, time=78.26s\n",
            "iter 11900: dev acc=0.3197\n",
            "Iter 12000: loss=131.3572, time=78.90s\n",
            "iter 12000: dev acc=0.3542\n",
            "new highscore\n",
            "Iter 12100: loss=128.0569, time=79.71s\n",
            "iter 12100: dev acc=0.3560\n",
            "new highscore\n",
            "Iter 12200: loss=140.4295, time=80.52s\n",
            "iter 12200: dev acc=0.3415\n",
            "Iter 12300: loss=145.5480, time=81.15s\n",
            "iter 12300: dev acc=0.3306\n",
            "Iter 12400: loss=140.8260, time=81.79s\n",
            "iter 12400: dev acc=0.3542\n",
            "Iter 12500: loss=142.6783, time=82.44s\n",
            "iter 12500: dev acc=0.3388\n",
            "Iter 12600: loss=129.4533, time=83.09s\n",
            "iter 12600: dev acc=0.3533\n",
            "Iter 12700: loss=141.3868, time=83.73s\n",
            "iter 12700: dev acc=0.3569\n",
            "new highscore\n",
            "Iter 12800: loss=134.2223, time=84.54s\n",
            "iter 12800: dev acc=0.3524\n",
            "Iter 12900: loss=122.7606, time=85.18s\n",
            "iter 12900: dev acc=0.3306\n",
            "Iter 13000: loss=141.3375, time=85.82s\n",
            "iter 13000: dev acc=0.3433\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-07f34c5b57a1>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 12700: train acc=0.4709, dev acc=0.3569, test acc=0.3701\n",
            "Best Test Accuracy for seed 42: 0.3701\n",
            "Shuffling training data\n",
            "Iter 100: loss=165.7891, time=0.18s\n",
            "iter 100: dev acc=0.2543\n",
            "new highscore\n",
            "Iter 200: loss=163.7531, time=0.98s\n",
            "iter 200: dev acc=0.2507\n",
            "Iter 300: loss=159.4045, time=1.61s\n",
            "iter 300: dev acc=0.2707\n",
            "new highscore\n",
            "Iter 400: loss=164.4126, time=2.46s\n",
            "iter 400: dev acc=0.2561\n",
            "Iter 500: loss=157.7531, time=3.10s\n",
            "iter 500: dev acc=0.2834\n",
            "new highscore\n",
            "Iter 600: loss=147.0043, time=3.90s\n",
            "iter 600: dev acc=0.2779\n",
            "Iter 700: loss=159.0828, time=4.54s\n",
            "iter 700: dev acc=0.2925\n",
            "new highscore\n",
            "Iter 800: loss=164.8504, time=5.37s\n",
            "iter 800: dev acc=0.2743\n",
            "Iter 900: loss=154.8687, time=6.01s\n",
            "iter 900: dev acc=0.2943\n",
            "new highscore\n",
            "Iter 1000: loss=151.7906, time=6.84s\n",
            "iter 1000: dev acc=0.2952\n",
            "new highscore\n",
            "Iter 1100: loss=161.3168, time=7.67s\n",
            "iter 1100: dev acc=0.2725\n",
            "Iter 1200: loss=158.1875, time=8.31s\n",
            "iter 1200: dev acc=0.2743\n",
            "Iter 1300: loss=163.0278, time=8.96s\n",
            "iter 1300: dev acc=0.2852\n",
            "Iter 1400: loss=154.1617, time=9.61s\n",
            "iter 1400: dev acc=0.2679\n",
            "Iter 1500: loss=154.4582, time=10.25s\n",
            "iter 1500: dev acc=0.2861\n",
            "Iter 1600: loss=159.0405, time=10.89s\n",
            "iter 1600: dev acc=0.2607\n",
            "Iter 1700: loss=159.7942, time=11.54s\n",
            "iter 1700: dev acc=0.2916\n",
            "Iter 1800: loss=159.3054, time=12.19s\n",
            "iter 1800: dev acc=0.2870\n",
            "Iter 1900: loss=161.3730, time=12.83s\n",
            "iter 1900: dev acc=0.3034\n",
            "new highscore\n",
            "Iter 2000: loss=153.7245, time=13.67s\n",
            "iter 2000: dev acc=0.3006\n",
            "Iter 2100: loss=157.2358, time=14.31s\n",
            "iter 2100: dev acc=0.2870\n",
            "Iter 2200: loss=161.2156, time=14.95s\n",
            "iter 2200: dev acc=0.2679\n",
            "Iter 2300: loss=158.1682, time=15.58s\n",
            "iter 2300: dev acc=0.2870\n",
            "Iter 2400: loss=155.3036, time=16.22s\n",
            "iter 2400: dev acc=0.2925\n",
            "Iter 2500: loss=155.5889, time=16.86s\n",
            "iter 2500: dev acc=0.3043\n",
            "new highscore\n",
            "Iter 2600: loss=156.6369, time=17.68s\n",
            "iter 2600: dev acc=0.2906\n",
            "Iter 2700: loss=152.5742, time=18.33s\n",
            "iter 2700: dev acc=0.2988\n",
            "Iter 2800: loss=160.7968, time=18.99s\n",
            "iter 2800: dev acc=0.2761\n",
            "Iter 2900: loss=159.0197, time=19.65s\n",
            "iter 2900: dev acc=0.2398\n",
            "Iter 3000: loss=150.3235, time=20.30s\n",
            "iter 3000: dev acc=0.2661\n",
            "Iter 3100: loss=159.1785, time=20.96s\n",
            "iter 3100: dev acc=0.2870\n",
            "Iter 3200: loss=150.1634, time=21.61s\n",
            "iter 3200: dev acc=0.2888\n",
            "Iter 3300: loss=144.9637, time=22.26s\n",
            "iter 3300: dev acc=0.3161\n",
            "new highscore\n",
            "Iter 3400: loss=153.7620, time=23.08s\n",
            "iter 3400: dev acc=0.3188\n",
            "new highscore\n",
            "Iter 3500: loss=160.1187, time=23.90s\n",
            "iter 3500: dev acc=0.2552\n",
            "Iter 3600: loss=158.4949, time=24.54s\n",
            "iter 3600: dev acc=0.3124\n",
            "Iter 3700: loss=156.4252, time=25.18s\n",
            "iter 3700: dev acc=0.3224\n",
            "new highscore\n",
            "Iter 3800: loss=153.9747, time=25.98s\n",
            "iter 3800: dev acc=0.3170\n",
            "Iter 3900: loss=145.5355, time=26.62s\n",
            "iter 3900: dev acc=0.2906\n",
            "Iter 4000: loss=154.3877, time=27.28s\n",
            "iter 4000: dev acc=0.3170\n",
            "Iter 4100: loss=153.9643, time=27.92s\n",
            "iter 4100: dev acc=0.2888\n",
            "Iter 4200: loss=152.8600, time=28.56s\n",
            "iter 4200: dev acc=0.3106\n",
            "Iter 4300: loss=154.2205, time=29.20s\n",
            "iter 4300: dev acc=0.3115\n",
            "Iter 4400: loss=153.5429, time=29.84s\n",
            "iter 4400: dev acc=0.3161\n",
            "Iter 4500: loss=160.6296, time=30.48s\n",
            "iter 4500: dev acc=0.3106\n",
            "Iter 4600: loss=150.2277, time=31.12s\n",
            "iter 4600: dev acc=0.2570\n",
            "Iter 4700: loss=153.9613, time=31.77s\n",
            "iter 4700: dev acc=0.3152\n",
            "Iter 4800: loss=158.5626, time=32.43s\n",
            "iter 4800: dev acc=0.2997\n",
            "Iter 4900: loss=159.1570, time=33.14s\n",
            "iter 4900: dev acc=0.2797\n",
            "Iter 5000: loss=155.6342, time=33.80s\n",
            "iter 5000: dev acc=0.2934\n",
            "Iter 5100: loss=153.7507, time=34.43s\n",
            "iter 5100: dev acc=0.3170\n",
            "Iter 5200: loss=150.4034, time=35.08s\n",
            "iter 5200: dev acc=0.3015\n",
            "Iter 5300: loss=151.9809, time=35.73s\n",
            "iter 5300: dev acc=0.3161\n",
            "Iter 5400: loss=147.8579, time=36.37s\n",
            "iter 5400: dev acc=0.3279\n",
            "new highscore\n",
            "Iter 5500: loss=155.3063, time=37.16s\n",
            "iter 5500: dev acc=0.3143\n",
            "Iter 5600: loss=149.3717, time=37.80s\n",
            "iter 5600: dev acc=0.3279\n",
            "Iter 5700: loss=158.6359, time=38.43s\n",
            "iter 5700: dev acc=0.3279\n",
            "Iter 5800: loss=145.3334, time=39.07s\n",
            "iter 5800: dev acc=0.3406\n",
            "new highscore\n",
            "Iter 5900: loss=153.6106, time=39.86s\n",
            "iter 5900: dev acc=0.2970\n",
            "Iter 6000: loss=153.7970, time=40.50s\n",
            "iter 6000: dev acc=0.3351\n",
            "Iter 6100: loss=144.7351, time=41.13s\n",
            "iter 6100: dev acc=0.3270\n",
            "Iter 6200: loss=148.7470, time=41.76s\n",
            "iter 6200: dev acc=0.3115\n",
            "Iter 6300: loss=154.8506, time=42.39s\n",
            "iter 6300: dev acc=0.3124\n",
            "Iter 6400: loss=153.7921, time=43.03s\n",
            "iter 6400: dev acc=0.3261\n",
            "Iter 6500: loss=149.7130, time=43.66s\n",
            "iter 6500: dev acc=0.3342\n",
            "Iter 6600: loss=161.4217, time=44.31s\n",
            "iter 6600: dev acc=0.3424\n",
            "new highscore\n",
            "Iter 6700: loss=149.0784, time=45.14s\n",
            "iter 6700: dev acc=0.3433\n",
            "new highscore\n",
            "Iter 6800: loss=148.5912, time=45.93s\n",
            "iter 6800: dev acc=0.3560\n",
            "new highscore\n",
            "Iter 6900: loss=146.3505, time=46.72s\n",
            "iter 6900: dev acc=0.3451\n",
            "Iter 7000: loss=157.0723, time=47.36s\n",
            "iter 7000: dev acc=0.3488\n",
            "Iter 7100: loss=150.9729, time=47.99s\n",
            "iter 7100: dev acc=0.3124\n",
            "Iter 7200: loss=159.2287, time=48.63s\n",
            "iter 7200: dev acc=0.3288\n",
            "Iter 7300: loss=152.8252, time=49.27s\n",
            "iter 7300: dev acc=0.3324\n",
            "Iter 7400: loss=153.0274, time=49.91s\n",
            "iter 7400: dev acc=0.3252\n",
            "Iter 7500: loss=149.7427, time=50.54s\n",
            "iter 7500: dev acc=0.3379\n",
            "Iter 7600: loss=146.8474, time=51.18s\n",
            "iter 7600: dev acc=0.3324\n",
            "Iter 7700: loss=152.3334, time=51.82s\n",
            "iter 7700: dev acc=0.3315\n",
            "Iter 7800: loss=146.0078, time=52.46s\n",
            "iter 7800: dev acc=0.3252\n",
            "Iter 7900: loss=145.5663, time=53.10s\n",
            "iter 7900: dev acc=0.3324\n",
            "Iter 8000: loss=150.4452, time=53.74s\n",
            "iter 8000: dev acc=0.3361\n",
            "Iter 8100: loss=151.4288, time=54.37s\n",
            "iter 8100: dev acc=0.3388\n",
            "Iter 8200: loss=142.7261, time=55.00s\n",
            "iter 8200: dev acc=0.3524\n",
            "Iter 8300: loss=153.9334, time=55.64s\n",
            "iter 8300: dev acc=0.3261\n",
            "Iter 8400: loss=154.9690, time=56.30s\n",
            "iter 8400: dev acc=0.3633\n",
            "new highscore\n",
            "Iter 8500: loss=152.0447, time=57.09s\n",
            "iter 8500: dev acc=0.3560\n",
            "Shuffling training data\n",
            "Iter 8600: loss=145.4383, time=57.75s\n",
            "iter 8600: dev acc=0.3143\n",
            "Iter 8700: loss=148.9186, time=58.39s\n",
            "iter 8700: dev acc=0.3252\n",
            "Iter 8800: loss=138.5641, time=59.04s\n",
            "iter 8800: dev acc=0.3433\n",
            "Iter 8900: loss=133.7885, time=59.68s\n",
            "iter 8900: dev acc=0.3397\n",
            "Iter 9000: loss=144.7734, time=60.33s\n",
            "iter 9000: dev acc=0.3588\n",
            "Iter 9100: loss=138.1598, time=60.97s\n",
            "iter 9100: dev acc=0.3397\n",
            "Iter 9200: loss=145.1822, time=61.62s\n",
            "iter 9200: dev acc=0.3043\n",
            "Iter 9300: loss=131.3535, time=62.26s\n",
            "iter 9300: dev acc=0.3415\n",
            "Iter 9400: loss=144.2831, time=62.91s\n",
            "iter 9400: dev acc=0.3170\n",
            "Iter 9500: loss=143.8068, time=63.54s\n",
            "iter 9500: dev acc=0.3370\n",
            "Iter 9600: loss=148.0169, time=64.18s\n",
            "iter 9600: dev acc=0.3606\n",
            "Iter 9700: loss=139.3146, time=64.82s\n",
            "iter 9700: dev acc=0.3342\n",
            "Iter 9800: loss=145.2825, time=65.45s\n",
            "iter 9800: dev acc=0.3588\n",
            "Iter 9900: loss=142.1157, time=66.10s\n",
            "iter 9900: dev acc=0.3624\n",
            "Iter 10000: loss=142.7732, time=66.74s\n",
            "iter 10000: dev acc=0.3615\n",
            "Iter 10100: loss=148.7627, time=67.39s\n",
            "iter 10100: dev acc=0.3470\n",
            "Iter 10200: loss=150.5044, time=68.04s\n",
            "iter 10200: dev acc=0.3506\n",
            "Iter 10300: loss=133.5859, time=68.68s\n",
            "iter 10300: dev acc=0.3488\n",
            "Iter 10400: loss=129.8335, time=69.31s\n",
            "iter 10400: dev acc=0.3606\n",
            "Iter 10500: loss=133.5918, time=69.94s\n",
            "iter 10500: dev acc=0.3515\n",
            "Iter 10600: loss=135.6042, time=70.57s\n",
            "iter 10600: dev acc=0.3551\n",
            "Iter 10700: loss=147.5511, time=71.21s\n",
            "iter 10700: dev acc=0.3470\n",
            "Iter 10800: loss=145.5936, time=71.85s\n",
            "iter 10800: dev acc=0.3506\n",
            "Iter 10900: loss=132.4699, time=72.49s\n",
            "iter 10900: dev acc=0.3415\n",
            "Iter 11000: loss=125.3159, time=73.12s\n",
            "iter 11000: dev acc=0.3243\n",
            "Iter 11100: loss=141.5921, time=73.76s\n",
            "iter 11100: dev acc=0.3442\n",
            "Iter 11200: loss=133.8380, time=74.39s\n",
            "iter 11200: dev acc=0.3342\n",
            "Iter 11300: loss=140.2588, time=75.02s\n",
            "iter 11300: dev acc=0.3433\n",
            "Iter 11400: loss=135.9709, time=75.65s\n",
            "iter 11400: dev acc=0.3542\n",
            "Iter 11500: loss=127.0918, time=76.29s\n",
            "iter 11500: dev acc=0.3733\n",
            "new highscore\n",
            "Iter 11600: loss=136.0463, time=77.09s\n",
            "iter 11600: dev acc=0.3560\n",
            "Iter 11700: loss=147.2748, time=77.74s\n",
            "iter 11700: dev acc=0.3688\n",
            "Iter 11800: loss=139.2998, time=78.38s\n",
            "iter 11800: dev acc=0.3633\n",
            "Iter 11900: loss=129.1201, time=79.03s\n",
            "iter 11900: dev acc=0.3678\n",
            "Iter 12000: loss=145.4579, time=79.68s\n",
            "iter 12000: dev acc=0.3733\n",
            "Iter 12100: loss=136.4097, time=80.33s\n",
            "iter 12100: dev acc=0.3588\n",
            "Iter 12200: loss=145.8868, time=80.97s\n",
            "iter 12200: dev acc=0.3588\n",
            "Iter 12300: loss=131.7442, time=81.61s\n",
            "iter 12300: dev acc=0.3497\n",
            "Iter 12400: loss=139.8602, time=82.25s\n",
            "iter 12400: dev acc=0.3470\n",
            "Iter 12500: loss=138.7866, time=82.90s\n",
            "iter 12500: dev acc=0.3551\n",
            "Iter 12600: loss=140.7963, time=83.54s\n",
            "iter 12600: dev acc=0.3588\n",
            "Iter 12700: loss=137.9759, time=84.17s\n",
            "iter 12700: dev acc=0.3433\n",
            "Iter 12800: loss=127.2901, time=84.81s\n",
            "iter 12800: dev acc=0.3551\n",
            "Iter 12900: loss=125.9827, time=85.46s\n",
            "iter 12900: dev acc=0.3642\n",
            "Iter 13000: loss=146.3665, time=86.09s\n",
            "iter 13000: dev acc=0.3551\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 11500: train acc=0.4507, dev acc=0.3733, test acc=0.3738\n",
            "Best Test Accuracy for seed 123: 0.3738\n",
            "Shuffling training data\n",
            "Iter 100: loss=161.3495, time=0.19s\n",
            "iter 100: dev acc=0.2352\n",
            "new highscore\n",
            "Iter 200: loss=153.5083, time=0.99s\n",
            "iter 200: dev acc=0.2707\n",
            "new highscore\n",
            "Iter 300: loss=157.7308, time=1.79s\n",
            "iter 300: dev acc=0.2716\n",
            "new highscore\n",
            "Iter 400: loss=163.0819, time=2.60s\n",
            "iter 400: dev acc=0.2534\n",
            "Iter 500: loss=153.7840, time=3.27s\n",
            "iter 500: dev acc=0.2607\n",
            "Iter 600: loss=159.8597, time=3.91s\n",
            "iter 600: dev acc=0.2616\n",
            "Iter 700: loss=163.2612, time=4.56s\n",
            "iter 700: dev acc=0.2570\n",
            "Iter 800: loss=159.4372, time=5.19s\n",
            "iter 800: dev acc=0.2470\n",
            "Iter 900: loss=152.8670, time=5.83s\n",
            "iter 900: dev acc=0.2552\n",
            "Iter 1000: loss=161.4045, time=6.46s\n",
            "iter 1000: dev acc=0.2543\n",
            "Iter 1100: loss=161.5527, time=7.09s\n",
            "iter 1100: dev acc=0.2761\n",
            "new highscore\n",
            "Iter 1200: loss=159.2959, time=7.89s\n",
            "iter 1200: dev acc=0.2743\n",
            "Iter 1300: loss=154.4947, time=8.52s\n",
            "iter 1300: dev acc=0.2470\n",
            "Iter 1400: loss=152.7288, time=9.16s\n",
            "iter 1400: dev acc=0.2825\n",
            "new highscore\n",
            "Iter 1500: loss=158.9801, time=9.96s\n",
            "iter 1500: dev acc=0.2807\n",
            "Iter 1600: loss=159.0137, time=10.60s\n",
            "iter 1600: dev acc=0.2298\n",
            "Iter 1700: loss=157.6234, time=11.24s\n",
            "iter 1700: dev acc=0.2779\n",
            "Iter 1800: loss=158.5389, time=11.89s\n",
            "iter 1800: dev acc=0.2906\n",
            "new highscore\n",
            "Iter 1900: loss=150.1430, time=12.69s\n",
            "iter 1900: dev acc=0.2870\n",
            "Iter 2000: loss=151.8930, time=13.33s\n",
            "iter 2000: dev acc=0.2943\n",
            "new highscore\n",
            "Iter 2100: loss=144.4163, time=14.11s\n",
            "iter 2100: dev acc=0.3061\n",
            "new highscore\n",
            "Iter 2200: loss=166.6429, time=14.90s\n",
            "iter 2200: dev acc=0.2988\n",
            "Iter 2300: loss=152.3230, time=15.54s\n",
            "iter 2300: dev acc=0.2934\n",
            "Iter 2400: loss=156.8229, time=16.18s\n",
            "iter 2400: dev acc=0.2925\n",
            "Iter 2500: loss=146.6713, time=16.82s\n",
            "iter 2500: dev acc=0.2961\n",
            "Iter 2600: loss=155.0729, time=17.46s\n",
            "iter 2600: dev acc=0.3061\n",
            "Iter 2700: loss=155.6860, time=18.09s\n",
            "iter 2700: dev acc=0.3124\n",
            "new highscore\n",
            "Iter 2800: loss=150.2930, time=18.88s\n",
            "iter 2800: dev acc=0.3106\n",
            "Iter 2900: loss=158.0811, time=19.52s\n",
            "iter 2900: dev acc=0.2943\n",
            "Iter 3000: loss=157.6074, time=20.16s\n",
            "iter 3000: dev acc=0.2888\n",
            "Iter 3100: loss=152.2119, time=20.79s\n",
            "iter 3100: dev acc=0.3025\n",
            "Iter 3200: loss=155.5562, time=21.42s\n",
            "iter 3200: dev acc=0.3261\n",
            "new highscore\n",
            "Iter 3300: loss=155.7375, time=22.22s\n",
            "iter 3300: dev acc=0.2934\n",
            "Iter 3400: loss=147.6356, time=22.86s\n",
            "iter 3400: dev acc=0.2961\n",
            "Iter 3500: loss=159.0695, time=23.51s\n",
            "iter 3500: dev acc=0.2570\n",
            "Iter 3600: loss=153.3910, time=24.15s\n",
            "iter 3600: dev acc=0.2788\n",
            "Iter 3700: loss=154.6276, time=24.79s\n",
            "iter 3700: dev acc=0.2825\n",
            "Iter 3800: loss=153.8078, time=25.43s\n",
            "iter 3800: dev acc=0.3179\n",
            "Iter 3900: loss=153.5411, time=26.06s\n",
            "iter 3900: dev acc=0.3143\n",
            "Iter 4000: loss=152.4264, time=26.69s\n",
            "iter 4000: dev acc=0.3015\n",
            "Iter 4100: loss=155.3074, time=27.33s\n",
            "iter 4100: dev acc=0.2716\n",
            "Iter 4200: loss=153.5682, time=27.97s\n",
            "iter 4200: dev acc=0.3025\n",
            "Iter 4300: loss=153.8540, time=28.61s\n",
            "iter 4300: dev acc=0.3206\n",
            "Iter 4400: loss=155.1103, time=29.24s\n",
            "iter 4400: dev acc=0.3306\n",
            "new highscore\n",
            "Iter 4500: loss=158.0910, time=30.04s\n",
            "iter 4500: dev acc=0.3070\n",
            "Iter 4600: loss=152.3191, time=30.67s\n",
            "iter 4600: dev acc=0.3170\n",
            "Iter 4700: loss=157.7118, time=31.31s\n",
            "iter 4700: dev acc=0.3052\n",
            "Iter 4800: loss=144.0614, time=31.94s\n",
            "iter 4800: dev acc=0.2943\n",
            "Iter 4900: loss=150.9352, time=32.58s\n",
            "iter 4900: dev acc=0.3025\n",
            "Iter 5000: loss=151.2955, time=33.21s\n",
            "iter 5000: dev acc=0.3025\n",
            "Iter 5100: loss=153.7384, time=33.85s\n",
            "iter 5100: dev acc=0.3088\n",
            "Iter 5200: loss=157.1428, time=34.49s\n",
            "iter 5200: dev acc=0.3179\n",
            "Iter 5300: loss=146.6381, time=35.14s\n",
            "iter 5300: dev acc=0.3061\n",
            "Iter 5400: loss=159.7282, time=35.78s\n",
            "iter 5400: dev acc=0.3170\n",
            "Iter 5500: loss=147.5822, time=36.42s\n",
            "iter 5500: dev acc=0.3015\n",
            "Iter 5600: loss=154.7446, time=37.06s\n",
            "iter 5600: dev acc=0.3043\n",
            "Iter 5700: loss=156.3921, time=37.69s\n",
            "iter 5700: dev acc=0.3115\n",
            "Iter 5800: loss=139.5726, time=38.33s\n",
            "iter 5800: dev acc=0.3215\n",
            "Iter 5900: loss=155.2684, time=38.97s\n",
            "iter 5900: dev acc=0.3097\n",
            "Iter 6000: loss=149.3217, time=39.61s\n",
            "iter 6000: dev acc=0.3297\n",
            "Iter 6100: loss=145.9093, time=40.25s\n",
            "iter 6100: dev acc=0.3006\n",
            "Iter 6200: loss=145.6389, time=40.89s\n",
            "iter 6200: dev acc=0.3388\n",
            "new highscore\n",
            "Iter 6300: loss=158.8919, time=41.69s\n",
            "iter 6300: dev acc=0.3124\n",
            "Iter 6400: loss=154.7005, time=42.32s\n",
            "iter 6400: dev acc=0.3361\n",
            "Iter 6500: loss=153.0607, time=42.95s\n",
            "iter 6500: dev acc=0.3179\n",
            "Iter 6600: loss=154.9987, time=43.59s\n",
            "iter 6600: dev acc=0.2897\n",
            "Iter 6700: loss=146.3056, time=44.22s\n",
            "iter 6700: dev acc=0.2943\n",
            "Iter 6800: loss=145.8216, time=44.86s\n",
            "iter 6800: dev acc=0.3034\n",
            "Iter 6900: loss=154.0807, time=45.49s\n",
            "iter 6900: dev acc=0.2988\n",
            "Iter 7000: loss=152.4102, time=46.15s\n",
            "iter 7000: dev acc=0.3179\n",
            "Iter 7100: loss=148.5154, time=46.79s\n",
            "iter 7100: dev acc=0.3270\n",
            "Iter 7200: loss=148.3407, time=47.43s\n",
            "iter 7200: dev acc=0.3288\n",
            "Iter 7300: loss=144.8931, time=48.06s\n",
            "iter 7300: dev acc=0.3270\n",
            "Iter 7400: loss=148.3011, time=48.70s\n",
            "iter 7400: dev acc=0.3306\n",
            "Iter 7500: loss=148.2574, time=49.34s\n",
            "iter 7500: dev acc=0.3261\n",
            "Iter 7600: loss=155.1059, time=49.98s\n",
            "iter 7600: dev acc=0.3243\n",
            "Iter 7700: loss=145.3980, time=50.61s\n",
            "iter 7700: dev acc=0.3215\n",
            "Iter 7800: loss=151.8929, time=51.24s\n",
            "iter 7800: dev acc=0.3224\n",
            "Iter 7900: loss=148.5827, time=51.88s\n",
            "iter 7900: dev acc=0.2906\n",
            "Iter 8000: loss=147.8281, time=52.52s\n",
            "iter 8000: dev acc=0.3224\n",
            "Iter 8100: loss=148.8998, time=53.16s\n",
            "iter 8100: dev acc=0.3279\n",
            "Iter 8200: loss=151.8403, time=53.81s\n",
            "iter 8200: dev acc=0.3261\n",
            "Iter 8300: loss=151.2342, time=54.46s\n",
            "iter 8300: dev acc=0.3415\n",
            "new highscore\n",
            "Iter 8400: loss=153.8709, time=55.26s\n",
            "iter 8400: dev acc=0.3351\n",
            "Iter 8500: loss=144.8536, time=55.90s\n",
            "iter 8500: dev acc=0.3351\n",
            "Shuffling training data\n",
            "Iter 8600: loss=136.6558, time=56.55s\n",
            "iter 8600: dev acc=0.3342\n",
            "Iter 8700: loss=119.8981, time=57.19s\n",
            "iter 8700: dev acc=0.3315\n",
            "Iter 8800: loss=135.6702, time=57.83s\n",
            "iter 8800: dev acc=0.3361\n",
            "Iter 8900: loss=132.9973, time=58.48s\n",
            "iter 8900: dev acc=0.3288\n",
            "Iter 9000: loss=133.6212, time=59.13s\n",
            "iter 9000: dev acc=0.3351\n",
            "Iter 9100: loss=141.5822, time=59.77s\n",
            "iter 9100: dev acc=0.3361\n",
            "Iter 9200: loss=142.7035, time=60.40s\n",
            "iter 9200: dev acc=0.3388\n",
            "Iter 9300: loss=132.3083, time=61.03s\n",
            "iter 9300: dev acc=0.3233\n",
            "Iter 9400: loss=138.9962, time=61.67s\n",
            "iter 9400: dev acc=0.3297\n",
            "Iter 9500: loss=133.5695, time=62.31s\n",
            "iter 9500: dev acc=0.3361\n",
            "Iter 9600: loss=140.7936, time=62.95s\n",
            "iter 9600: dev acc=0.3061\n",
            "Iter 9700: loss=128.8337, time=63.58s\n",
            "iter 9700: dev acc=0.3197\n",
            "Iter 9800: loss=127.7076, time=64.26s\n",
            "iter 9800: dev acc=0.3324\n",
            "Iter 9900: loss=139.0625, time=64.91s\n",
            "iter 9900: dev acc=0.3215\n",
            "Iter 10000: loss=139.8320, time=65.55s\n",
            "iter 10000: dev acc=0.3424\n",
            "new highscore\n",
            "Iter 10100: loss=139.1417, time=66.35s\n",
            "iter 10100: dev acc=0.3270\n",
            "Iter 10200: loss=146.1480, time=66.99s\n",
            "iter 10200: dev acc=0.3506\n",
            "new highscore\n",
            "Iter 10300: loss=136.1984, time=67.78s\n",
            "iter 10300: dev acc=0.3451\n",
            "Iter 10400: loss=132.4993, time=68.42s\n",
            "iter 10400: dev acc=0.3342\n",
            "Iter 10500: loss=135.9706, time=69.06s\n",
            "iter 10500: dev acc=0.3288\n",
            "Iter 10600: loss=142.6958, time=69.72s\n",
            "iter 10600: dev acc=0.3442\n",
            "Iter 10700: loss=137.8282, time=70.38s\n",
            "iter 10700: dev acc=0.3479\n",
            "Iter 10800: loss=132.3138, time=71.05s\n",
            "iter 10800: dev acc=0.3542\n",
            "new highscore\n",
            "Iter 10900: loss=145.0813, time=71.85s\n",
            "iter 10900: dev acc=0.3324\n",
            "Iter 11000: loss=139.0130, time=72.48s\n",
            "iter 11000: dev acc=0.3506\n",
            "Iter 11100: loss=150.2017, time=73.12s\n",
            "iter 11100: dev acc=0.3470\n",
            "Iter 11200: loss=134.3522, time=73.76s\n",
            "iter 11200: dev acc=0.3206\n",
            "Iter 11300: loss=136.8245, time=74.40s\n",
            "iter 11300: dev acc=0.3624\n",
            "new highscore\n",
            "Iter 11400: loss=139.3558, time=75.20s\n",
            "iter 11400: dev acc=0.3579\n",
            "Iter 11500: loss=142.6224, time=75.85s\n",
            "iter 11500: dev acc=0.3406\n",
            "Iter 11600: loss=129.1438, time=76.49s\n",
            "iter 11600: dev acc=0.3442\n",
            "Iter 11700: loss=129.9211, time=77.13s\n",
            "iter 11700: dev acc=0.3497\n",
            "Iter 11800: loss=139.4239, time=77.77s\n",
            "iter 11800: dev acc=0.3315\n",
            "Iter 11900: loss=127.4594, time=78.42s\n",
            "iter 11900: dev acc=0.3388\n",
            "Iter 12000: loss=138.8194, time=79.05s\n",
            "iter 12000: dev acc=0.3542\n",
            "Iter 12100: loss=131.8910, time=79.68s\n",
            "iter 12100: dev acc=0.3451\n",
            "Iter 12200: loss=140.0188, time=80.32s\n",
            "iter 12200: dev acc=0.3533\n",
            "Iter 12300: loss=141.1662, time=80.98s\n",
            "iter 12300: dev acc=0.3497\n",
            "Iter 12400: loss=149.1820, time=81.62s\n",
            "iter 12400: dev acc=0.3361\n",
            "Iter 12500: loss=146.6561, time=82.27s\n",
            "iter 12500: dev acc=0.3433\n",
            "Iter 12600: loss=129.7817, time=82.91s\n",
            "iter 12600: dev acc=0.3479\n",
            "Iter 12700: loss=128.4627, time=83.55s\n",
            "iter 12700: dev acc=0.3506\n",
            "Iter 12800: loss=141.1794, time=84.18s\n",
            "iter 12800: dev acc=0.3424\n",
            "Iter 12900: loss=133.1439, time=84.81s\n",
            "iter 12900: dev acc=0.3025\n",
            "Iter 13000: loss=142.4080, time=85.44s\n",
            "iter 13000: dev acc=0.2925\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 11300: train acc=0.4608, dev acc=0.3624, test acc=0.3570\n",
            "Best Test Accuracy for seed 999: 0.3570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Compute mean and standard deviation across seeds\n",
        "    mean_accuracy = np.mean(test_accuracies_dcbow)\n",
        "    std_accuracy = np.std(test_accuracies_dcbow)\n",
        "    print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "    print(f\"Standard Deviation: {std_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "uiuBCMSEbHqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZ5flHwiiHY"
      },
      "source": [
        "# Pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NX35vecmHy6"
      },
      "source": [
        "The Stanford Sentiment Treebank is a rather small data set, since it required fine-grained manual annotatation. This makes it difficult for the Deep CBOW model to learn good word embeddings, i.e. to learn good word representations for the words in our vocabulary.\n",
        "In fact, the only error signal that the network receives is from predicting the sentiment of entire sentences!\n",
        "\n",
        "To start off with better word representations, we can download **pre-trained word embeddings**.\n",
        "You can choose which pre-trained word embeddings to use:\n",
        "\n",
        "- **GloVe**. The \"original\" Stanford Sentiment classification [paper](http://aclweb.org/anthology/P/P15/P15-1150.pdf) used Glove embeddings, which are just another method (like *word2vec*) to get word embeddings from unannotated text. Glove is described in the following paper which you should cite if you use them:\n",
        "> Jeffrey Pennington, Richard Socher, and Christopher Manning. [\"Glove: Global vectors for word representation.\"](https://nlp.stanford.edu/pubs/glove.pdf) EMNLP 2014.\n",
        "\n",
        "- **Word2Vec**. This is the method that you learned about in class, described in:\n",
        "> Mikolov, Tomas, et al. [\"Distributed representations of words and phrases and their compositionality.\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) Advances in neural information processing systems. 2013.\n",
        "\n",
        "Using these pre-trained word embeddings, we can initialize our word embedding lookup table and start form a point where similar words are already close to one another in the distributional semantic space.\n",
        "\n",
        "You can choose to keep the word embeddings **fixed** or to train them further, specialising them to the task at hand.\n",
        "We will keep them fixed for now.\n",
        "\n",
        "For the purposes of this lab, it is enough if you understand how word2vec works (whichever vectors you use), but if you are interested, we encourage you to also check out the GloVe paper.\n",
        "\n",
        "You can either download the word2vec vectors, or the Glove vectors.\n",
        "If you want to compare your results to the Stanford paper later on, then you should use Glove.\n",
        "**At the end of this lab you have the option to compare which vectors give you the best performance. For now, simply choose one of them and continue with that.**\n",
        "\n",
        "[**OPTIONAL in case you don't want to mount Google Drive:** instead of running all the 5 boxes below, you can 1) download the GloVe and word2vec in your local machine, 2) upload them on your Drive folder (\"My Drive\"). Then, uncomment the first 2 lines in box 6 before writing your code!]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "QTHqbBZN3WrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a6d935-da0d-4c68-cf52-cb2008797ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading glove file...\n",
            "File downloaded and saved as glove.filtered.txt\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# URL to the glove file\n",
        "url = \"https://gist.githubusercontent.com/bastings/b094de2813da58056a05e8e7950d4ad1/raw/3fbd3976199c2b88de2ae62afc0ecc6f15e6f7ce/glove.840B.300d.sst.txt\"\n",
        "\n",
        "# File name to save the downloaded file\n",
        "file_name = \"glove.filtered.txt\"\n",
        "\n",
        "# Download the file\n",
        "print(\"Downloading glove file...\")\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the file locally\n",
        "with open(file_name, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"File downloaded and saved as {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qgGCNe7R4Qqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23bc89ca-4a40-4708-a5de-be76f735599e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 20725 word embeddings!\n",
            "Vocabulary size: 20727\n",
            "First 10 vocabulary entries:\n",
            "0: <unk>\n",
            "1: <pad>\n",
            "2: ,\n",
            "3: .\n",
            "4: the\n",
            "5: and\n",
            "6: to\n",
            "7: of\n",
            "8: a\n",
            "9: in\n"
          ]
        }
      ],
      "source": [
        "# Parse the Word2Vec file\n",
        "glove = {}\n",
        "with open(\"glove.filtered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.strip().split()\n",
        "        word = values[0]  # The first value is the word\n",
        "        vector = [float(x) for x in values[1:]]  # The rest are the embedding values\n",
        "        glove[word] = vector\n",
        "\n",
        "print(f\"Loaded {len(glove)} word embeddings!\")\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Initialize vocabulary object\n",
        "v = Vocabulary()\n",
        "\n",
        "# Add special tokens manually\n",
        "v.add_token(\"<unk>\")\n",
        "v.add_token(\"<pad>\")\n",
        "\n",
        "# Define embedding dimensions and initialize list for vectors\n",
        "embedding_dim = 300  # Update if using different dimensions\n",
        "vectors = []\n",
        "\n",
        "# Add special token embeddings\n",
        "vectors.append(np.random.uniform(-0.01, 0.01, embedding_dim))  # <unk>\n",
        "vectors.append(np.zeros(embedding_dim))  # <pad>\n",
        "\n",
        "# Add pre-trained word embeddings to vocabulary\n",
        "for word, vector in glove.items():\n",
        "    if word not in v.w2i:  # If the word is not already in the vocab\n",
        "        v.add_token(word)\n",
        "        vectors.append(vector)\n",
        "\n",
        "# Convert vectors to a NumPy array\n",
        "vectors = np.stack(vectors, axis=0)\n",
        "\n",
        "# Print vocabulary size and first few entries\n",
        "print(f\"Vocabulary size: {len(v.w2i)}\")\n",
        "print(\"First 10 vocabulary entries:\")\n",
        "for i, word in enumerate(v.i2w[:10]):\n",
        "    print(f\"{i}: {word}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "l8Z1igvpTrZq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DeepCBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, vocab, num_classes, embedding_dim=300, hidden_dim=100):\n",
        "        \"\"\"\n",
        "        Deep CBOW Model\n",
        "        :param vocab_size: Size of the vocabulary\n",
        "        :param num_classes: Number of output classes\n",
        "        :param embedding_dim: Size of the word embeddings (E)\n",
        "        :param hidden_dim: Size of the hidden layers (D)\n",
        "        \"\"\"\n",
        "        super(DeepCBOW, self).__init__()\n",
        "\n",
        "        # Embedding layer: Map word indices to embedding vectors\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Output layer: Stack of linear layers + tanh activations\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),  # E -> D\n",
        "            nn.Tanh(),                            # Non-linearity\n",
        "            nn.Linear(hidden_dim, hidden_dim),    # D -> D\n",
        "            nn.Tanh(),                            # Non-linearity\n",
        "            nn.Linear(hidden_dim, num_classes)    # D -> 5\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Deep CBOW model\n",
        "        :param x: Input word indices (batch_size, seq_len)\n",
        "        :return: Logits (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # 1. Look up embeddings for each word in the input\n",
        "        embeddings = self.embed(x)  # Shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        # 2. Sum embeddings along the sequence length dimension\n",
        "        sentence_vector = embeddings.sum(dim=1)  # Shape: [batch_size, embedding_dim]\n",
        "\n",
        "        # 3. Pass through the output layer\n",
        "        logits = self.output_layer(sentence_vector)  # Shape: [batch_size, num_classes]\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfEd38W0NnAI"
      },
      "source": [
        "#### Exercise: train Deep CBOW with (fixed) pre-trained embeddings\n",
        "\n",
        "Now train Deep CBOW again using the pre-trained word vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GcMFdNzWvVi"
      },
      "source": [
        "Create a function to set set for reproducbility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "z_6ooqgEsB20"
      },
      "outputs": [],
      "source": [
        "# We define a new class that inherits from DeepCBOW.\n",
        "class PTDeepCBOW(DeepCBOW):\n",
        "  def __init__(self, vocab_size, vocab, output_dim, embedding_dim, hidden_dim):\n",
        "    super(PTDeepCBOW, self).__init__(\n",
        "        vocab_size, vocab, output_dim, embedding_dim, hidden_dim)\n",
        "    self.vocab = vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FBC3OlA1oF3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb0c1be-eed6-440d-b463-0781b2e820cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling training data\n",
            "Iter 1000: loss=1455.3107, time=1.55s\n",
            "iter 1000: dev acc=0.3942\n",
            "new highscore\n",
            "Iter 2000: loss=1390.2019, time=3.59s\n",
            "iter 2000: dev acc=0.3470\n",
            "Iter 3000: loss=1388.0997, time=5.58s\n",
            "iter 3000: dev acc=0.4005\n",
            "new highscore\n",
            "Iter 4000: loss=1351.8830, time=7.61s\n",
            "iter 4000: dev acc=0.4169\n",
            "new highscore\n",
            "Iter 5000: loss=1358.7709, time=9.65s\n",
            "iter 5000: dev acc=0.4160\n",
            "Iter 6000: loss=1348.6428, time=11.66s\n",
            "iter 6000: dev acc=0.3969\n",
            "Iter 7000: loss=1290.5094, time=13.67s\n",
            "iter 7000: dev acc=0.4178\n",
            "new highscore\n",
            "Iter 8000: loss=1356.0794, time=15.72s\n",
            "iter 8000: dev acc=0.4105\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1329.9029, time=17.70s\n",
            "iter 9000: dev acc=0.4078\n",
            "Iter 10000: loss=1301.8730, time=19.68s\n",
            "iter 10000: dev acc=0.3960\n",
            "Iter 11000: loss=1298.8526, time=21.68s\n",
            "iter 11000: dev acc=0.4096\n",
            "Iter 12000: loss=1327.6230, time=23.69s\n",
            "iter 12000: dev acc=0.4169\n",
            "Iter 13000: loss=1293.6260, time=25.68s\n",
            "iter 13000: dev acc=0.4015\n",
            "Iter 14000: loss=1298.6122, time=27.66s\n",
            "iter 14000: dev acc=0.4096\n",
            "Iter 15000: loss=1288.4036, time=29.65s\n",
            "iter 15000: dev acc=0.4060\n",
            "Iter 16000: loss=1320.6305, time=31.64s\n",
            "iter 16000: dev acc=0.4178\n",
            "Iter 17000: loss=1311.3201, time=33.67s\n",
            "iter 17000: dev acc=0.4078\n",
            "Shuffling training data\n",
            "Iter 18000: loss=1246.3742, time=35.67s\n",
            "iter 18000: dev acc=0.4369\n",
            "new highscore\n",
            "Iter 19000: loss=1266.7115, time=37.80s\n",
            "iter 19000: dev acc=0.3896\n",
            "Iter 20000: loss=1324.7783, time=39.84s\n",
            "iter 20000: dev acc=0.3960\n",
            "Iter 21000: loss=1291.1128, time=41.83s\n",
            "iter 21000: dev acc=0.4151\n",
            "Iter 22000: loss=1250.3552, time=43.85s\n",
            "iter 22000: dev acc=0.4242\n",
            "Iter 23000: loss=1257.3559, time=45.82s\n",
            "iter 23000: dev acc=0.4387\n",
            "new highscore\n",
            "Iter 24000: loss=1274.1161, time=47.94s\n",
            "iter 24000: dev acc=0.3906\n",
            "Iter 25000: loss=1308.5712, time=49.93s\n",
            "iter 25000: dev acc=0.4305\n",
            "Shuffling training data\n",
            "Iter 26000: loss=1270.6106, time=51.92s\n",
            "iter 26000: dev acc=0.4314\n",
            "Iter 27000: loss=1270.5364, time=53.92s\n",
            "iter 27000: dev acc=0.3851\n",
            "Iter 28000: loss=1277.9315, time=55.91s\n",
            "iter 28000: dev acc=0.4214\n",
            "Iter 29000: loss=1231.0974, time=57.90s\n",
            "iter 29000: dev acc=0.4178\n",
            "Iter 30000: loss=1279.0054, time=59.90s\n",
            "iter 30000: dev acc=0.4114\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-07f34c5b57a1>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 23000: train acc=0.4686, dev acc=0.4387, test acc=0.4534\n",
            "Best Test Accuracy for seed 42: 0.4534\n",
            "Shuffling training data\n",
            "Iter 1000: loss=1497.4186, time=1.55s\n",
            "iter 1000: dev acc=0.3924\n",
            "new highscore\n",
            "Iter 2000: loss=1393.2240, time=3.62s\n",
            "iter 2000: dev acc=0.3860\n",
            "Iter 3000: loss=1335.4659, time=5.63s\n",
            "iter 3000: dev acc=0.4005\n",
            "new highscore\n",
            "Iter 4000: loss=1351.2476, time=7.72s\n",
            "iter 4000: dev acc=0.3851\n",
            "Iter 5000: loss=1340.8471, time=9.71s\n",
            "iter 5000: dev acc=0.3678\n",
            "Iter 6000: loss=1299.4600, time=11.70s\n",
            "iter 6000: dev acc=0.4051\n",
            "new highscore\n",
            "Iter 7000: loss=1363.5977, time=13.71s\n",
            "iter 7000: dev acc=0.4287\n",
            "new highscore\n",
            "Iter 8000: loss=1364.0745, time=15.73s\n",
            "iter 8000: dev acc=0.4205\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1323.9758, time=17.78s\n",
            "iter 9000: dev acc=0.4169\n",
            "Iter 10000: loss=1293.4403, time=19.78s\n",
            "iter 10000: dev acc=0.4151\n",
            "Iter 11000: loss=1318.1495, time=21.78s\n",
            "iter 11000: dev acc=0.3797\n",
            "Iter 12000: loss=1287.7797, time=23.77s\n",
            "iter 12000: dev acc=0.4087\n",
            "Iter 13000: loss=1312.1938, time=25.77s\n",
            "iter 13000: dev acc=0.3815\n",
            "Iter 14000: loss=1275.2214, time=27.76s\n",
            "iter 14000: dev acc=0.4114\n",
            "Iter 15000: loss=1298.4356, time=29.83s\n",
            "iter 15000: dev acc=0.3878\n",
            "Iter 16000: loss=1321.1343, time=31.82s\n",
            "iter 16000: dev acc=0.4360\n",
            "new highscore\n",
            "Iter 17000: loss=1302.3276, time=33.86s\n",
            "iter 17000: dev acc=0.4169\n",
            "Shuffling training data\n",
            "Iter 18000: loss=1294.2024, time=35.85s\n",
            "iter 18000: dev acc=0.4005\n",
            "Iter 19000: loss=1263.0959, time=37.84s\n",
            "iter 19000: dev acc=0.4060\n",
            "Iter 20000: loss=1268.7968, time=39.85s\n",
            "iter 20000: dev acc=0.3969\n",
            "Iter 21000: loss=1245.9636, time=41.89s\n",
            "iter 21000: dev acc=0.4405\n",
            "new highscore\n",
            "Iter 22000: loss=1287.4554, time=43.95s\n",
            "iter 22000: dev acc=0.4024\n",
            "Iter 23000: loss=1299.4184, time=45.98s\n",
            "iter 23000: dev acc=0.4096\n",
            "Iter 24000: loss=1286.3450, time=47.98s\n",
            "iter 24000: dev acc=0.4178\n",
            "Iter 25000: loss=1294.7124, time=50.01s\n",
            "iter 25000: dev acc=0.4133\n",
            "Shuffling training data\n",
            "Iter 26000: loss=1282.7846, time=52.07s\n",
            "iter 26000: dev acc=0.4133\n",
            "Iter 27000: loss=1252.8054, time=54.11s\n",
            "iter 27000: dev acc=0.4242\n",
            "Iter 28000: loss=1246.1941, time=56.11s\n",
            "iter 28000: dev acc=0.4151\n",
            "Iter 29000: loss=1254.6155, time=58.13s\n",
            "iter 29000: dev acc=0.4278\n",
            "Iter 30000: loss=1272.5638, time=60.12s\n",
            "iter 30000: dev acc=0.4305\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 21000: train acc=0.4739, dev acc=0.4405, test acc=0.4357\n",
            "Best Test Accuracy for seed 123: 0.4357\n",
            "Shuffling training data\n",
            "Iter 1000: loss=1482.3304, time=1.56s\n",
            "iter 1000: dev acc=0.3969\n",
            "new highscore\n",
            "Iter 2000: loss=1394.3348, time=3.71s\n",
            "iter 2000: dev acc=0.3560\n",
            "Iter 3000: loss=1386.6983, time=5.78s\n",
            "iter 3000: dev acc=0.4005\n",
            "new highscore\n",
            "Iter 4000: loss=1329.9871, time=7.89s\n",
            "iter 4000: dev acc=0.4024\n",
            "new highscore\n",
            "Iter 5000: loss=1353.4255, time=10.03s\n",
            "iter 5000: dev acc=0.3924\n",
            "Iter 6000: loss=1322.2522, time=12.06s\n",
            "iter 6000: dev acc=0.4060\n",
            "new highscore\n",
            "Iter 7000: loss=1318.8302, time=14.08s\n",
            "iter 7000: dev acc=0.3660\n",
            "Iter 8000: loss=1325.0693, time=16.05s\n",
            "iter 8000: dev acc=0.3660\n",
            "Shuffling training data\n",
            "Iter 9000: loss=1316.6658, time=18.04s\n",
            "iter 9000: dev acc=0.4078\n",
            "new highscore\n",
            "Iter 10000: loss=1284.9460, time=20.09s\n",
            "iter 10000: dev acc=0.3896\n",
            "Iter 11000: loss=1304.7957, time=22.08s\n",
            "iter 11000: dev acc=0.3906\n",
            "Iter 12000: loss=1281.3075, time=24.12s\n",
            "iter 12000: dev acc=0.4105\n",
            "new highscore\n",
            "Iter 13000: loss=1293.4429, time=26.22s\n",
            "iter 13000: dev acc=0.4060\n",
            "Iter 14000: loss=1302.5269, time=28.20s\n",
            "iter 14000: dev acc=0.4169\n",
            "new highscore\n",
            "Iter 15000: loss=1318.7805, time=30.24s\n",
            "iter 15000: dev acc=0.4151\n",
            "Iter 16000: loss=1326.5435, time=32.22s\n",
            "iter 16000: dev acc=0.4142\n",
            "Iter 17000: loss=1317.9936, time=34.21s\n",
            "iter 17000: dev acc=0.4214\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 18000: loss=1278.9360, time=36.29s\n",
            "iter 18000: dev acc=0.4287\n",
            "new highscore\n",
            "Iter 19000: loss=1314.0471, time=38.32s\n",
            "iter 19000: dev acc=0.4242\n",
            "Iter 20000: loss=1273.9717, time=40.30s\n",
            "iter 20000: dev acc=0.4105\n",
            "Iter 21000: loss=1326.5590, time=42.27s\n",
            "iter 21000: dev acc=0.4187\n",
            "Iter 22000: loss=1266.0315, time=44.25s\n",
            "iter 22000: dev acc=0.4124\n",
            "Iter 23000: loss=1283.5453, time=46.24s\n",
            "iter 23000: dev acc=0.4242\n",
            "Iter 24000: loss=1266.7952, time=48.23s\n",
            "iter 24000: dev acc=0.4124\n",
            "Iter 25000: loss=1278.4266, time=50.21s\n",
            "iter 25000: dev acc=0.4114\n",
            "Shuffling training data\n",
            "Iter 26000: loss=1237.6487, time=52.19s\n",
            "iter 26000: dev acc=0.3906\n",
            "Iter 27000: loss=1249.5973, time=54.16s\n",
            "iter 27000: dev acc=0.4178\n",
            "Iter 28000: loss=1244.1429, time=56.14s\n",
            "iter 28000: dev acc=0.4087\n",
            "Iter 29000: loss=1276.2755, time=58.14s\n",
            "iter 29000: dev acc=0.4051\n",
            "Iter 30000: loss=1255.8756, time=60.13s\n",
            "iter 30000: dev acc=0.3915\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 18000: train acc=0.4638, dev acc=0.4287, test acc=0.4367\n",
            "Best Test Accuracy for seed 999: 0.4367\n"
          ]
        }
      ],
      "source": [
        "seeds = [42, 123, 999]\n",
        "test_accuracies = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Initialize model\n",
        "    model = PTDeepCBOW(\n",
        "        vocab_size=len(v.w2i),\n",
        "        vocab=v,\n",
        "        embedding_dim=300,\n",
        "        hidden_dim=100,\n",
        "        output_dim=5\n",
        "    ).to(device)\n",
        "    model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "    model.embed.weight.requires_grad = False  # Freeze pre-trained embeddings\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "    # Train model\n",
        "    losses, accuracies, test_acc = train_model(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        num_iterations=30000,  # Number of training iterations\n",
        "        print_every=1000,      # Print loss every 1000 iterations\n",
        "        eval_every=1000,       # Evaluate on validation set every 1000 iterations\n",
        "        batch_fn=get_examples, # Your batch generation function\n",
        "        prep_fn=prepare_example,\n",
        "        eval_fn=simple_evaluate,\n",
        "        batch_size=32,         # Training batch size\n",
        "        eval_batch_size=64     # Validation/test batch size\n",
        "    )\n",
        "\n",
        "    print(f\"Best Test Accuracy for seed {seed}: {test_acc:.4f}\")\n",
        "    test_accuracies.append(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and standard deviation across seeds\n",
        "mean_accuracy = np.mean(test_accuracies)\n",
        "std_accuracy = np.std(test_accuracies)\n",
        "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation: {std_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "czKGQYTcbLNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g41yW4PL9jG0"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "zJ9m5kLMd7-v"
      },
      "outputs": [],
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "  \"\"\"Our own LSTM cell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(MyLSTMCell, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "\n",
        "# Define weights for input gates\n",
        "    self.W_ii = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.W_if = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.W_ig = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.W_io = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "\n",
        "        # Define weights for hidden state gates\n",
        "    self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "\n",
        "        # Define biases\n",
        "    self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_g = nn.Parameter(torch.Tensor(hidden_size))\n",
        "    self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, input_, hx, mask=None):\n",
        "    \"\"\"\n",
        "    input is (batch, input_size)\n",
        "    hx is ((batch, hidden_size), (batch, hidden_size))\n",
        "    \"\"\"\n",
        "    prev_h, prev_c = hx\n",
        "\n",
        "    # Compute the input gate\n",
        "    i = torch.sigmoid(\n",
        "            torch.mm(input_, self.W_ii.T) + torch.mm(prev_h, self.W_hi.T) + self.b_i\n",
        "        )\n",
        "    # Compute the forget gate\n",
        "    f = torch.sigmoid(\n",
        "        torch.mm(input_, self.W_if.T) + torch.mm(prev_h, self.W_hf.T) + self.b_f\n",
        "    )\n",
        "    # Compute the candidate cell state\n",
        "    g = torch.tanh(\n",
        "        torch.mm(input_, self.W_ig.T) + torch.mm(prev_h, self.W_hg.T) + self.b_g\n",
        "    )\n",
        "     # Compute the output gate\n",
        "    o = torch.sigmoid(\n",
        "        torch.mm(input_, self.W_io.T) + torch.mm(prev_h, self.W_ho.T) + self.b_o\n",
        "    )\n",
        "\n",
        "        # Update the cell state\n",
        "    c = f * prev_c + i * g\n",
        "\n",
        "        # Compute the hidden state\n",
        "    h = o * torch.tanh(c)\n",
        "\n",
        "    return h, c\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"{}({:d}, {:d})\".format(\n",
        "        self.__class__.__name__, self.input_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9gA-UcqSBe0"
      },
      "source": [
        "#### LSTM Classifier\n",
        "\n",
        "Having an LSTM cell is not enough: we still need some code that calls it repeatedly, and then makes a prediction from the final hidden state.\n",
        "You will find that code below. Make sure that you understand it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3iuYZm5poEn5"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "  \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(LSTMClassifier, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
        "\n",
        "    self.output_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),  # explained later\n",
        "        nn.Linear(hidden_dim, output_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
        "    T = x.size(1)  # timesteps (the number of words in the sentence)\n",
        "\n",
        "    input_ = self.embed(x)\n",
        "\n",
        "    # here we create initial hidden states containing zeros\n",
        "    # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
        "    hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "    cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
        "\n",
        "    # process input sentences one word/timestep at a time\n",
        "    # input is batch-major (i.e., batch size is the first dimension)\n",
        "    # so the first word(s) is (are) input_[:, 0]\n",
        "    outputs = []\n",
        "    for i in range(T):\n",
        "      hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
        "      outputs.append(hx)\n",
        "\n",
        "    # if we have a single example, our final LSTM state is the last hx\n",
        "    if B == 1:\n",
        "      final = hx\n",
        "    else:\n",
        "      #\n",
        "      # This part is explained in next section, ignore this else-block for now.\n",
        "      #\n",
        "      # We processed sentences with different lengths, so some of the sentences\n",
        "      # had already finished and we have been adding padding inputs to hx.\n",
        "      # We select the final state based on the length of each sentence.\n",
        "\n",
        "      # two lines below not needed if using LSTM from pytorch\n",
        "      outputs = torch.stack(outputs, dim=0)           # [T, B, D]\n",
        "      outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
        "\n",
        "      # to be super-sure we're not accidentally indexing the wrong state\n",
        "      # we zero out positions that are invalid\n",
        "      pad_positions = (x == 1).unsqueeze(-1)\n",
        "\n",
        "      outputs = outputs.contiguous()\n",
        "      outputs = outputs.masked_fill_(pad_positions, 0.)\n",
        "\n",
        "      mask = (x != 1)  # true for valid positions [B, T]\n",
        "      lengths = mask.sum(dim=1)                 # [B, 1]\n",
        "\n",
        "      indexes = (lengths - 1) + torch.arange(B, device=x.device, dtype=x.dtype) * T\n",
        "      final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
        "\n",
        "    # we use the last hidden state to classify the sentence\n",
        "    logits = self.output_layer(final)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEw6XHQY_AAQ"
      },
      "source": [
        "# Mini-batching\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "IoAE2JBiXJ3P"
      },
      "outputs": [],
      "source": [
        "def get_minibatch(data, batch_size=25, shuffle=True):\n",
        "  \"\"\"Return minibatches, optional shuffling\"\"\"\n",
        "\n",
        "  if shuffle:\n",
        "    print(\"Shuffling training data\")\n",
        "    random.shuffle(data)  # shuffle training data each epoch\n",
        "\n",
        "  batch = []\n",
        "\n",
        "  # yield minibatches\n",
        "  for example in data:\n",
        "    batch.append(example)\n",
        "\n",
        "    if len(batch) == batch_size:\n",
        "      yield batch\n",
        "      batch = []\n",
        "\n",
        "  # in case there is something left\n",
        "  if len(batch) > 0:\n",
        "    yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwZM-XYkT8Zx"
      },
      "source": [
        "#### Padding function\n",
        "We will need a function that adds padding 1s to a sequence of IDs so that\n",
        "it becomes as long as the longest sequence in the minibatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp0sK1ghw4Ft",
        "outputId": "e62f8607-d8fe-4416-b0e8-f160eec96a82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "def pad(tokens, length, pad_value=1):\n",
        "  \"\"\"add padding 1s to a sequence to that it has the desired length\"\"\"\n",
        "  return tokens + [pad_value] * (length - len(tokens))\n",
        "\n",
        "# example\n",
        "tokens = [2, 3, 4]\n",
        "pad(tokens, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL2iixMYUgfh"
      },
      "source": [
        "#### New `prepare` function\n",
        "\n",
        "We will also need a new function that turns a mini-batch into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ZID0cqozWks8"
      },
      "outputs": [],
      "source": [
        "def prepare_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Minibatch is a list of examples.\n",
        "  This function converts words to IDs and returns\n",
        "  torch tensors to be used as input/targets.\n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "\n",
        "  # vocab returns 0 if the word is not there\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen) for ex in mb]\n",
        "\n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "OwDAtCv1x2hB"
      },
      "outputs": [],
      "source": [
        "# Let's test our new function.\n",
        "# This should give us 3 examples.\n",
        "mb = next(get_minibatch(train_data, batch_size=3, shuffle=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "dg8zEK8zyUCH"
      },
      "outputs": [],
      "source": [
        "# We should find padding 1s at the end\n",
        "x, y = prepare_minibatch(mb, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYBJEoSNUwI0"
      },
      "source": [
        "#### Evaluate (mini-batch version)\n",
        "\n",
        "We can now update our evaluation function to use mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "eiZZpEghzqou"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data,\n",
        "             batch_fn=get_minibatch, prep_fn=prepare_minibatch,\n",
        "             batch_size=16):\n",
        "  \"\"\"Accuracy of a model on given data set (using mini-batches)\"\"\"\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.eval()  # disable dropout\n",
        "\n",
        "  for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
        "    x, targets = prep_fn(mb, model.vocab)\n",
        "    with torch.no_grad():\n",
        "      logits = model(x)\n",
        "\n",
        "    predictions = logits.argmax(dim=-1).view(-1)\n",
        "\n",
        "    # add the number of correct predictions to the total correct\n",
        "    correct += (predictions == targets.view(-1)).sum().item()\n",
        "    total += targets.size(0)\n",
        "\n",
        "  return correct, total, correct / float(total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23wAZomozh_2"
      },
      "source": [
        "# LSTM (Mini-batched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-gkPU7jzBe2"
      },
      "source": [
        "With this, let's run the LSTM again but now using mini-batches!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "226Xg9OPzFbA",
        "outputId": "28a09059-a971-41c7-a69d-1cd83297b1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMClassifier(\n",
            "  (embed): Embedding(20727, 300, padding_idx=1)\n",
            "  (rnn): MyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [20727, 300] requires_grad=False\n",
            "rnn.W_ii                 [168, 300]   requires_grad=True\n",
            "rnn.W_if                 [168, 300]   requires_grad=True\n",
            "rnn.W_ig                 [168, 300]   requires_grad=True\n",
            "rnn.W_io                 [168, 300]   requires_grad=True\n",
            "rnn.W_hi                 [168, 168]   requires_grad=True\n",
            "rnn.W_hf                 [168, 168]   requires_grad=True\n",
            "rnn.W_hg                 [168, 168]   requires_grad=True\n",
            "rnn.W_ho                 [168, 168]   requires_grad=True\n",
            "rnn.b_i                  [168]        requires_grad=True\n",
            "rnn.b_f                  [168]        requires_grad=True\n",
            "rnn.b_g                  [168]        requires_grad=True\n",
            "rnn.b_o                  [168]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 6534113\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=374.7291, time=13.50s\n",
            "iter 250: dev acc=0.3915\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=333.6250, time=27.32s\n",
            "iter 500: dev acc=0.4024\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=321.5325, time=41.33s\n",
            "iter 750: dev acc=0.4078\n",
            "new highscore\n",
            "Iter 1000: loss=315.2297, time=55.35s\n",
            "iter 1000: dev acc=0.4351\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=307.4664, time=69.50s\n",
            "iter 1250: dev acc=0.4423\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=302.3626, time=83.48s\n",
            "iter 1500: dev acc=0.4387\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-07f34c5b57a1>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 1250: train acc=0.4837, dev acc=0.4423, test acc=0.4629\n",
            "Best Test Accuracy for seed 42: 0.4629\n",
            "LSTMClassifier(\n",
            "  (embed): Embedding(20727, 300, padding_idx=1)\n",
            "  (rnn): MyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [20727, 300] requires_grad=False\n",
            "rnn.W_ii                 [168, 300]   requires_grad=True\n",
            "rnn.W_if                 [168, 300]   requires_grad=True\n",
            "rnn.W_ig                 [168, 300]   requires_grad=True\n",
            "rnn.W_io                 [168, 300]   requires_grad=True\n",
            "rnn.W_hi                 [168, 168]   requires_grad=True\n",
            "rnn.W_hf                 [168, 168]   requires_grad=True\n",
            "rnn.W_hg                 [168, 168]   requires_grad=True\n",
            "rnn.W_ho                 [168, 168]   requires_grad=True\n",
            "rnn.b_i                  [168]        requires_grad=True\n",
            "rnn.b_f                  [168]        requires_grad=True\n",
            "rnn.b_g                  [168]        requires_grad=True\n",
            "rnn.b_o                  [168]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 6534113\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=374.8997, time=13.19s\n",
            "iter 250: dev acc=0.3697\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=335.5190, time=27.12s\n",
            "iter 500: dev acc=0.4160\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=324.3808, time=41.34s\n",
            "iter 750: dev acc=0.4160\n",
            "Iter 1000: loss=315.7060, time=55.28s\n",
            "iter 1000: dev acc=0.4205\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=307.7629, time=69.13s\n",
            "iter 1250: dev acc=0.4214\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=301.6088, time=83.05s\n",
            "iter 1500: dev acc=0.4505\n",
            "new highscore\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 1500: train acc=0.5005, dev acc=0.4505, test acc=0.4489\n",
            "Best Test Accuracy for seed 123: 0.4489\n",
            "LSTMClassifier(\n",
            "  (embed): Embedding(20727, 300, padding_idx=1)\n",
            "  (rnn): MyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [20727, 300] requires_grad=False\n",
            "rnn.W_ii                 [168, 300]   requires_grad=True\n",
            "rnn.W_if                 [168, 300]   requires_grad=True\n",
            "rnn.W_ig                 [168, 300]   requires_grad=True\n",
            "rnn.W_io                 [168, 300]   requires_grad=True\n",
            "rnn.W_hi                 [168, 168]   requires_grad=True\n",
            "rnn.W_hf                 [168, 168]   requires_grad=True\n",
            "rnn.W_hg                 [168, 168]   requires_grad=True\n",
            "rnn.W_ho                 [168, 168]   requires_grad=True\n",
            "rnn.b_i                  [168]        requires_grad=True\n",
            "rnn.b_f                  [168]        requires_grad=True\n",
            "rnn.b_g                  [168]        requires_grad=True\n",
            "rnn.b_o                  [168]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 6534113\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=375.0222, time=13.34s\n",
            "iter 250: dev acc=0.3915\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=331.2790, time=27.24s\n",
            "iter 500: dev acc=0.3878\n",
            "Shuffling training data\n",
            "Iter 750: loss=322.8311, time=41.43s\n",
            "iter 750: dev acc=0.4287\n",
            "new highscore\n",
            "Iter 1000: loss=316.8854, time=55.61s\n",
            "iter 1000: dev acc=0.4287\n",
            "Shuffling training data\n",
            "Iter 1250: loss=307.5809, time=69.74s\n",
            "iter 1250: dev acc=0.4342\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=300.8081, time=83.93s\n",
            "iter 1500: dev acc=0.4487\n",
            "new highscore\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 1500: train acc=0.5074, dev acc=0.4487, test acc=0.4606\n",
            "Best Test Accuracy for seed 999: 0.4606\n"
          ]
        }
      ],
      "source": [
        "seeds = [42, 123, 999]\n",
        "lstm_test_accuracies = []\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seed(seed)\n",
        "\n",
        "  lstm_model = LSTMClassifier(\n",
        "  len(v.w2i), 300, 168, len(t2i), v)\n",
        "\n",
        "  # copy pre-trained vectors into embeddings table\n",
        "  with torch.no_grad():\n",
        "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "    lstm_model.embed.weight.requires_grad = False\n",
        "\n",
        "  print(lstm_model)\n",
        "  print_parameters(lstm_model)\n",
        "\n",
        "  lstm_model = lstm_model.to(device)\n",
        "\n",
        "  batch_size = 25\n",
        "  optimizer = optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
        "\n",
        "  lstm_losses, lstm_accuracies, best_acc = train_model( lstm_model, optimizer, num_iterations=1710,\n",
        "  print_every=250, eval_every=250,\n",
        "  batch_size=batch_size,\n",
        "  batch_fn=get_minibatch,\n",
        "  prep_fn=prepare_minibatch,\n",
        "  eval_fn=evaluate)\n",
        "\n",
        "  print(f\"Best Test Accuracy for seed {seed}: {best_acc:.4f}\")\n",
        "  lstm_test_accuracies.append(best_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and standard deviation across seeds\n",
        "mean_accuracy = np.mean(lstm_test_accuracies)\n",
        "std_accuracy = np.std(lstm_test_accuracies)\n",
        "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation: {std_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "f1sswoAlbY7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7520f3-9e33-4510-ebbd-35f61646099f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Test Accuracy: 0.4575\n",
            "Standard Deviation: 0.0061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eep3X9go3rtj"
      },
      "source": [
        "## Fine tune word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ01PSst3yAr",
        "outputId": "a9822009-b4fa-4ec1-ceff-2870b165f695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMClassifier(\n",
            "  (embed): Embedding(20727, 300, padding_idx=1)\n",
            "  (rnn): MyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [20727, 300] requires_grad=True\n",
            "rnn.W_ii                 [168, 300]   requires_grad=True\n",
            "rnn.W_if                 [168, 300]   requires_grad=True\n",
            "rnn.W_ig                 [168, 300]   requires_grad=True\n",
            "rnn.W_io                 [168, 300]   requires_grad=True\n",
            "rnn.W_hi                 [168, 168]   requires_grad=True\n",
            "rnn.W_hf                 [168, 168]   requires_grad=True\n",
            "rnn.W_hg                 [168, 168]   requires_grad=True\n",
            "rnn.W_ho                 [168, 168]   requires_grad=True\n",
            "rnn.b_i                  [168]        requires_grad=True\n",
            "rnn.b_f                  [168]        requires_grad=True\n",
            "rnn.b_g                  [168]        requires_grad=True\n",
            "rnn.b_o                  [168]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 6534113\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=380.1879, time=15.51s\n",
            "iter 250: dev acc=0.3606\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=272.7565, time=31.69s\n",
            "iter 500: dev acc=0.3669\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=186.8014, time=47.60s\n",
            "iter 750: dev acc=0.3651\n",
            "Iter 1000: loss=87.9986, time=63.52s\n",
            "iter 1000: dev acc=0.3624\n",
            "Shuffling training data\n",
            "Iter 1250: loss=40.9564, time=79.47s\n",
            "iter 1250: dev acc=0.3642\n",
            "Shuffling training data\n",
            "Iter 1500: loss=25.5946, time=95.22s\n",
            "iter 1500: dev acc=0.3588\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-07f34c5b57a1>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 500: train acc=0.7870, dev acc=0.3669, test acc=0.3828\n",
            "Best Test Accuracy for seed 42: 0.3828\n",
            "LSTMClassifier(\n",
            "  (embed): Embedding(20727, 300, padding_idx=1)\n",
            "  (rnn): MyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [20727, 300] requires_grad=True\n",
            "rnn.W_ii                 [168, 300]   requires_grad=True\n",
            "rnn.W_if                 [168, 300]   requires_grad=True\n",
            "rnn.W_ig                 [168, 300]   requires_grad=True\n",
            "rnn.W_io                 [168, 300]   requires_grad=True\n",
            "rnn.W_hi                 [168, 168]   requires_grad=True\n",
            "rnn.W_hf                 [168, 168]   requires_grad=True\n",
            "rnn.W_hg                 [168, 168]   requires_grad=True\n",
            "rnn.W_ho                 [168, 168]   requires_grad=True\n",
            "rnn.b_i                  [168]        requires_grad=True\n",
            "rnn.b_f                  [168]        requires_grad=True\n",
            "rnn.b_g                  [168]        requires_grad=True\n",
            "rnn.b_o                  [168]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 6534113\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=380.8234, time=15.19s\n",
            "iter 250: dev acc=0.3460\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=272.3482, time=31.13s\n",
            "iter 500: dev acc=0.3651\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=183.7474, time=47.13s\n",
            "iter 750: dev acc=0.3688\n",
            "new highscore\n",
            "Iter 1000: loss=84.6614, time=63.31s\n",
            "iter 1000: dev acc=0.3651\n",
            "Shuffling training data\n",
            "Iter 1250: loss=40.7604, time=79.06s\n",
            "iter 1250: dev acc=0.3615\n",
            "Shuffling training data\n",
            "Iter 1500: loss=25.9995, time=94.63s\n",
            "iter 1500: dev acc=0.3551\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 750: train acc=0.9219, dev acc=0.3688, test acc=0.3724\n",
            "Best Test Accuracy for seed 123: 0.3724\n",
            "LSTMClassifier(\n",
            "  (embed): Embedding(20727, 300, padding_idx=1)\n",
            "  (rnn): MyLSTMCell(300, 168)\n",
            "  (output_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=168, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "embed.weight             [20727, 300] requires_grad=True\n",
            "rnn.W_ii                 [168, 300]   requires_grad=True\n",
            "rnn.W_if                 [168, 300]   requires_grad=True\n",
            "rnn.W_ig                 [168, 300]   requires_grad=True\n",
            "rnn.W_io                 [168, 300]   requires_grad=True\n",
            "rnn.W_hi                 [168, 168]   requires_grad=True\n",
            "rnn.W_hf                 [168, 168]   requires_grad=True\n",
            "rnn.W_hg                 [168, 168]   requires_grad=True\n",
            "rnn.W_ho                 [168, 168]   requires_grad=True\n",
            "rnn.b_i                  [168]        requires_grad=True\n",
            "rnn.b_f                  [168]        requires_grad=True\n",
            "rnn.b_g                  [168]        requires_grad=True\n",
            "rnn.b_o                  [168]        requires_grad=True\n",
            "output_layer.1.weight    [5, 168]     requires_grad=True\n",
            "output_layer.1.bias      [5]          requires_grad=True\n",
            "\n",
            "Total number of parameters: 6534113\n",
            "\n",
            "Shuffling training data\n",
            "Iter 250: loss=381.3801, time=15.21s\n",
            "iter 250: dev acc=0.3624\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=265.2496, time=31.32s\n",
            "iter 500: dev acc=0.3615\n",
            "Shuffling training data\n",
            "Iter 750: loss=181.0942, time=47.15s\n",
            "iter 750: dev acc=0.3769\n",
            "new highscore\n",
            "Iter 1000: loss=80.9423, time=63.10s\n",
            "iter 1000: dev acc=0.3688\n",
            "Shuffling training data\n",
            "Iter 1250: loss=41.0655, time=79.07s\n",
            "iter 1250: dev acc=0.3660\n",
            "Shuffling training data\n",
            "Iter 1500: loss=26.4587, time=94.97s\n",
            "iter 1500: dev acc=0.3642\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 750: train acc=0.9278, dev acc=0.3769, test acc=0.3593\n",
            "Best Test Accuracy for seed 999: 0.3593\n"
          ]
        }
      ],
      "source": [
        "seeds = [42, 123, 999]\n",
        "lstm_test_accuracies_em = []\n",
        "\n",
        "for seed in seeds:\n",
        "  set_seed(seed)\n",
        "  lstm_model = LSTMClassifier(\n",
        "  len(v.w2i), 300, 168, len(t2i), v)\n",
        "\n",
        "  # copy pre-trained vectors into embeddings table\n",
        "  with torch.no_grad():\n",
        "    lstm_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "\n",
        "  lstm_model.embed.weight.requires_grad = True\n",
        "\n",
        "# Now fine-tune your embeddings together with the model\n",
        "\n",
        "  print(lstm_model)\n",
        "  print_parameters(lstm_model)\n",
        "\n",
        "  lstm_model = lstm_model.to(device)\n",
        "\n",
        "  batch_size = 25\n",
        "  optimizer = optim.Adam([\n",
        "    {\"params\": lstm_model.embed.parameters(), \"lr\":0.1},       # Fine-tune embeddings with a lower LR\n",
        "    {\"params\": lstm_model.rnn.parameters(), \"lr\": 3e-4},\n",
        "    {\"params\": lstm_model.output_layer.parameters(), \"lr\": 3e-4} # Higher LR for the classifier\n",
        "])\n",
        "  lstm_losses, lstm_accuracies, best_acc = train_model(\n",
        "    lstm_model, optimizer, num_iterations=1710,\n",
        "    print_every=250, eval_every=250,\n",
        "    batch_size=batch_size,\n",
        "    batch_fn=get_minibatch,\n",
        "    prep_fn=prepare_minibatch,\n",
        "    eval_fn=evaluate)\n",
        "\n",
        "  print(f\"Best Test Accuracy for seed {seed}: {best_acc:.4f}\")\n",
        "  lstm_test_accuracies_em.append(best_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUq5ULfqbcfF",
        "outputId": "b1ad3e18-390e-4960-a4fa-35d0483df44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Test Accuracy: 0.3715\n",
            "Standard Deviation: 0.0096\n"
          ]
        }
      ],
      "source": [
        "mean_accuracy = np.mean(lstm_test_accuracies_em)\n",
        "std_accuracy = np.std(lstm_test_accuracies_em)\n",
        "\n",
        "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation: {std_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7WjcxXntMi5"
      },
      "source": [
        "# Tree-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rDzvSos3JFp"
      },
      "source": [
        "## Computation\n",
        "\n",
        "Do you remember the `transitions_from_treestring` function all the way in the beginning of this lab? Every example contains a **transition sequence** produced by this function. Let's look at it again:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyj_UD6GtO5M"
      },
      "source": [
        "In the final part of this lab we will exploit the tree structure of the SST data.\n",
        "Until now we only used the surface tokens, but remember that our data examples include binary trees with a sentiment score at every node.\n",
        "\n",
        "In particular, we will implement **N-ary Tree-LSTMs** which are described in:\n",
        "\n",
        "> Kai Sheng Tai, Richard Socher, and Christopher D. Manning. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://aclweb.org/anthology/P/P15/P15-1150.pdf) ACL 2015.\n",
        "\n",
        "Since our trees are binary (i.e., N=2), we can refer to these as *Binary Tree-LSTMs*. If you study equations (9) to (14) in the paper, you will find that they are not all too different from the original LSTM that you already have.\n",
        "\n",
        "You should read this paper carefully and make sure that you understand the approach. You will also find our LSTM baseline there.\n",
        "Note however that Tree-LSTMs were proposed around the same time by two other groups:\n",
        "\n",
        "> Phong Le and Willem Zuidema. [Compositional distributional semantics with long short term memory](http://anthology.aclweb.org/S/S15/S15-1002.pdf). *SEM 2015.\n",
        "\n",
        "> Xiaodan Zhu, Parinaz Sobihani,  and Hongyu Guo. [Long short-term memory over recursive structures](http://proceedings.mlr.press/v37/zhub15.pdf). ICML 2015.\n",
        "\n",
        "It is good scientific practice to cite all three papers in your report.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SsFmBFwYXrN"
      },
      "source": [
        "## Computation\n",
        "\n",
        "Do you remember the `transitions_from_treestring` function all the way in the beginning of this lab? Every example contains a **transition sequence** produced by this function. Let's look at it again:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "outputId": "500680ff-d608-4fc0-c384-e1636b6a693f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UynRwejlYXrN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                               3                                                                                                                                         \n",
            "      _________________________________________________________________________________________|____                                                                                                                                      \n",
            "     |                                                                                              4                                                                                                                                    \n",
            "     |                      ________________________________________________________________________|__________________________________________________________________________________________________________________________________   \n",
            "     |                     3                                                                                                                                                                                                           | \n",
            "     |         ____________|___________                                                                                                                                                                                                |  \n",
            "     |        |                        4                                                                                                                                                                                               | \n",
            "     |        |      __________________|_________________________________                                                                                                                                                              |  \n",
            "     |        |     |                                                    2                                                                                                                                                             | \n",
            "     |        |     |                                    ________________|_________________                                                                                                                                            |  \n",
            "     |        |     |                                   |                                  3                                                                                                                                           | \n",
            "     |        |     |                                   |                              ____|___                                                                                                                                        |  \n",
            "     |        |     |                                   |                             |        3                                                                                                                                       | \n",
            "     |        |     |                                   |                             |     ___|_________                                                                                                                              |  \n",
            "     |        |     |                                   |                             |    |             3                                                                                                                             | \n",
            "     |        |     |                                   |                             |    |    _________|_________________                                                                                                            |  \n",
            "     |        |     |                                   2                             |    |   |                           3                                                                                                           | \n",
            "     |        |     |                          _________|_________________________    |    |   |     ______________________|________________________________                                                                           |  \n",
            "     |        |     |                         2                                   |   |    |   |    |                                                       3                                                                          | \n",
            "     |        |     |           ______________|_______________________________    |   |    |   |    |     __________________________________________________|______________                                                            |  \n",
            "     |        |     |          2                                              |   |   |    |   |    |    |                                                                 4                                                           | \n",
            "     |        |     |       ___|___                                           |   |   |    |   |    |    |                  _______________________________________________|________________                                           |  \n",
            "     |        |     |      |       2                                          |   |   |    |   |    |    |                 |                                                                2                                          | \n",
            "     |        |     |      |    ___|___                                       |   |   |    |   |    |    |                 |                           _____________________________________|_______                                   |  \n",
            "     |        |     |      |   |       2                                      |   |   |    |   |    |    |                 |                          |                                             2                                  | \n",
            "     |        |     |      |   |    ___|________________                      |   |   |    |   |    |    |                 |                          |                                      _______|________________________          |  \n",
            "     |        |     |      |   |   |                    2                     |   |   |    |   |    |    |                 |                          |                                     2                                |         | \n",
            "     |        |     |      |   |   |    ________________|___                  |   |   |    |   |    |    |                 |                          |                               ______|_____________________           |         |  \n",
            "     |        |     |      |   |   |   |                    2                 |   |   |    |   |    |    |                 3                          |                              2                            |          |         | \n",
            "     |        |     |      |   |   |   |             _______|___              |   |   |    |   |    |    |    _____________|_____                     |                     _________|______________              |          |         |  \n",
            "     |        |     |      |   |   |   |            |           2             |   |   |    |   |    |    |   |                   3                    |                    2                        2             |          |         | \n",
            "     |        |     |      |   |   |   |            |        ___|___          |   |   |    |   |    |    |   |         __________|________            |            ________|_________        _______|___          |          |         |  \n",
            "     2        |     |      |   |   |   |            2       |       2         |   |   |    |   |    |    |   |        3                   2           |           1                  |      |           2         |          2         | \n",
            "  ___|___     |     |      |   |   |   |       _____|___    |    ___|____     |   |   |    |   |    |    |   |     ___|____           ____|_____      |      _____|________          |      |        ___|____     |     _____|____     |  \n",
            " 2       2    2     2      2   2   2   2      2         2   3   2        2    2   2   2    2   2    2    2   2    2        3         2          3     2     2              2         2      2       2        2    2    2          2    2 \n",
            " |       |    |     |      |   |   |   |      |         |   |   |        |    |   |   |    |   |    |    |   |    |        |         |          |     |     |              |         |      |       |        |    |    |          |    |  \n",
            "The     Rock  is destined  to  be the 21st Century      's new  ``     Conan  '' and that  he  's going  to make  a      splash     even     greater than Arnold     Schwarzenegger  ,  Jean-Claud Van     Damme  or Steven     Segal  . \n",
            "\n",
            "Transitions:\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-85-a3509d08ee20>:2: DeprecationWarning: \n",
            "    Class TreePrettyPrinter has been deprecated.  Import\n",
            "    `TreePrettyPrinter` using `from nltk.tree import\n",
            "    TreePrettyPrinter` instead.\n",
            "  print(TreePrettyPrinter(ex.tree))\n"
          ]
        }
      ],
      "source": [
        "ex = next(examplereader(\"trees/train.txt\"))\n",
        "print(TreePrettyPrinter(ex.tree))\n",
        "print(\"Transitions:\")\n",
        "print(ex.transitions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceBFe9fU4BI_"
      },
      "source": [
        "Note that the tree is **binary**. Every node has two children, except for pre-terminal nodes.\n",
        "\n",
        "A tree like this can be described by a sequence of **SHIFT (0)** and **REDUCE (1)** actions.\n",
        "\n",
        "To construct a tree, we can use the transitions as follows:\n",
        "- **reverse** the sentence (a list of tokens) and call this the **buffer**\n",
        "   - the first word is now on top (last in the list), and we would get it when calling pop() on the buffer\n",
        "- create an empty list and call it the **stack**\n",
        "- iterate through the transition sequence:\n",
        "  - if it says SHIFT(0), we pop a word from the buffer, and push it to the stack\n",
        "  - if it says REDUCE(1), we pop the **top two items** from the stack, and combine them (e.g. with a Tree-LSTM!), creating a new node that we push back on the stack\n",
        "  \n",
        "Convince yourself that going through the transition sequence above will result in the tree that you see.\n",
        "For example, we would start by putting the following words on the stack (by shifting 5 times, starting with `It`):\n",
        "\n",
        "```\n",
        "Top of the stack:\n",
        "-----------------\n",
        "film\n",
        "lovely\n",
        "a\n",
        "'s  \n",
        "It\n",
        "```\n",
        "Now we find a REDUCE in the transition sequence, so we get the top two words (film and lovely), and combine them, so our new stack becomes:\n",
        "```\n",
        "Top of the stack:\n",
        "-----------------\n",
        "lovely film\n",
        "a\n",
        "'s  \n",
        "It\n",
        "```\n",
        "\n",
        "We will use this approach when encoding sentences with our Tree-LSTM.\n",
        "Now, our sentence is a reversed list of word embeddings.\n",
        "When we shift, we move a word embedding to the stack.\n",
        "When we reduce, we apply the Tree-LSTM to the top two vectors, and the result is a single vector that we put back on the stack.\n",
        "After going through the whole transition sequence, we will have the root node on our stack! We can use that to classify the sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDp13MWUYXrN"
      },
      "source": [
        "## Obtaining the transition sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO7VKWVpAbWj"
      },
      "source": [
        "\n",
        "So what goes on in the `transitions_from_treestring` function?\n",
        "\n",
        "The idea ([explained in this blog post](https://devblogs.nvidia.com/recursive-neural-networks-pytorch/)) is that, if we had a tree, we could traverse through the tree, and every time that we find a node containing only a word, we output a SHIFT.\n",
        "Every time **after** we have finished visiting the children of a node, we output a REDUCE.\n",
        "(What is this tree traversal called?)\n",
        "\n",
        "However, our `transitions_from_treestring` function operates directly on the string representation. It works as follows.\n",
        "\n",
        "We start with the representation:\n",
        "\n",
        "```\n",
        "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\n",
        "```\n",
        "\n",
        "First we remove pre-terminal nodes (and add spaces before closing brackets):\n",
        "\n",
        "```\n",
        "(3 It (4 (4 's (4 (3 a (4 lovely film ) ) (3 with (4 (3 lovely performances ) (2 by (2 (2 Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Then we remove node labels:\n",
        "\n",
        "```\n",
        "( It ( ( 's ( ( a ( lovely film ) ) ( with ( ( lovely performances) ( by ( ( Buy and )  Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Then we remove opening brackets:\n",
        "\n",
        "```\n",
        "It 's a lovely film ) ) with lovely performances ) by Buy and ) Accorsi ) ) ) ) ) ) . ) )\n",
        "```\n",
        "\n",
        "Now we replace words by S (for SHIFT), and closing brackets by R (for REDUCE):\n",
        "\n",
        "```\n",
        "S S S S S R R S S S R S S S R S R R R R R R S R R\n",
        "0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1\n",
        "```\n",
        "\n",
        "Et voilà. We just obtained the transition sequence!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "outputId": "dac5e82e-1e63-4a10-9c75-7fc206c9b396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EigJrDCMYXrO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S S R S S S S S S S S R S S S R R R R R R R S R S R S S S S S S S S R S S R R R S S S R S R S S S R R R S R S S R R R R R R R R R R R R S R R\n",
            "0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n"
          ]
        }
      ],
      "source": [
        "# for comparison\n",
        "seq = ex.transitions\n",
        "s = \" \".join([\"S\" if t == 0 else \"R\" for t in seq])\n",
        "print(s)\n",
        "print(\" \".join(map(str, seq)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSjmin-mYXrO"
      },
      "source": [
        "## Coding the Tree-LSTM\n",
        "\n",
        "The code below contains a Binary Tree-LSTM cell.\n",
        "It is used in the TreeLSTM class below it, which in turn is used in the TreeLSTMClassifier.\n",
        "The job of the TreeLSTM class is to encode a complete sentence and return the root node.\n",
        "The job of the TreeLSTMCell is to return a new state when provided with two children (a reduce action). By repeatedly calling the TreeLSTMCell, the TreeLSTM will encode a sentence. This can be done for multiple sentences at the same time.\n",
        "\n",
        "\n",
        "#### Exercise\n",
        "Check the `forward` function and complete the Tree-LSTM formulas.\n",
        "You can see that we defined a large linear layer for you, that projects the *concatenation* of the left and right child into the input gate, left forget gate, right forget gate, candidate, and output gate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "DxNbkn24YXrO"
      },
      "outputs": [],
      "source": [
        "class TreeLSTMCell(nn.Module):\n",
        "  \"\"\"A Binary Tree LSTM cell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(TreeLSTMCell, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "\n",
        "    self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
        "    self.dropout_layer = nn.Dropout(p=0.25)\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"This is PyTorch's default initialization method\"\"\"\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, hx_l, hx_r, mask=None):\n",
        "    \"\"\"\n",
        "    hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
        "    hx_r is ((batch, hidden_size), (batch, hidden_size))\n",
        "    \"\"\"\n",
        "    prev_h_l, prev_c_l = hx_l  # left child\n",
        "    prev_h_r, prev_c_r = hx_r  # right child\n",
        "\n",
        "    B = prev_h_l.size(0)\n",
        "\n",
        "    # we concatenate the left and right children\n",
        "    # you can also project from them separately and then sum\n",
        "    children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
        "\n",
        "    # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
        "    # this is done for speed, and you could also do it separately\n",
        "    proj = self.reduce_layer(children)  # shape: B x 5D\n",
        "\n",
        "    # each shape: B x D\n",
        "    i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
        "\n",
        "    # main Tree LSTM computation\n",
        "    # The shape of each of these is [batch_size, hidden_size]\n",
        "\n",
        "    # Apply activations\n",
        "    i = torch.sigmoid(i)          # Input gate\n",
        "    f_l = torch.sigmoid(f_l)      # Left forget gate\n",
        "    f_r = torch.sigmoid(f_r)      # Right forget gate\n",
        "    g = torch.tanh(g)             # Candidate memory\n",
        "    o = torch.sigmoid(o)          # Output gate\n",
        "\n",
        "    # Compute the new cell state\n",
        "    c = i * g + f_l * prev_c_l + f_r * prev_c_r\n",
        "\n",
        "    # Compute the new hidden state\n",
        "    h = o * torch.tanh(c)\n",
        "\n",
        "    return h, c\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"{}({:d}, {:d})\".format(\n",
        "        self.__class__.__name__, self.input_size, self.hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p_DuncmYXrO"
      },
      "source": [
        "## Explanation of the TreeLSTM class\n",
        "\n",
        "\n",
        "The code below contains the TreeLSTM class, which implements everything we need in order to encode a sentence from word embeddings. The calculations are the same as in the paper, implemented such that the class `TreeLSTMCell` above is as general as possible and only takes two children to reduce them into a parent.\n",
        "\n",
        "\n",
        "**Initialize $\\mathbf{h}$ and $\\mathbf{c}$ outside of the cell for the leaves**\n",
        "\n",
        "At the leaves of each tree the children nodes are **empty**, whereas in higher levels the nodes are binary tree nodes that *do* have a left and right child (but no input $x$). By initializing the leaf nodes outside of the cell class (`TreeLSTMCell`), we avoid if-else statements in the forward pass.\n",
        "\n",
        "The `TreeLSTM` class (among other things) pre-calculates an initial $h$ and $c$ for every word in the sentence. Since the initial left and right child are 0, the only calculations we need to do are based on $x$, and we can drop the forget gate calculation (`prev_c_l` and `prev_c_r` are zero). The calculations we do in order to initalize $h$ and $c$ are then:\n",
        "\n",
        "$$\n",
        "c_1 =  W^{(u)}x_1 \\\\\n",
        "o_1 = \\sigma (W^{(i)}x_1) \\\\\n",
        "h_1 = o_1 \\odot \\text{tanh}(c_1)$$\n",
        "*NB: note that these equations are chosen as initializations of $c$ and $h$, other initializations are possible and might work equally well.*\n",
        "\n",
        "**Sentence Representations**\n",
        "\n",
        "All our leaf nodes are now initialized, so we can start processing the sentence in its tree form. Each sentence is represented by a buffer (initially a list with a concatenation of $[h_1, c_1]$ for every word in the reversed sentence), a stack (initially an empty list) and a transition sequence. To encode our sentence, we construct the tree from its transition sequence as explained earlier.\n",
        "\n",
        "*A short example that constructs a tree:*\n",
        "\n",
        "We loop over the time dimension of the batched transition sequences (i.e. row by row), which contain values of 0's, 1's and 2's (representing SHIFT, REDUCE and padding respectively). If we have a batch of size 2 where the first example has a transition sequence given by [0, 0, 1, 0, 0, 0, 1] and the second by [0, 0, 1, 0, 0, 1], our transition batch will be given by the following two-dimensional numpy array:\n",
        "\n",
        "$$\n",
        "\\text{transitions} =\n",
        "\\begin{pmatrix}\n",
        "0 & 0\\\\\n",
        "0 & 0\\\\\n",
        "1 & 1\\\\\n",
        "0 & 0\\\\\n",
        "0 & 0\\\\\n",
        "0 & 1\\\\\n",
        "1 & 2\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "The inner loop (`for transition, buffer, stack in zip(t_batch, buffers, stacks)`) goes over each example in the batch and updates its buffer and stack. The nested loop for this example will then do roughy the following:\n",
        "\n",
        "```\n",
        "Time = 0:  t_batch = [0, 0], the inner loop performs 2 SHIFTs.\n",
        "\n",
        "Time = 1:  t_batch = [0, 0], \"..\"\n",
        "\n",
        "Time = 2:  t_batch = [1, 1], causing the inner loop to fill the list child_l and child_r for both examples in the batch. Now the statement if child_l will return True, triggering a REDUCE action to be performed by our Tree LSTM cell with a batch size of 2.\n",
        "\n",
        "Time = 3:  t_batch = [0, 0], \"..\".\n",
        "\n",
        "Time = 4:  t_batch = [0, 0], \"..\"\n",
        "\n",
        "Time = 5:  t_batch = [0, 1], one SHIFT will be done and another REDUCE action will be performed by our Tree LSTM, this time of batch size 1.  \n",
        "\n",
        "Time = 6:  t_batch = [1, 2], triggering another REDUCE action with batch size 1.\n",
        "```\n",
        "*NB: note that this was an artificial example for the purpose of demonstrating parts of the code, the transition sequences do not necessarily represent actual trees.*\n",
        "\n",
        "**Batching and Unbatching**\n",
        "\n",
        "Within the body of the outer loop over time, we use the functions for batching and unbatching.\n",
        "\n",
        "*Batching*\n",
        "\n",
        "Before passing two lists of children to the reduce layer (an instance of `TreeLSTMCell`), we batch the children as they are at this point a list of tensors of variable length based on how many REDUCE actions there are to perform at a certain time step across the batch (let's call the length `L`). To do an efficient forward pass we want to transform the list to a pair of tensors of shape `([L, D], [L, D])`, which the function `batch` achieves.\n",
        "\n",
        "*Unbatching*\n",
        "\n",
        "In the same line where we batched the children, we unbatch the output of the forward pass to become a list of states of length `L` again. We do this because we need to loop over each example's transition at the current time step and push the children that are reduced into a parent to the stack.\n",
        "\n",
        "*The batch and unbatch functions let us switch between the \"PyTorch world\" (Tensors) and the Python world (easy to manipulate lists).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "0Yklh_lNYXrO"
      },
      "outputs": [],
      "source": [
        "# Helper functions for batching and unbatching states\n",
        "# For speed we want to combine computations by batching, but\n",
        "# for processing logic we want to turn the output into lists again\n",
        "# to easily manipulate.\n",
        "\n",
        "def batch(states):\n",
        "  \"\"\"\n",
        "  Turns a list of states into a single tensor for fast processing.\n",
        "  This function also chunks (splits) each state into a (h, c) pair\"\"\"\n",
        "  return torch.cat(states, 0).chunk(2, 1)\n",
        "\n",
        "def unbatch(state):\n",
        "  \"\"\"\n",
        "  Turns a tensor back into a list of states.\n",
        "  First, (h, c) are merged into a single state.\n",
        "  Then the result is split into a list of sentences.\n",
        "  \"\"\"\n",
        "  return torch.split(torch.cat(state, 1), 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzxrc7XeYXrP"
      },
      "source": [
        "Take some time to understand the class below, having read the explanation above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "LQY9PHLdYXrP"
      },
      "outputs": [],
      "source": [
        "class TreeLSTM(nn.Module):\n",
        "  \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, bias=True):\n",
        "    \"\"\"Creates the weights for this LSTM\"\"\"\n",
        "    super(TreeLSTM, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "    self.reduce = TreeLSTMCell(input_size, hidden_size)\n",
        "\n",
        "    # project word to initial c\n",
        "    self.proj_x = nn.Linear(input_size, hidden_size)\n",
        "    self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    self.buffers_dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "  def forward(self, x, transitions):\n",
        "    \"\"\"\n",
        "    WARNING: assuming x is reversed!\n",
        "    :param x: word embeddings [B, T, E]\n",
        "    :param transitions: [2T-1, B]\n",
        "    :return: root states\n",
        "    \"\"\"\n",
        "\n",
        "    B = x.size(0)  # batch size\n",
        "    T = x.size(1)  # time\n",
        "\n",
        "    # compute an initial c and h for each word\n",
        "    # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
        "    # We do not handle input x in the TreeLSTMCell itself.\n",
        "    buffers_c = self.proj_x(x)\n",
        "    buffers_h = buffers_c.tanh()\n",
        "    buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
        "    buffers_h = buffers_h_gate * buffers_h\n",
        "\n",
        "    # concatenate h and c for each word\n",
        "    buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
        "\n",
        "    D = buffers.size(-1) // 2\n",
        "\n",
        "    # we turn buffers into a list of stacks (1 stack for each sentence)\n",
        "    # first we split buffers so that it is a list of sentences (length B)\n",
        "    # then we split each sentence to be a list of word vectors\n",
        "    buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
        "    buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
        "\n",
        "    # create B empty stacks\n",
        "    stacks = [[] for _ in buffers]\n",
        "\n",
        "    # t_batch holds 1 transition for each sentence\n",
        "    for t_batch in transitions:\n",
        "\n",
        "      child_l = []  # contains the left child for each sentence with reduce action\n",
        "      child_r = []  # contains the corresponding right child\n",
        "\n",
        "      # iterate over sentences in the batch\n",
        "      # each has a transition t, a buffer and a stack\n",
        "      for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
        "        if transition == SHIFT:\n",
        "          stack.append(buffer.pop())\n",
        "        elif transition == REDUCE:\n",
        "          assert len(stack) >= 2, \\\n",
        "            \"Stack too small! Should not happen with valid transition sequences\"\n",
        "          child_r.append(stack.pop())  # right child is on top\n",
        "          child_l.append(stack.pop())\n",
        "\n",
        "      # if there are sentences with reduce transition, perform them batched\n",
        "      if child_l:\n",
        "        reduced = iter(unbatch(self.reduce(batch(child_l), batch(child_r))))\n",
        "        for transition, stack in zip(t_batch, stacks):\n",
        "          if transition == REDUCE:\n",
        "            stack.append(next(reduced))\n",
        "\n",
        "    final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
        "    final = torch.cat(final, dim=0)  # tensor [B, D]\n",
        "\n",
        "    return final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g_HKJdnYXrP"
      },
      "source": [
        "Just like the LSTM before, we will need an extra class that does the classifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "E55hnbpOYXrP"
      },
      "outputs": [],
      "source": [
        "class TreeLSTMClassifier(nn.Module):\n",
        "  \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
        "    super(TreeLSTMClassifier, self).__init__()\n",
        "    self.vocab = vocab\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
        "    self.treelstm = TreeLSTM(embedding_dim, hidden_dim)\n",
        "    self.output_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # x is a pair here of words and transitions; we unpack it here.\n",
        "    # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
        "    x, transitions = x\n",
        "    emb = self.embed(x)\n",
        "\n",
        "    # we use the root/top state of the Tree LSTM to classify the sentence\n",
        "    root_states = self.treelstm(emb, transitions)\n",
        "\n",
        "    # we use the last hidden state to classify the sentence\n",
        "    logits = self.output_layer(root_states)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcTTV56RYXrP"
      },
      "source": [
        "## Special `prepare` function for Tree-LSTM\n",
        "\n",
        "We need yet another `prepare` function. For our implementation, sentences need to be *reversed*. We will do that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "olwjEtT1YXrP"
      },
      "outputs": [],
      "source": [
        "def prepare_treelstm_minibatch(mb, vocab):\n",
        "  \"\"\"\n",
        "  Returns sentences reversed (last word first)\n",
        "  Returns transitions together with the sentences.\n",
        "  \"\"\"\n",
        "  batch_size = len(mb)\n",
        "  maxlen = max([len(ex.tokens) for ex in mb])\n",
        "\n",
        "  # vocab returns 0 if the word is not there\n",
        "  # NOTE: reversed sequence!\n",
        "  x = [pad([vocab.w2i.get(t, 0) for t in ex.tokens], maxlen)[::-1] for ex in mb]\n",
        "\n",
        "  x = torch.LongTensor(x)\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = [ex.label for ex in mb]\n",
        "  y = torch.LongTensor(y)\n",
        "  y = y.to(device)\n",
        "\n",
        "  maxlen_t = max([len(ex.transitions) for ex in mb])\n",
        "  transitions = [pad(ex.transitions, maxlen_t, pad_value=2) for ex in mb]\n",
        "  transitions = np.array(transitions)\n",
        "  transitions = transitions.T  # time-major\n",
        "\n",
        "  return (x, transitions), y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUsrlL9ayVe"
      },
      "source": [
        "## Training Tree LSTM embeddings Fixed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHvXJljdoF3Y",
        "outputId": "151914d5-9f66-4c38-a6ed-8d27fd6662c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Tree LSTM with seed 42...\n",
            "Shuffling training data\n",
            "Iter 250: loss=381.5925, time=20.12s\n",
            "iter 250: dev acc=0.3787\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=334.8407, time=41.30s\n",
            "iter 500: dev acc=0.4087\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=322.1638, time=62.33s\n",
            "iter 750: dev acc=0.4242\n",
            "new highscore\n",
            "Iter 1000: loss=314.5846, time=83.17s\n",
            "iter 1000: dev acc=0.4332\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=307.4767, time=104.08s\n",
            "iter 1250: dev acc=0.4332\n",
            "Shuffling training data\n",
            "Iter 1500: loss=303.2588, time=124.97s\n",
            "iter 1500: dev acc=0.4496\n",
            "new highscore\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-5fe97a37418c>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 1500: train acc=0.4872, dev acc=0.4496, test acc=0.4715\n",
            "Test Accuracy for seed 42: 0.4715\n",
            "Training Tree LSTM with seed 123...\n",
            "Shuffling training data\n",
            "Iter 250: loss=381.8712, time=19.38s\n",
            "iter 250: dev acc=0.4160\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=332.7980, time=40.29s\n",
            "iter 500: dev acc=0.4278\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=321.5158, time=61.09s\n",
            "iter 750: dev acc=0.4314\n",
            "new highscore\n",
            "Iter 1000: loss=311.9049, time=82.01s\n",
            "iter 1000: dev acc=0.4323\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=307.8301, time=102.75s\n",
            "iter 1250: dev acc=0.4414\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=304.1260, time=123.70s\n",
            "iter 1500: dev acc=0.4450\n",
            "new highscore\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 1500: train acc=0.4868, dev acc=0.4450, test acc=0.4647\n",
            "Test Accuracy for seed 123: 0.4647\n",
            "Training Tree LSTM with seed 999...\n",
            "Shuffling training data\n",
            "Iter 250: loss=382.9954, time=19.46s\n",
            "iter 250: dev acc=0.3842\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=335.0793, time=40.39s\n",
            "iter 500: dev acc=0.4051\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=318.6879, time=61.35s\n",
            "iter 750: dev acc=0.4178\n",
            "new highscore\n",
            "Iter 1000: loss=317.2190, time=82.22s\n",
            "iter 1000: dev acc=0.4387\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=304.7859, time=103.00s\n",
            "iter 1250: dev acc=0.4396\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=306.1135, time=124.48s\n",
            "iter 1500: dev acc=0.4441\n",
            "new highscore\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 1500: train acc=0.4897, dev acc=0.4441, test acc=0.4593\n",
            "Test Accuracy for seed 999: 0.4593\n"
          ]
        }
      ],
      "source": [
        "# Define the random seeds\n",
        "seeds = [42, 123, 999]\n",
        "tree_test_accuracies = []\n",
        "\n",
        "# Set random seed\n",
        "for seed in seeds:\n",
        "    set_seed(seed)\n",
        "    tree_model = TreeLSTMClassifier(\n",
        "        vocab_size=len(v.w2i),\n",
        "        embedding_dim=300,\n",
        "        hidden_dim=150,\n",
        "        output_dim=len(t2i),\n",
        "        vocab=v\n",
        "    ).to(device)\n",
        "\n",
        "    # Load pre-trained embeddings into the embedding layer\n",
        "    with torch.no_grad():\n",
        "        tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "        tree_model.embed.weight.requires_grad = False\n",
        "\n",
        "    # Train the model and get results\n",
        "    print(f\"Training Tree LSTM with seed {seed}...\")\n",
        "    tree_losses, tree_accuracies, best_acc = train_model(\n",
        "        model=tree_model,\n",
        "        optimizer=optim.Adam(tree_model.parameters(), lr=2e-4),\n",
        "        num_iterations=1710,\n",
        "        print_every=250,\n",
        "        eval_every=250,\n",
        "        prep_fn=prepare_treelstm_minibatch,\n",
        "        eval_fn=evaluate,\n",
        "        batch_fn=get_minibatch,\n",
        "        batch_size=25,\n",
        "        eval_batch_size=25\n",
        "    )\n",
        "\n",
        "    # Store the test accuracy for the best model\n",
        "    print(f\"Test Accuracy for seed {seed}: {best_acc:.4f}\")\n",
        "    tree_test_accuracies.append(best_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieaWJ8i3ivl_",
        "outputId": "a86428b5-ded3-4e54-f65d-05190a2ba9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Test Accuracy: 0.4652\n",
            "Standard Deviation: 0.0050\n"
          ]
        }
      ],
      "source": [
        "mean_accuracy = np.mean(tree_test_accuracies)\n",
        "std_accuracy = np.std(tree_test_accuracies)\n",
        "\n",
        "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation: {std_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree-LSTM embeddings tuned"
      ],
      "metadata": {
        "id": "zak1oOZ1bs1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [42, 123, 999]\n",
        "tree_test_accuracies_em = []\n",
        "\n",
        "for seed in seeds:\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Initialize Tree LSTM model\n",
        "    tree_model = TreeLSTMClassifier(\n",
        "        vocab_size=len(v.w2i),\n",
        "        embedding_dim=300,\n",
        "        hidden_dim=150,\n",
        "        output_dim=len(t2i),\n",
        "        vocab=v\n",
        "    ).to(device)\n",
        "\n",
        "    # Load pre-trained embeddings into the embedding layer\n",
        "    with torch.no_grad():\n",
        "        tree_model.embed.weight.data.copy_(torch.from_numpy(vectors))\n",
        "        tree_model.embed.weight.requires_grad = True\n",
        "\n",
        "    # Train the model and get results\n",
        "    print(f\"Training Tree LSTM with seed {seed}...\")\n",
        "    tree_losses, tree_accuracies, best_acc = train_model(\n",
        "        model=tree_model,\n",
        "        optimizer=optim.Adam(tree_model.parameters(), lr=2e-4),\n",
        "        num_iterations=1710,\n",
        "        print_every=250,\n",
        "        eval_every=250,\n",
        "        prep_fn=prepare_treelstm_minibatch,\n",
        "        eval_fn=evaluate,\n",
        "        batch_fn=get_minibatch,\n",
        "        batch_size=25,\n",
        "        eval_batch_size=25\n",
        "    )\n",
        "\n",
        "    # Store the test accuracy for the best model\n",
        "    print(f\"Test Accuracy for seed {seed}: {best_acc:.4f}\")\n",
        "    tree_test_accuracies_em.append(best_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WteuI4bmRPUd",
        "outputId": "764e8142-325c-449f-d8cf-9264e7e1a56c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Tree LSTM with seed 42...\n",
            "Shuffling training data\n",
            "Iter 250: loss=381.6732, time=19.65s\n",
            "iter 250: dev acc=0.3806\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=327.7947, time=40.73s\n",
            "iter 500: dev acc=0.4160\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=310.8171, time=61.57s\n",
            "iter 750: dev acc=0.4369\n",
            "new highscore\n",
            "Iter 1000: loss=290.0288, time=82.51s\n",
            "iter 1000: dev acc=0.4214\n",
            "Shuffling training data\n",
            "Iter 1250: loss=269.4803, time=103.64s\n",
            "iter 1250: dev acc=0.4460\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=247.4710, time=125.06s\n",
            "iter 1500: dev acc=0.4342\n",
            "Done training\n",
            "Loading best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-5fe97a37418c>:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best model iter 1250: train acc=0.5884, dev acc=0.4460, test acc=0.4769\n",
            "Test Accuracy for seed 42: 0.4769\n",
            "Training Tree LSTM with seed 123...\n",
            "Shuffling training data\n",
            "Iter 250: loss=379.6563, time=19.67s\n",
            "iter 250: dev acc=0.3924\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=324.1874, time=41.01s\n",
            "iter 500: dev acc=0.4187\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=304.8659, time=62.18s\n",
            "iter 750: dev acc=0.4396\n",
            "new highscore\n",
            "Iter 1000: loss=286.8690, time=83.37s\n",
            "iter 1000: dev acc=0.4478\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=265.2946, time=104.60s\n",
            "iter 1250: dev acc=0.4559\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=246.9187, time=125.93s\n",
            "iter 1500: dev acc=0.4496\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 1250: train acc=0.6093, dev acc=0.4559, test acc=0.4606\n",
            "Test Accuracy for seed 123: 0.4606\n",
            "Training Tree LSTM with seed 999...\n",
            "Shuffling training data\n",
            "Iter 250: loss=383.4045, time=19.82s\n",
            "iter 250: dev acc=0.3787\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 500: loss=328.5691, time=41.28s\n",
            "iter 500: dev acc=0.4242\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 750: loss=306.0881, time=62.45s\n",
            "iter 750: dev acc=0.4251\n",
            "new highscore\n",
            "Iter 1000: loss=289.4373, time=84.02s\n",
            "iter 1000: dev acc=0.4314\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1250: loss=266.1829, time=105.53s\n",
            "iter 1250: dev acc=0.4550\n",
            "new highscore\n",
            "Shuffling training data\n",
            "Iter 1500: loss=246.8117, time=126.66s\n",
            "iter 1500: dev acc=0.4405\n",
            "Done training\n",
            "Loading best model\n",
            "best model iter 1250: train acc=0.6060, dev acc=0.4550, test acc=0.4661\n",
            "Test Accuracy for seed 999: 0.4661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "DHcHHaLtguUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13795f9-c59d-4e61-e40f-e81c074fff5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Test Accuracy: 0.4679\n",
            "Standard Deviation: 0.0068\n"
          ]
        }
      ],
      "source": [
        "mean_accuracy = np.mean(tree_test_accuracies_em)\n",
        "std_accuracy = np.std(tree_test_accuracies_em)\n",
        "\n",
        "print(f\"Mean Test Accuracy: {mean_accuracy:.4f}\")\n",
        "print(f\"Standard Deviation: {std_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YbNKef3lymaj",
        "AVMgsbIea8bA",
        "y0067ax54-rd",
        "uWBTzkuE3CtZ",
        "rLtBAIQGynkB",
        "oKNQjEc0yXnJ",
        "dIk6OtSdzGRP",
        "XEPsLvI-3D5b",
        "E9mB1_XhMPNN",
        "zpFt_Fo2TdN0",
        "iZanOMesTfEZ",
        "MQZ5flHwiiHY",
        "g41yW4PL9jG0",
        "X9gA-UcqSBe0",
        "YEw6XHQY_AAQ",
        "xYBJEoSNUwI0",
        "23wAZomozh_2",
        "eep3X9go3rtj",
        "q7WjcxXntMi5",
        "7SsFmBFwYXrN",
        "oDp13MWUYXrN",
        "KSjmin-mYXrO",
        "7p_DuncmYXrO",
        "CcTTV56RYXrP",
        "zak1oOZ1bs1i"
      ],
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}